{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the AI Racing League Promoting equity and innovation in AI education. The AI Racing League is a fun way to learn the concepts behind artificial intelligence! We learn AI by teaching small remote-controlled (RC) cars to drive around a track autonomously. The cars use a low-cost Raspberry Pi computer (or NVIDIA Nano) with a camera. Students drive around the track and gather a set of images to \"train\" a neural network that can be used to automatically steer the car. In addition to teaching machine learning, this course also teaches concepts like Python programming, computer vision, data science and generative AI. Our curriculum is inspired by the DonkeyCar and the CoderDojo mentoring system. We feature a wide variety of lesson plans from the 5th grade up to college-level participants. Our secret sauce is to combine the right hardware, software, mentors and a flexible learning curriculum to create fun events that students love to participate in. Current Status We are in the process of restarting our events starting in the fall of 2023. Please contact Dan McCreary if you would like to participate. The following organizations have expressed an interest in helping out: Code Savvy - Contact: Valarie Lockhart - The AI Racing League runs as a project under Code Savvy. They are a 503C not-for-profit organization. Minneanalytics - Contact: Dan Atkins - contact Dan Atkins directly if you are looking for funding for your team. Washburn High School - Contact: Peter Grul Best Buy Education - Contact: Wes Strait - Wes and his team have run several events using the Amazon deep racer. Minnesota STEM Partnership - Contact: Michael Wulf - Michael created his first AI Racing League team in the summer of 2021. Create Minneapolis Contacts: Krista Stommes and Rob Mission The mission of the AI Racing League is to create and deliver educational materials that will make fun AI training accessible to everyone. We place a special focus on students from disadvantaged communities including women and minorities. We work as a sub-project of the CodeSavvy not-for-profit organization and we adhere to their guidelines for the quality and security of our students. This means that all our volunteers have background checks and we limit the student-to-mentor ratios to no more than three students per mentor. We are committed to equal-opportunity mentoring. We strive to recruit, train and retain the best mentors we can find. We are inspired by the values behind the CoderDojo mentoring system and their innovative use of flexible concept cards . We attempt to publish concept cards that provide a flexible and agile training environment for a wide variety of learners. We believe strongly in student-led initiatives and project-based learning. We feel students learn the most when they are building things together in teams. We believe our curriculum should be broad to support a wide variety of students from begging Python to advanced AI. Rather than force students down a single path of learning, we believe our instructors should be more like travel guides to help students explore their areas of interest. Our curriculum needs to adapt to single-hour events up to multi-year mentoring. See Rhizomatic Learning for what inspires us. Checkout Our Other Sites: AI Racing League Beginning Python [MBOT] MicroPython for Kids ChatGPT for Teachers AI for Kids MBOT IoT Hackday - costume contest Other Resources Education Material: If you would like to teach AI Racing in the classroom, at a meetup or even in a corporation check out our resources here Resources: Want to connect or contribute to the community check us out here About us: Want to know more about us? Check us out","title":"Home"},{"location":"#welcome-to-the-ai-racing-league","text":"Promoting equity and innovation in AI education. The AI Racing League is a fun way to learn the concepts behind artificial intelligence! We learn AI by teaching small remote-controlled (RC) cars to drive around a track autonomously. The cars use a low-cost Raspberry Pi computer (or NVIDIA Nano) with a camera. Students drive around the track and gather a set of images to \"train\" a neural network that can be used to automatically steer the car. In addition to teaching machine learning, this course also teaches concepts like Python programming, computer vision, data science and generative AI. Our curriculum is inspired by the DonkeyCar and the CoderDojo mentoring system. We feature a wide variety of lesson plans from the 5th grade up to college-level participants. Our secret sauce is to combine the right hardware, software, mentors and a flexible learning curriculum to create fun events that students love to participate in.","title":"Welcome to the AI Racing League"},{"location":"#current-status","text":"We are in the process of restarting our events starting in the fall of 2023. Please contact Dan McCreary if you would like to participate. The following organizations have expressed an interest in helping out: Code Savvy - Contact: Valarie Lockhart - The AI Racing League runs as a project under Code Savvy. They are a 503C not-for-profit organization. Minneanalytics - Contact: Dan Atkins - contact Dan Atkins directly if you are looking for funding for your team. Washburn High School - Contact: Peter Grul Best Buy Education - Contact: Wes Strait - Wes and his team have run several events using the Amazon deep racer. Minnesota STEM Partnership - Contact: Michael Wulf - Michael created his first AI Racing League team in the summer of 2021. Create Minneapolis Contacts: Krista Stommes and Rob","title":"Current Status"},{"location":"#mission","text":"The mission of the AI Racing League is to create and deliver educational materials that will make fun AI training accessible to everyone. We place a special focus on students from disadvantaged communities including women and minorities. We work as a sub-project of the CodeSavvy not-for-profit organization and we adhere to their guidelines for the quality and security of our students. This means that all our volunteers have background checks and we limit the student-to-mentor ratios to no more than three students per mentor. We are committed to equal-opportunity mentoring. We strive to recruit, train and retain the best mentors we can find. We are inspired by the values behind the CoderDojo mentoring system and their innovative use of flexible concept cards . We attempt to publish concept cards that provide a flexible and agile training environment for a wide variety of learners. We believe strongly in student-led initiatives and project-based learning. We feel students learn the most when they are building things together in teams. We believe our curriculum should be broad to support a wide variety of students from begging Python to advanced AI. Rather than force students down a single path of learning, we believe our instructors should be more like travel guides to help students explore their areas of interest. Our curriculum needs to adapt to single-hour events up to multi-year mentoring. See Rhizomatic Learning for what inspires us.","title":"Mission"},{"location":"#checkout-our-other-sites","text":"AI Racing League Beginning Python [MBOT] MicroPython for Kids ChatGPT for Teachers AI for Kids MBOT IoT Hackday - costume contest","title":"Checkout Our Other Sites:"},{"location":"#other-resources","text":"Education Material: If you would like to teach AI Racing in the classroom, at a meetup or even in a corporation check out our resources here Resources: Want to connect or contribute to the community check us out here About us: Want to know more about us? Check us out","title":"Other Resources"},{"location":"about/","text":"About the AI Racing League The AI Racing League is a fun way to learn the concepts behind artificial intelligence! We learn AI by teaching small remote-controlled (RC) cars to drive around a track autonomously. The cars use a low-cost Raspberry Pi computer (or NVIDIA Nano) with a camera. Students drive around the track and gather a set of images to \"train\" a neural network that can be used to automatically steer the car. In addition to teaching machine learning, this course also teaches concepts like Python programming, computer vision, data science and generative AI. Our curriculum is inspired by the DonkeyCar and the CoderDojo mentoring system. We feature a wide variety of lesson plans from the 5th grade up to college-level participants. Our secret sauce is to combine the right hardware, software, mentors and a flexible learning curriculum to create fun events that students love to participate in. DIY Robocars and Donkey Cars DIY Robocars and the Donkey Cars are the communities that inspired the AI Racing League. We are grateful for all they have done and their willingness to share their knowledge through open source software. Coder Dojo A community of over 2,300 free, open and local programming clubs for young people 58,000 young people are being creative with technology with the help of 12,000 volunteers in 94 countries Visit https://coderdojo.com Code Savvy Code Savvy strives to make kids and teens more code-savvy through creative educational programs and services. We incubate and support community-based programs that bring technology and know-how to local kids and educators, all the while championing gender and ethnic diversity. Code Savvy is dedicated to ensuring the next generation of computer science professionals represents the billions of users of tomorrow\u2019s innovative technologies. Visit https://codesavvy.org Licensing Like all CoderDojo-created content, you are free to use this content in K-12 noncommercial educational settings for teaching without paying license fees. We also encourage our community to create variations and help us enlarge the curriculum. We always appreciate attribution! Details of the license terms are here: Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)","title":"About at ARL"},{"location":"about/#about-the-ai-racing-league","text":"The AI Racing League is a fun way to learn the concepts behind artificial intelligence! We learn AI by teaching small remote-controlled (RC) cars to drive around a track autonomously. The cars use a low-cost Raspberry Pi computer (or NVIDIA Nano) with a camera. Students drive around the track and gather a set of images to \"train\" a neural network that can be used to automatically steer the car. In addition to teaching machine learning, this course also teaches concepts like Python programming, computer vision, data science and generative AI. Our curriculum is inspired by the DonkeyCar and the CoderDojo mentoring system. We feature a wide variety of lesson plans from the 5th grade up to college-level participants. Our secret sauce is to combine the right hardware, software, mentors and a flexible learning curriculum to create fun events that students love to participate in.","title":"About the AI Racing League"},{"location":"about/#diy-robocars-and-donkey-cars","text":"DIY Robocars and the Donkey Cars are the communities that inspired the AI Racing League. We are grateful for all they have done and their willingness to share their knowledge through open source software.","title":"DIY Robocars and Donkey Cars"},{"location":"about/#coder-dojo","text":"A community of over 2,300 free, open and local programming clubs for young people 58,000 young people are being creative with technology with the help of 12,000 volunteers in 94 countries Visit https://coderdojo.com","title":"Coder Dojo"},{"location":"about/#code-savvy","text":"Code Savvy strives to make kids and teens more code-savvy through creative educational programs and services. We incubate and support community-based programs that bring technology and know-how to local kids and educators, all the while championing gender and ethnic diversity. Code Savvy is dedicated to ensuring the next generation of computer science professionals represents the billions of users of tomorrow\u2019s innovative technologies. Visit https://codesavvy.org","title":"Code Savvy"},{"location":"about/#licensing","text":"Like all CoderDojo-created content, you are free to use this content in K-12 noncommercial educational settings for teaching without paying license fees. We also encourage our community to create variations and help us enlarge the curriculum. We always appreciate attribution! Details of the license terms are here: Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)","title":"Licensing"},{"location":"business-plan-2020/","text":"AI Racing League Business Plan for 2020 Summary This business plan governs the AI Racing League for the calendar year 2020. We are operating as a project within CodeSavvy. CodeSavvy is a register 501C organization. CodeSavvy's mission is to promote coding skills in youth with a focus on promoting mentoring and training for girls and disavantaged youth. Brief History The AI Racing League was founded in the summary of 2019. We were inspired by the viral global DonkeyCar project. We wondered if the DonkeyCar racing events could be used to teach AI in the classroom. The goals of the founders were to promote fun events that taught AI to girls and disavantaged youth. We recieved an initial round of $9K in funding from the Optum Technology Social Responsibility and lauched our first event in August of 2019. This event was done at the International School of Minnesota and attracted members from the AI Research community, schools, educators and students. Since August we have participated in over a dozen events promoting AI instruction. We have trained and initial set of approximatly 50 mentors that are familair with the use of the DonkeyCar Status as of January 2020 Inventory of Assets 2 GPU Servers 10 DonkeyCars AI Racing League Web Site AI Racing League Concept Cards AI Racing League Concept Dependancy Graph Documented best practices (Lessons learned, SD image inventory etc.) Miscellenous training material Marketing materials including testimonials Post on social media Goals for 2020 Financial Goals","title":"Business Plan"},{"location":"business-plan-2020/#ai-racing-league-business-plan-for-2020","text":"","title":"AI Racing League Business Plan for 2020"},{"location":"business-plan-2020/#summary","text":"This business plan governs the AI Racing League for the calendar year 2020. We are operating as a project within CodeSavvy. CodeSavvy is a register 501C organization. CodeSavvy's mission is to promote coding skills in youth with a focus on promoting mentoring and training for girls and disavantaged youth.","title":"Summary"},{"location":"business-plan-2020/#brief-history","text":"The AI Racing League was founded in the summary of 2019. We were inspired by the viral global DonkeyCar project. We wondered if the DonkeyCar racing events could be used to teach AI in the classroom. The goals of the founders were to promote fun events that taught AI to girls and disavantaged youth. We recieved an initial round of $9K in funding from the Optum Technology Social Responsibility and lauched our first event in August of 2019. This event was done at the International School of Minnesota and attracted members from the AI Research community, schools, educators and students. Since August we have participated in over a dozen events promoting AI instruction. We have trained and initial set of approximatly 50 mentors that are familair with the use of the DonkeyCar","title":"Brief History"},{"location":"business-plan-2020/#status-as-of-january-2020","text":"","title":"Status as of January 2020"},{"location":"business-plan-2020/#inventory-of-assets","text":"2 GPU Servers 10 DonkeyCars AI Racing League Web Site AI Racing League Concept Cards AI Racing League Concept Dependancy Graph Documented best practices (Lessons learned, SD image inventory etc.) Miscellenous training material Marketing materials including testimonials Post on social media","title":"Inventory of Assets"},{"location":"business-plan-2020/#goals-for-2020","text":"","title":"Goals for 2020"},{"location":"business-plan-2020/#financial-goals","text":"","title":"Financial Goals"},{"location":"command-line-tips/","text":"Command Line Tips Get Setup 1 2 3 git config --global user.name \"Joe Smith\" git config --global user.email \"Joe.Smith123@gmail.com\" git config --global credential.helper store Raspberry Pi Command Line Tips From Terminal, you can open the current directory in the File Manager using the xdg-open command. This is similar to the Mac open command. 1 $ xdg-open . See If the PWM Board Is Working 1 i2cdetect -l This should return 1 i2c -1 i2c bcm2835 ( i2c @7e804000 ) I2C adapter 1 i2cdetect -y 1 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- -- 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: 40 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: 70 -- -- -- -- -- -- -- Note that the line 40 and 70 has values under column 0 (I2C bus 1) If you unplug the data you should get: 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- -- 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: -- -- -- -- -- -- -- -- SD Card Speed Test Home -> Accessories -> Raspberry Pi Diagnostics 1 2 3 4 5 6 7 8 9 10 11 12 13 Raspberry Pi Diagnostics - version 0 . 9 Sat Jul 3 14 : 25 : 23 2021 Test : SD Card Speed Test Run 1 prepare-file ; 0 ; 0 ; 20628 ; 40 seq-write ; 0 ; 0 ; 21999 ; 42 rand-4k-write ; 0 ; 0 ; 4498 ; 1124 rand-4k-read ; 8695 ; 2173 ; 0 ; 0 Sequential write speed 21999 KB / sec ( target 10000 ) - PASS Random write speed 1124 IOPS ( target 500 ) - PASS Random read speed 2173 IOPS ( target 1500 ) - PASS Test PASS","title":"Command Line Tips"},{"location":"command-line-tips/#command-line-tips","text":"","title":"Command Line Tips"},{"location":"command-line-tips/#get-setup","text":"1 2 3 git config --global user.name \"Joe Smith\" git config --global user.email \"Joe.Smith123@gmail.com\" git config --global credential.helper store","title":"Get Setup"},{"location":"command-line-tips/#raspberry-pi-command-line-tips","text":"From Terminal, you can open the current directory in the File Manager using the xdg-open command. This is similar to the Mac open command. 1 $ xdg-open .","title":"Raspberry Pi Command Line Tips"},{"location":"command-line-tips/#see-if-the-pwm-board-is-working","text":"1 i2cdetect -l This should return 1 i2c -1 i2c bcm2835 ( i2c @7e804000 ) I2C adapter 1 i2cdetect -y 1 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- -- 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: 40 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: 70 -- -- -- -- -- -- -- Note that the line 40 and 70 has values under column 0 (I2C bus 1) If you unplug the data you should get: 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- -- 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: -- -- -- -- -- -- -- --","title":"See If the PWM Board Is Working"},{"location":"command-line-tips/#sd-card-speed-test","text":"Home -> Accessories -> Raspberry Pi Diagnostics 1 2 3 4 5 6 7 8 9 10 11 12 13 Raspberry Pi Diagnostics - version 0 . 9 Sat Jul 3 14 : 25 : 23 2021 Test : SD Card Speed Test Run 1 prepare-file ; 0 ; 0 ; 20628 ; 40 seq-write ; 0 ; 0 ; 21999 ; 42 rand-4k-write ; 0 ; 0 ; 4498 ; 1124 rand-4k-read ; 8695 ; 2173 ; 0 ; 0 Sequential write speed 21999 KB / sec ( target 10000 ) - PASS Random write speed 1124 IOPS ( target 500 ) - PASS Random read speed 2173 IOPS ( target 1500 ) - PASS Test PASS","title":"SD Card Speed Test"},{"location":"contacts/","text":"AI Racing League Contacts Dan McCreary - LinkedIn","title":"Contacts"},{"location":"contacts/#ai-racing-league-contacts","text":"Dan McCreary - LinkedIn","title":"AI Racing League Contacts"},{"location":"curriculum/","text":"Curriculum We have a long term vision of using an intelligent agent that will recommend the right content for each of our students based on their current knowledge and their learning goals. Beginning (Green) Concepts Batteries Motors Donkey Car Activity Go to the donkey car station and look at the sample Donkey Car. Ask a mentor to show you the parts. Questions 1) What are the key parts of the Donkey Car? The key parts are: * RC Car chassis * Nvidia Jetson Nano * Servo controller * Camera * Battery for the Nano 2) How do the front wheels turn? A 180 degree servo is used to steer the car 3) Can you find an electric motor? There is only a single motor in the RC chassis 4) Can you find a battery? Are their multiple batteries? There are two batteries - one for the motor and one for the Jetson Nano 5) Where is the Jetson Nano (computer)? It is right on top! 6) Where is the camera? Is it on the front or back of the car? The camera is on the top facing forward 7) What happens to the opposite wheel when you turn it? The transmission makes the wheels turn in opposite direction when one wheel is turned. - Is this correct? 8) How much does a Donkey Car cost? The car costs around $250 each. The RC chassis cost about $110. 9) Why do you think they call it a \u201cDonkey Car\u201d? They call it a \"Donkey Car\" because, like a Donkey, it is functional but not very sleek. Intermediate Concepts Machine Learning Activity Go to the machine learning station and watch the demos. Ask about the difference between if-else statements and machine learning. Questions 1) What is Machine Learning? How does it differ from traditional rule-based programming? Machine learning is a method of data analysis that automates analytical model building, based on the idea that systems can learn from data. Rule-based programming is built off of if-else statements in code, and therefore every possible situation has to be thought of in advance by the programmer. Therefore, machine learning is well suited for situations where all possible inputs may not be defined. 2) How does a computer learn? The computer learns through a process called training. Training is the process of adjusting a mathematical formula by feeding it data and adjusting the formula until it produces the desired output. 3) What are the major groups of machine learning? There are 5 major groups of algorithms within machine learning. They are: * The connectionists (Neural Networks) * The analogizers (Support Vector Machines) * The Bayesians (Bayes\u2019 Theorem) * The evolutionaries (Genetic Algorithms) * The symbolists (Inverse Deduction) 4) Applications of machine learning are everywhere, what are some examples? Some applications of machine learning are: * Voice Assistants (Siri, Alexa, etc.) * Translation * Self-Driving Cars Blue Concepts Black Concepts","title":"Curriculum Overview"},{"location":"curriculum/#curriculum","text":"We have a long term vision of using an intelligent agent that will recommend the right content for each of our students based on their current knowledge and their learning goals.","title":"Curriculum"},{"location":"curriculum/#beginning-green-concepts","text":"","title":"Beginning (Green) Concepts"},{"location":"curriculum/#batteries","text":"","title":"Batteries"},{"location":"curriculum/#motors","text":"","title":"Motors"},{"location":"curriculum/#donkey-car","text":"","title":"Donkey Car"},{"location":"curriculum/#activity","text":"Go to the donkey car station and look at the sample Donkey Car. Ask a mentor to show you the parts.","title":"Activity"},{"location":"curriculum/#questions","text":"1) What are the key parts of the Donkey Car? The key parts are: * RC Car chassis * Nvidia Jetson Nano * Servo controller * Camera * Battery for the Nano 2) How do the front wheels turn? A 180 degree servo is used to steer the car 3) Can you find an electric motor? There is only a single motor in the RC chassis 4) Can you find a battery? Are their multiple batteries? There are two batteries - one for the motor and one for the Jetson Nano 5) Where is the Jetson Nano (computer)? It is right on top! 6) Where is the camera? Is it on the front or back of the car? The camera is on the top facing forward 7) What happens to the opposite wheel when you turn it? The transmission makes the wheels turn in opposite direction when one wheel is turned. - Is this correct? 8) How much does a Donkey Car cost? The car costs around $250 each. The RC chassis cost about $110. 9) Why do you think they call it a \u201cDonkey Car\u201d? They call it a \"Donkey Car\" because, like a Donkey, it is functional but not very sleek.","title":"Questions"},{"location":"curriculum/#intermediate-concepts","text":"","title":"Intermediate Concepts"},{"location":"curriculum/#machine-learning","text":"","title":"Machine Learning"},{"location":"curriculum/#activity_1","text":"Go to the machine learning station and watch the demos. Ask about the difference between if-else statements and machine learning.","title":"Activity"},{"location":"curriculum/#questions_1","text":"1) What is Machine Learning? How does it differ from traditional rule-based programming? Machine learning is a method of data analysis that automates analytical model building, based on the idea that systems can learn from data. Rule-based programming is built off of if-else statements in code, and therefore every possible situation has to be thought of in advance by the programmer. Therefore, machine learning is well suited for situations where all possible inputs may not be defined. 2) How does a computer learn? The computer learns through a process called training. Training is the process of adjusting a mathematical formula by feeding it data and adjusting the formula until it produces the desired output. 3) What are the major groups of machine learning? There are 5 major groups of algorithms within machine learning. They are: * The connectionists (Neural Networks) * The analogizers (Support Vector Machines) * The Bayesians (Bayes\u2019 Theorem) * The evolutionaries (Genetic Algorithms) * The symbolists (Inverse Deduction) 4) Applications of machine learning are everywhere, what are some examples? Some applications of machine learning are: * Voice Assistants (Siri, Alexa, etc.) * Translation * Self-Driving Cars","title":"Questions"},{"location":"curriculum/#blue-concepts","text":"","title":"Blue Concepts"},{"location":"curriculum/#black-concepts","text":"","title":"Black Concepts"},{"location":"demo/","text":"Demos Although our students love hands-on learning with our DonkeyCars, there are other aspects of Artificial Intelligence we like to discuss in our classes. Here are some demos we use in our classrooms. The Teachable Machine by Google This demo works with almost any PC that has a built-in video camera. You give it a set of images or pictures, or sounds and you build a model that predicts what a new images our sounds might be. This is called a \"classification\" model. Teachable Machine Much of our classroom work is centered around the hot topic of Deep Learning. But AI is much more than just Deep Learning. Here are a few other areas to consider. (Taken from the book The Master Algorithm) 5 Camps of Machine Learning Demos Connectionists (Neural Networks) Check Out TensorFlow Playground Analogizers (Support Vector Machines) TODO: FIND GOOD DEMO Bayesians (Bayes\u2019 Theorem) Check Out A Bayes' Theorem Example Evolutionaries (Genetic Algorithms) Watch This Animation Learn to Walk Symbolists (Inverse Deduction) Look at this Decision Tree Demo","title":"Demos"},{"location":"demo/#demos","text":"Although our students love hands-on learning with our DonkeyCars, there are other aspects of Artificial Intelligence we like to discuss in our classes. Here are some demos we use in our classrooms.","title":"Demos"},{"location":"demo/#the-teachable-machine-by-google","text":"This demo works with almost any PC that has a built-in video camera. You give it a set of images or pictures, or sounds and you build a model that predicts what a new images our sounds might be. This is called a \"classification\" model. Teachable Machine Much of our classroom work is centered around the hot topic of Deep Learning. But AI is much more than just Deep Learning. Here are a few other areas to consider. (Taken from the book The Master Algorithm)","title":"The Teachable Machine by Google"},{"location":"demo/#5-camps-of-machine-learning-demos","text":"","title":"5 Camps of Machine Learning Demos"},{"location":"demo/#connectionists-neural-networks","text":"Check Out TensorFlow Playground","title":"Connectionists (Neural Networks)"},{"location":"demo/#analogizers-support-vector-machines","text":"TODO: FIND GOOD DEMO","title":"Analogizers (Support Vector Machines)"},{"location":"demo/#bayesians-bayes-theorem","text":"Check Out A Bayes' Theorem Example","title":"Bayesians (Bayes\u2019 Theorem)"},{"location":"demo/#evolutionaries-genetic-algorithms","text":"Watch This Animation Learn to Walk","title":"Evolutionaries (Genetic Algorithms)"},{"location":"demo/#symbolists-inverse-deduction","text":"Look at this Decision Tree Demo","title":"Symbolists (Inverse Deduction)"},{"location":"faqs/","text":"AI Racing League Frequently Asked Questions How much does it cost? All our events are free. However, ff you want to build your own car you are welcome to bring these to the events. Parts for a DonkeyCar typically run about $250 US. See our Car Parts Lists for details. What do I have to know before I come? Nothing! We have material for beginners without any prior knowledge of AI. What car part hardware do you use? We use mostly NVIDIA Nano and Raspberry Pi 4 for our single board computers. We use a wide variety of RC-car engines but the [Exceed Magnet] 1/16 scale RC car is a low-cost standard. See our Car Parts Lists for details. Typical car parts cost around $250 US. What GPUs do you use and how much do they cost? We use a standard PC chassis running Lunix with a NVIDIA GPU such as a GTX 2080. These PCs can be purchased for around $1,500. Se our GPU Parts List for details. How do I sign up as a student? The best way to get involved is by signing up as a student at the CoderDojo Twin Cities web site: Coderdojotc.org How do I become a mentor? The best way to get involved is by signing up as a mentor at the CoderDojo Twin Cities web site: https://www.coderdojotc.org/mentor_signup/ How do I start my own chapter of the AI Racing League Please connect with Dan McCreary on LinkedIn and indicate in the note you would like to start your own chapter. Be sure to include information about your leadership and technical background and any related experience working with STEM programs. Can I get a grant to purchase hardware for our school or club? We are working on arranging a grant application process. The best way to start this process is to gather a small group of volunteers that can create a sustainable club. Include people that have a combination of fundraising, technology, education and marketing skills. Reach out to local school administration officials to build a community of science/math and STEM educators. Network with local companies that are trying to build local talent in AI and machine learning. Please contact Dan McCreary on LinkedIn for details.","title":"FAQs"},{"location":"faqs/#ai-racing-league-frequently-asked-questions","text":"","title":"AI Racing League Frequently Asked Questions"},{"location":"faqs/#how-much-does-it-cost","text":"All our events are free. However, ff you want to build your own car you are welcome to bring these to the events. Parts for a DonkeyCar typically run about $250 US. See our Car Parts Lists for details.","title":"How much does it cost?"},{"location":"faqs/#what-do-i-have-to-know-before-i-come","text":"Nothing! We have material for beginners without any prior knowledge of AI.","title":"What do I have to know before I come?"},{"location":"faqs/#what-car-part-hardware-do-you-use","text":"We use mostly NVIDIA Nano and Raspberry Pi 4 for our single board computers. We use a wide variety of RC-car engines but the [Exceed Magnet] 1/16 scale RC car is a low-cost standard. See our Car Parts Lists for details. Typical car parts cost around $250 US.","title":"What car part hardware do you use?"},{"location":"faqs/#what-gpus-do-you-use-and-how-much-do-they-cost","text":"We use a standard PC chassis running Lunix with a NVIDIA GPU such as a GTX 2080. These PCs can be purchased for around $1,500. Se our GPU Parts List for details.","title":"What GPUs do you use and how much do they cost?"},{"location":"faqs/#how-do-i-sign-up-as-a-student","text":"The best way to get involved is by signing up as a student at the CoderDojo Twin Cities web site: Coderdojotc.org","title":"How do I sign up as a student?"},{"location":"faqs/#how-do-i-become-a-mentor","text":"The best way to get involved is by signing up as a mentor at the CoderDojo Twin Cities web site: https://www.coderdojotc.org/mentor_signup/","title":"How do I become a mentor?"},{"location":"faqs/#how-do-i-start-my-own-chapter-of-the-ai-racing-league","text":"Please connect with Dan McCreary on LinkedIn and indicate in the note you would like to start your own chapter. Be sure to include information about your leadership and technical background and any related experience working with STEM programs.","title":"How do I start my own chapter of the AI Racing League"},{"location":"faqs/#can-i-get-a-grant-to-purchase-hardware-for-our-school-or-club","text":"We are working on arranging a grant application process. The best way to start this process is to gather a small group of volunteers that can create a sustainable club. Include people that have a combination of fundraising, technology, education and marketing skills. Reach out to local school administration officials to build a community of science/math and STEM educators. Network with local companies that are trying to build local talent in AI and machine learning. Please contact Dan McCreary on LinkedIn for details.","title":"Can I get a grant to purchase hardware for our school or club?"},{"location":"glossary/","text":"AI Racing League Glossary of Term Calibration A step in setting up a DonkeyCar where around five values configuration file is created that reflect the physical aspects of the RC car. There are three parameters for the throttle and two parameters for the steering. It is important to get these five parameters correct so you can precisely drive your DonkeyCar. Catalog File A format of storing our image-related throttle and steering data in line-oriented file where each line contains the serialized JSON information when the image was captured. Note that the catalog files are not pure JSON files. Only the data within each line is a valid data JSON object. The catalog file formats have changed between DonkeyCar releases. The current version is called V2 format. CoderDojo An international program of over 2,300 coding clubs that uses data-driven practices to get students interested in coding. Many of the aspects of the AI Racing League uses these same principals. Key aspects of CoderDojo are: No fees for events - accessible to all Mentor ratios of no more than three students per mentor Project-based learning Focus on getting students to work in teams (social learning) Student-directed projects Focus on programs for girls in coding and underprivileged youth Their main web site is: http://coderdojo.com/ CoderDojo Twin Cities Python Labs These labs are a set of free-courses to learn Python using fun turtle graphics. There is no experience needed. Link to CoderDojo Twin Cities Python Labs CoderDojo Twin Cities MicroPython Labs These labs are a set of free-courses to learn MicroPython. You should have a background in Python to use these labs. There are lessons in sensors, motors and robots. Link to CoderDojo Twin Cities MicroPython Labs CoderDojo Base MicroPython Robot This is a $25 robot that you build and program with MicroPython. If you are working on a project that will lead up to a full DonkeyCar, this is an ideal project to get you started. The robot will get you familiar with concepts like PWM, motor controllers and sensors. Blog article Microsite Raspberry Pi Pico MicroPython Base Robot Code Savvy Code Savvy is a not-for-profit organization with 501(c)3 status from the IRS that the AI Racing League works as a sub-project. All the AI Racing League financials are organized under a Code Savvy program. Donations to the AI Racing League should be done though Code Savvy donations. Questions about Code Savvy can be sent to kidscode@codesavvy.org Code Savvy Web Site Concept Cards These are small laminated cards that have concepts information on them that students can learn. The idea is one-concept per card. See the CoderDojo TC Guide for Authoring Concept Cards Donkey Car This is a trademarked name of a car that is used at our events. The name implies \"ugly\" so you know that they are not designed to look pretty, just functional cars with a camera on the front. DonkeyCar web site GPU Server Each of the AI Racing League events usually has at least one GPU server for training our models. These are typically small portable PCs with a GPU card in them. The entire GPU server cost around $1,200 each and can train a 20,000 image data set in under five minutes. We typically suggest that clubs writing grants use a NVIDIA GEFORCE RTX 2070 8GB or similar card since it is both fast enough for 10-team events but cost effective that schools can afford them. These card are often available used on e-Bay for a few hundred dollars. Note that we have tried to use cloud-based services at some of our events but we can't be guaranteed that there is enough WiFi bandwidth to move large datasets and models to and from the cloud. We feel that the tasks involved in setting up the GPU server is also a valuable skill for our students. See the GPU Parts List for a list of components. Electronic Speed Control An electronic circuit that controls and regulates the speed of an electric motor. It also can reverse the direction of the motor. Our ESC Wikipedia Page on Electronic Speed Control Fifteen Degree Camera Angle The angle our cameras need to point down to have a good view of the road ahead. Normalized Values that have been converted into a standard that can be used across many situations. For example, we don't store the exact PWM ratios of the throttle and steering values in our catalog files. We convert these values into ranges from 0.0 to 1.0 so that all our machine learning models can share them. This is why we also need the configuration values when the drive commands are used to convert the normalized values back to the appropriate PWM ranges unique to each car. Pulse Width Modulation The way that we control the [Electronic Speed Controller] (ESC) and the servo by sending digital square waves with a variable ratio of the width of the positive part of the square wave. Wikipeda Page on Pulse-width modulation Tracks We want our clubs to all have affordable but high-quality tracks that are easy to roll up and store. Our suggestion is to find used billboard vinyl in a dark color (black or dark blue) and then use white and yellow tape to place down the lines. https://billboardtarps.com/product-category/billboard-vinyl/ YouTube Video Training Step The step in DonkeyCar setup where we take approximately 20,000 small image files and the throttle and steering information with each image to build a deep neural network. The training step requires us to move the data off the DonkeyCar's SD card and transfer the data to a more powerful GPU server. Using a typical $1,200 GPU server we can build a model file in around five minutes. This file is then transferred back to the DonkeyCar for autonomous driving. Tubs This is the term that the DonkeyCar software uses to store training data. Each tub consists of a catalog of information about the drive and the images associated with that drive. Note that the format of the tubs changes over time so old tubs formats may need to be converted to newer formats. Script to Convert Tugs from V1 format to V2 Format DonkeyCar Catalog Format Tub Catalog Format Each Tub is a directory (folder) has two components: A sub folder called \"images\" that contains the jpeg images gathered during a training run. There are typically 10,000 to 20,000 small images in a tub image folder. A file that describes the data about all the images called a Catalog file. The Catalog file is similar to a JSON file but it has no root data elements. NVIDIA Nano One of the two single board computers we use in our DonkeyCars. The current Nanos have 4GB RAM and a GPU for accelerating real-time inference. The full product name is the NVIDIA Jetson Nano. The price for a 4GB Nano is around $99 but they occasionally go on sale for $79. The Nano became available for sale in the US in April of 2019. A 2GB version has also been sold for $59 but the lack of RAM memory makes it difficult to use for many of our AI Racing League events and we don't recommend it. Note that we do not use the Nano for training. We transfer the data to a GPU server that has more parallel cores for training.","title":"Glossary"},{"location":"glossary/#ai-racing-league-glossary-of-term","text":"","title":"AI Racing League Glossary of Term"},{"location":"glossary/#calibration","text":"A step in setting up a DonkeyCar where around five values configuration file is created that reflect the physical aspects of the RC car. There are three parameters for the throttle and two parameters for the steering. It is important to get these five parameters correct so you can precisely drive your DonkeyCar.","title":"Calibration"},{"location":"glossary/#catalog-file","text":"A format of storing our image-related throttle and steering data in line-oriented file where each line contains the serialized JSON information when the image was captured. Note that the catalog files are not pure JSON files. Only the data within each line is a valid data JSON object. The catalog file formats have changed between DonkeyCar releases. The current version is called V2 format.","title":"Catalog File"},{"location":"glossary/#coderdojo","text":"An international program of over 2,300 coding clubs that uses data-driven practices to get students interested in coding. Many of the aspects of the AI Racing League uses these same principals. Key aspects of CoderDojo are: No fees for events - accessible to all Mentor ratios of no more than three students per mentor Project-based learning Focus on getting students to work in teams (social learning) Student-directed projects Focus on programs for girls in coding and underprivileged youth Their main web site is: http://coderdojo.com/","title":"CoderDojo"},{"location":"glossary/#coderdojo-twin-cities-python-labs","text":"These labs are a set of free-courses to learn Python using fun turtle graphics. There is no experience needed. Link to CoderDojo Twin Cities Python Labs","title":"CoderDojo Twin Cities Python Labs"},{"location":"glossary/#coderdojo-twin-cities-micropython-labs","text":"These labs are a set of free-courses to learn MicroPython. You should have a background in Python to use these labs. There are lessons in sensors, motors and robots. Link to CoderDojo Twin Cities MicroPython Labs","title":"CoderDojo Twin Cities MicroPython Labs"},{"location":"glossary/#coderdojo-base-micropython-robot","text":"This is a $25 robot that you build and program with MicroPython. If you are working on a project that will lead up to a full DonkeyCar, this is an ideal project to get you started. The robot will get you familiar with concepts like PWM, motor controllers and sensors. Blog article Microsite Raspberry Pi Pico MicroPython Base Robot","title":"CoderDojo Base MicroPython Robot"},{"location":"glossary/#code-savvy","text":"Code Savvy is a not-for-profit organization with 501(c)3 status from the IRS that the AI Racing League works as a sub-project. All the AI Racing League financials are organized under a Code Savvy program. Donations to the AI Racing League should be done though Code Savvy donations. Questions about Code Savvy can be sent to kidscode@codesavvy.org Code Savvy Web Site","title":"Code Savvy"},{"location":"glossary/#concept-cards","text":"These are small laminated cards that have concepts information on them that students can learn. The idea is one-concept per card. See the CoderDojo TC Guide for Authoring Concept Cards","title":"Concept Cards"},{"location":"glossary/#donkey-car","text":"This is a trademarked name of a car that is used at our events. The name implies \"ugly\" so you know that they are not designed to look pretty, just functional cars with a camera on the front. DonkeyCar web site","title":"Donkey Car"},{"location":"glossary/#gpu-server","text":"Each of the AI Racing League events usually has at least one GPU server for training our models. These are typically small portable PCs with a GPU card in them. The entire GPU server cost around $1,200 each and can train a 20,000 image data set in under five minutes. We typically suggest that clubs writing grants use a NVIDIA GEFORCE RTX 2070 8GB or similar card since it is both fast enough for 10-team events but cost effective that schools can afford them. These card are often available used on e-Bay for a few hundred dollars. Note that we have tried to use cloud-based services at some of our events but we can't be guaranteed that there is enough WiFi bandwidth to move large datasets and models to and from the cloud. We feel that the tasks involved in setting up the GPU server is also a valuable skill for our students. See the GPU Parts List for a list of components.","title":"GPU Server"},{"location":"glossary/#electronic-speed-control","text":"An electronic circuit that controls and regulates the speed of an electric motor. It also can reverse the direction of the motor. Our ESC Wikipedia Page on Electronic Speed Control","title":"Electronic Speed Control"},{"location":"glossary/#fifteen-degree-camera-angle","text":"The angle our cameras need to point down to have a good view of the road ahead.","title":"Fifteen Degree Camera Angle"},{"location":"glossary/#normalized","text":"Values that have been converted into a standard that can be used across many situations. For example, we don't store the exact PWM ratios of the throttle and steering values in our catalog files. We convert these values into ranges from 0.0 to 1.0 so that all our machine learning models can share them. This is why we also need the configuration values when the drive commands are used to convert the normalized values back to the appropriate PWM ranges unique to each car.","title":"Normalized"},{"location":"glossary/#pulse-width-modulation","text":"The way that we control the [Electronic Speed Controller] (ESC) and the servo by sending digital square waves with a variable ratio of the width of the positive part of the square wave. Wikipeda Page on Pulse-width modulation","title":"Pulse Width Modulation"},{"location":"glossary/#tracks","text":"We want our clubs to all have affordable but high-quality tracks that are easy to roll up and store. Our suggestion is to find used billboard vinyl in a dark color (black or dark blue) and then use white and yellow tape to place down the lines. https://billboardtarps.com/product-category/billboard-vinyl/ YouTube Video","title":"Tracks"},{"location":"glossary/#training-step","text":"The step in DonkeyCar setup where we take approximately 20,000 small image files and the throttle and steering information with each image to build a deep neural network. The training step requires us to move the data off the DonkeyCar's SD card and transfer the data to a more powerful GPU server. Using a typical $1,200 GPU server we can build a model file in around five minutes. This file is then transferred back to the DonkeyCar for autonomous driving.","title":"Training Step"},{"location":"glossary/#tubs","text":"This is the term that the DonkeyCar software uses to store training data. Each tub consists of a catalog of information about the drive and the images associated with that drive. Note that the format of the tubs changes over time so old tubs formats may need to be converted to newer formats. Script to Convert Tugs from V1 format to V2 Format DonkeyCar Catalog Format","title":"Tubs"},{"location":"glossary/#tub-catalog-format","text":"Each Tub is a directory (folder) has two components: A sub folder called \"images\" that contains the jpeg images gathered during a training run. There are typically 10,000 to 20,000 small images in a tub image folder. A file that describes the data about all the images called a Catalog file. The Catalog file is similar to a JSON file but it has no root data elements.","title":"Tub Catalog Format"},{"location":"glossary/#nvidia-nano","text":"One of the two single board computers we use in our DonkeyCars. The current Nanos have 4GB RAM and a GPU for accelerating real-time inference. The full product name is the NVIDIA Jetson Nano. The price for a 4GB Nano is around $99 but they occasionally go on sale for $79. The Nano became available for sale in the US in April of 2019. A 2GB version has also been sold for $59 but the lack of RAM memory makes it difficult to use for many of our AI Racing League events and we don't recommend it. Note that we do not use the Nano for training. We transfer the data to a GPU server that has more parallel cores for training.","title":"NVIDIA Nano"},{"location":"google-analytics/","text":"Google Analytics Our Google Tracking ID is: G-RL4MZ0MHZ4 You can see the activity here: Google Dashboard How We Enabled Google Analytics mkdocs material supports Google Analysis. We only need to add four lines to our mkdocs.yml configuration file. 1 2 3 4 extra : analytics : provider : google property : G - RL4MZ0MHZ4 See our mkdocs.yml on GitHub here: mkdocs.yml file in GitHub The following line is placed into each HTML web page in the site branch: 1 < script async = \"\" src = \"https://www.googletagmanager.com/gtag/js?id=G-RL4MZ0MHZ4\" ></ script >","title":"Google Analytics"},{"location":"google-analytics/#google-analytics","text":"Our Google Tracking ID is: G-RL4MZ0MHZ4 You can see the activity here: Google Dashboard","title":"Google Analytics"},{"location":"google-analytics/#how-we-enabled-google-analytics","text":"mkdocs material supports Google Analysis. We only need to add four lines to our mkdocs.yml configuration file. 1 2 3 4 extra : analytics : provider : google property : G - RL4MZ0MHZ4 See our mkdocs.yml on GitHub here: mkdocs.yml file in GitHub The following line is placed into each HTML web page in the site branch: 1 < script async = \"\" src = \"https://www.googletagmanager.com/gtag/js?id=G-RL4MZ0MHZ4\" ></ script >","title":"How We Enabled Google Analytics"},{"location":"hackathon/","text":"Hackathon Ideas Here are a set of ideas that can be used to plan a Hackathon around the DonkeyCar. Of course, if people are not familiar with the DonkeyCar, just getting it to work is a good project! These are more for teams that are extending the DonkeyCar software. Beginner Projects - done in under one day Ceate a UNIX shell script for managing the tub and model files. The script should present a menu of options that includes seending one or more tubs to a server for training and getting the model file back. Bonus points for listing models and sizes. Build an RGB strip interface to display car and GPU status. The car status can move from red to green as the number of images gathered reaches a 10K image target. Use the Arduino Moving Rainbow site as your guide. Background with Arduino is helpful. Build an RGB strip interface to display the GPU training status. The RGB strip should change from red to green as the model error approaches a reasonable value like 0.01. Background with Arduino is helpful. You will also need to modify the TensorFlow interface to write error values to a serial port using python. Medium Projects - could be done with prep over a weekend Ceate a web interface for managing the tub and model files. The web interface should present a menu of options that includes seending one or more tubs to a server for training and getting the model file back. Bonus points for listing models and sizes. Create a Jypyter notebook for analysing data and managing the tub and model files. The web interface should present a menu of options that includes seending one or more tubs to a server for training and getting the model file back. Bonus points for listing models and sizes. Advanced Projects Create a mobile application to drive the car during training. This is the best practice for many low-cost robots like the MiP. Use the tilt APIs to make the car easy to steer. Create small and large versions of the DonkeyCar. The small version should work on a 12x16 track and the large version should work outdoors on grass. Use a Raspberry Pi or Nvidia Nano. Create a learning management system for running AI Racing League events. Create a list of concepts and load a dependancy list of concepts into a graph database like TigerGraph. Create a quiz that attendees use to enter their learning objectives and background. The app should then recomend what content they should use, what tables they should visit, and what mentors they should connect with in the right order. Bonus points of you build a cell phone front end. Car to GPU File Transfer Scripts Make it easy to transfer DonkeyCar test data to our GPU server. Start with a UNIX shell script that compresses the tub file and puts the data on a jump drive. Then work on using SSH to copy the files to the GPU server. Then add configuration of the Avahi application and the mDNS protocols to autodiscover the ARL GPU servers and prompte the user. Mobile App to Drive The Car Most robot systems like the MIP have a simple mobile application for driving your robot around. There are two modes: A tilt mode (where you steer by tilting the phone) and a pressure mode where you can control the speed and direction by pressing on a virtual joystick. The problem we have with the current DonkeyCar 3.X system is that the web-based application is difficult to use. The tilt mode does not work on web browsers. We suggest you use a program like AppInventor for Android or Google Flutter and Dash building mobile apps. Leaderboard Web Page Create a web application that tracks what teams are in the lead. The app should be a single-page application that allows team scores to be updated on a web form. The leaderboard can also be \"smart\" a look for the team config files on each DonkeyCar on the local-area network. OLED Extension Add a low-cost OLED screen to each car using the SPI bus. Have the OLED screen show key parameters such as hostname, static IP address, disk space free, training data size etc. Bonus points for a mode button to cycle through screens. See Dan McCreary for the hardware. LED Strips for Training Server Status Add an low-cost WS-2811-B LED strip to the GPU server. Make the strip blue when idle, red when you start training an new model, and have it fade to green as the model converges. See Dan McCreary for the hardware. Training Graph As students walk in, give them a tablet to register. It will also ask them basic questions. It will then ask them how long they will be there. It will then suggest a set of activities and some concepts to master. The graph is a dependacy graph of all the concepts we teach at the event. Also suggest a probability they will have fun at the event. Single Source Publishing for Concept Cards Our cards need to be authored in MarkDown but we want to disply on the web, in PPT and with PDF. To do this we want to adopt a single-source publishing pipeline.","title":"Hackathon"},{"location":"hackathon/#hackathon-ideas","text":"Here are a set of ideas that can be used to plan a Hackathon around the DonkeyCar. Of course, if people are not familiar with the DonkeyCar, just getting it to work is a good project! These are more for teams that are extending the DonkeyCar software.","title":"Hackathon Ideas"},{"location":"hackathon/#beginner-projects-done-in-under-one-day","text":"Ceate a UNIX shell script for managing the tub and model files. The script should present a menu of options that includes seending one or more tubs to a server for training and getting the model file back. Bonus points for listing models and sizes. Build an RGB strip interface to display car and GPU status. The car status can move from red to green as the number of images gathered reaches a 10K image target. Use the Arduino Moving Rainbow site as your guide. Background with Arduino is helpful. Build an RGB strip interface to display the GPU training status. The RGB strip should change from red to green as the model error approaches a reasonable value like 0.01. Background with Arduino is helpful. You will also need to modify the TensorFlow interface to write error values to a serial port using python.","title":"Beginner Projects - done in under one day"},{"location":"hackathon/#medium-projects-could-be-done-with-prep-over-a-weekend","text":"Ceate a web interface for managing the tub and model files. The web interface should present a menu of options that includes seending one or more tubs to a server for training and getting the model file back. Bonus points for listing models and sizes. Create a Jypyter notebook for analysing data and managing the tub and model files. The web interface should present a menu of options that includes seending one or more tubs to a server for training and getting the model file back. Bonus points for listing models and sizes.","title":"Medium Projects - could be done with prep over a weekend"},{"location":"hackathon/#advanced-projects","text":"Create a mobile application to drive the car during training. This is the best practice for many low-cost robots like the MiP. Use the tilt APIs to make the car easy to steer. Create small and large versions of the DonkeyCar. The small version should work on a 12x16 track and the large version should work outdoors on grass. Use a Raspberry Pi or Nvidia Nano. Create a learning management system for running AI Racing League events. Create a list of concepts and load a dependancy list of concepts into a graph database like TigerGraph. Create a quiz that attendees use to enter their learning objectives and background. The app should then recomend what content they should use, what tables they should visit, and what mentors they should connect with in the right order. Bonus points of you build a cell phone front end.","title":"Advanced Projects"},{"location":"hackathon/#car-to-gpu-file-transfer-scripts","text":"Make it easy to transfer DonkeyCar test data to our GPU server. Start with a UNIX shell script that compresses the tub file and puts the data on a jump drive. Then work on using SSH to copy the files to the GPU server. Then add configuration of the Avahi application and the mDNS protocols to autodiscover the ARL GPU servers and prompte the user.","title":"Car to GPU File Transfer Scripts"},{"location":"hackathon/#mobile-app-to-drive-the-car","text":"Most robot systems like the MIP have a simple mobile application for driving your robot around. There are two modes: A tilt mode (where you steer by tilting the phone) and a pressure mode where you can control the speed and direction by pressing on a virtual joystick. The problem we have with the current DonkeyCar 3.X system is that the web-based application is difficult to use. The tilt mode does not work on web browsers. We suggest you use a program like AppInventor for Android or Google Flutter and Dash building mobile apps.","title":"Mobile App to Drive The Car"},{"location":"hackathon/#leaderboard-web-page","text":"Create a web application that tracks what teams are in the lead. The app should be a single-page application that allows team scores to be updated on a web form. The leaderboard can also be \"smart\" a look for the team config files on each DonkeyCar on the local-area network.","title":"Leaderboard Web Page"},{"location":"hackathon/#oled-extension","text":"Add a low-cost OLED screen to each car using the SPI bus. Have the OLED screen show key parameters such as hostname, static IP address, disk space free, training data size etc. Bonus points for a mode button to cycle through screens. See Dan McCreary for the hardware.","title":"OLED Extension"},{"location":"hackathon/#led-strips-for-training-server-status","text":"Add an low-cost WS-2811-B LED strip to the GPU server. Make the strip blue when idle, red when you start training an new model, and have it fade to green as the model converges. See Dan McCreary for the hardware.","title":"LED Strips for Training Server Status"},{"location":"hackathon/#training-graph","text":"As students walk in, give them a tablet to register. It will also ask them basic questions. It will then ask them how long they will be there. It will then suggest a set of activities and some concepts to master. The graph is a dependacy graph of all the concepts we teach at the event. Also suggest a probability they will have fun at the event.","title":"Training Graph"},{"location":"hackathon/#single-source-publishing-for-concept-cards","text":"Our cards need to be authored in MarkDown but we want to disply on the web, in PPT and with PDF. To do this we want to adopt a single-source publishing pipeline.","title":"Single Source Publishing for Concept Cards"},{"location":"learning-strategy/","text":"AI Racing League Educational Philosophy The AI Racing League educational philosophy is founded on the following values: Equal Opportunity - we work hard to keep all AI Racing League activities free and we work hard to encourage girls to be part of our programs. Student driven - we ask each student what their learning goals are and adapt to their needs Project based - we prefer to let students define their own projects and we find resources to support them. If learning about how to get a car to drive by itself that is great, but if they have other objectives we work hard to connect them with the mentor that will help them build whatever they are interested in building. Teamwork - we want students to work in teams so they learn not just about AI, but how to work with others. Flexibility - we want to be known as one of the most flexible learning organizations around. We don't force everyone to learn the same things. We take students in for 20 minutes or two years. We adapt to the needs of our students and we design our curriculum around their needs. Agility - in software there is a strong concept of \"Agile Development\" where team constantly get feedback on what works and adapt their goals every two weeks to meet the needs of their users. We use these same strategies to develop flexible content to meet the ever-changing interests of our students. Our Curriculum is based around building a series of concept cards that adhere to the \"one concept per card\" rule. Each card is a 5.5in X 8in laminated card with questions or challenges on the front and answers on the back. Concept cards have three difficulty levels with different colored borders. Green Borders - Beginner cards that anyone can start with at any time Blue Borders - Intermediate - where you may need to know some beginning concepts before you start Black Borders - Advanced - where you need to master several intermediate concepts before you take these on Our goal is to keep the concepts as \"flat\" as possible without a deep level of dependency. We try to keep at least half of our cards mostly green beginner cards. Students will walk into the AI Racing League and see a stack of cards. They will pick up one card or a set of cards and work on these. When they are done they return the cards and select another set of cards. Because of our Concept Cards in Google Docs Engineering Challenges To develop a world class curriculum, we need to partner with senior engineers and curriculum developers. Here are some of the challenges we need to address. Challenge #1: Make it easy for short term learning Engineers with experience in both hardware and software can build their own DonkeyCar from parts in a few weeks, our goal is to allow students from a wide variety of backgrounds to be able to participate in events in a flexible way. A typical CoderDojo event typically only lasts two hours and students may not have the appropriate background in hardware, Python programming or UNIX. Challenge #2: On site traning hardware Many people that are building DonkeyCars use a standard Mac or PC laptop. These systems take up to two hours to train a typical model - too long for many events. One solution would be to leverage clound-based GPUs to accelerate learning. This option typically requires transferring around 1/2 GB of images up to the clound for training the models. Models, which can typically be 10MB, then need to be transferred back from the clound to the local car. Our challenge here is that many locations may not have high-bandwith uploading and downloading services that could handle this traffic. One solution is to acquire some robust GPUs that students can use to quickly train complex models - typically in 15 to 20 minutes. This hardware needs to be easy to use - for example we need to do folder-based drag and drops and press a single button to begin training.","title":"Learning Strategy"},{"location":"learning-strategy/#ai-racing-league-educational-philosophy","text":"The AI Racing League educational philosophy is founded on the following values: Equal Opportunity - we work hard to keep all AI Racing League activities free and we work hard to encourage girls to be part of our programs. Student driven - we ask each student what their learning goals are and adapt to their needs Project based - we prefer to let students define their own projects and we find resources to support them. If learning about how to get a car to drive by itself that is great, but if they have other objectives we work hard to connect them with the mentor that will help them build whatever they are interested in building. Teamwork - we want students to work in teams so they learn not just about AI, but how to work with others. Flexibility - we want to be known as one of the most flexible learning organizations around. We don't force everyone to learn the same things. We take students in for 20 minutes or two years. We adapt to the needs of our students and we design our curriculum around their needs. Agility - in software there is a strong concept of \"Agile Development\" where team constantly get feedback on what works and adapt their goals every two weeks to meet the needs of their users. We use these same strategies to develop flexible content to meet the ever-changing interests of our students. Our Curriculum is based around building a series of concept cards that adhere to the \"one concept per card\" rule. Each card is a 5.5in X 8in laminated card with questions or challenges on the front and answers on the back. Concept cards have three difficulty levels with different colored borders. Green Borders - Beginner cards that anyone can start with at any time Blue Borders - Intermediate - where you may need to know some beginning concepts before you start Black Borders - Advanced - where you need to master several intermediate concepts before you take these on Our goal is to keep the concepts as \"flat\" as possible without a deep level of dependency. We try to keep at least half of our cards mostly green beginner cards. Students will walk into the AI Racing League and see a stack of cards. They will pick up one card or a set of cards and work on these. When they are done they return the cards and select another set of cards. Because of our Concept Cards in Google Docs","title":"AI Racing League Educational Philosophy"},{"location":"learning-strategy/#engineering-challenges","text":"To develop a world class curriculum, we need to partner with senior engineers and curriculum developers. Here are some of the challenges we need to address.","title":"Engineering Challenges"},{"location":"learning-strategy/#challenge-1-make-it-easy-for-short-term-learning","text":"Engineers with experience in both hardware and software can build their own DonkeyCar from parts in a few weeks, our goal is to allow students from a wide variety of backgrounds to be able to participate in events in a flexible way. A typical CoderDojo event typically only lasts two hours and students may not have the appropriate background in hardware, Python programming or UNIX.","title":"Challenge #1: Make it easy for short term learning"},{"location":"learning-strategy/#challenge-2-on-site-traning-hardware","text":"Many people that are building DonkeyCars use a standard Mac or PC laptop. These systems take up to two hours to train a typical model - too long for many events. One solution would be to leverage clound-based GPUs to accelerate learning. This option typically requires transferring around 1/2 GB of images up to the clound for training the models. Models, which can typically be 10MB, then need to be transferred back from the clound to the local car. Our challenge here is that many locations may not have high-bandwith uploading and downloading services that could handle this traffic. One solution is to acquire some robust GPUs that students can use to quickly train complex models - typically in 15 to 20 minutes. This hardware needs to be easy to use - for example we need to do folder-based drag and drops and press a single button to begin training.","title":"Challenge #2: On site traning hardware"},{"location":"media/","text":"Media Ready, Set, Algorithms! Teams Learn AI by Racing Cars Morningstar, Liberty Mutual workers are coming up with business ideas after exploring machine learning via mini self-driving vehicles","title":"Media"},{"location":"media/#media","text":"Ready, Set, Algorithms! Teams Learn AI by Racing Cars Morningstar, Liberty Mutual workers are coming up with business ideas after exploring machine learning via mini self-driving vehicles","title":"Media"},{"location":"presentations/","text":"AI Racing League Presentations These presentations are all licensed under our creative commons share alike non-commercial with attribution licenses. Welcome to the AI Racing League - slides used for the kickoff of a six week summer camp on building DonkeyCars AI Racing League Code Savvy - presented to out state Minnesota educators in April 2021 November 2019 Update - presentations done in November of 2019 after our fourth \"sprint\" making improvements on our process.","title":"List of Presentations"},{"location":"presentations/#ai-racing-league-presentations","text":"These presentations are all licensed under our creative commons share alike non-commercial with attribution licenses. Welcome to the AI Racing League - slides used for the kickoff of a six week summer camp on building DonkeyCars AI Racing League Code Savvy - presented to out state Minnesota educators in April 2021 November 2019 Update - presentations done in November of 2019 after our fourth \"sprint\" making improvements on our process.","title":"AI Racing League Presentations"},{"location":"resources/","text":"Community Here are some sites that are of interest: CoderDojo Twin Cities - where you can sign up to be a mentor or student Twin Cities AI Racing League Meetup Site - where we announce our public meetings DonkeyCar Hardware DonkeyCar web site Donkey Car Nano Setup Page DonkeyCar Assembly Video - Chris Anderson's detailed assembly video from 2018. Track Options Hardware Options Raspberry Pi 3, 4, the Nvidia Nano, the Nvdia DX2, and the Intel Mobius Neural Stick The base DonkeyCar today uses the Raspberry Pi 3+ which has a list price of $35. This hardware is just barly able to process images in real-time. Small changes in lighting will throw the car off the track. The new Raspberry Pi 4 with 4GB RAM is a new option. The Nvidia Nano on the other hand has 128 CUDA core processors and has more than enough power to drive around a track in real time with varied lighting conditions. This is the hardware we have used for our first generation cars in the AI Racing League. There are also college-level autonomous driving teams that use the more expensive Nvidia DX2 hardware. Nvidia Nano Jetson Nano References Joseph Bastulli PyTorch Nano Nvidia Jetson Developer Kit Nvidia Jetson Nano Kaya Video Adding a Joystick to your DonkeyCar - From Dan McCreary's Blog Videos Video of Wide Track PID Theory and Steering - why using machine learning is easier than setting PID parameters. This is covered in control theory. Real time optimal control of an autonomous RC car with minimum-time maneuvers - nice video of optimization of driving algorithm using a \"U\" shaped track. Sparkfun Autonomous Vehicle Race from 2016 Ed Murphy on Maker Faire","title":"Resources"},{"location":"resources/#community","text":"Here are some sites that are of interest: CoderDojo Twin Cities - where you can sign up to be a mentor or student Twin Cities AI Racing League Meetup Site - where we announce our public meetings","title":"Community"},{"location":"resources/#donkeycar-hardware","text":"DonkeyCar web site Donkey Car Nano Setup Page DonkeyCar Assembly Video - Chris Anderson's detailed assembly video from 2018.","title":"DonkeyCar Hardware"},{"location":"resources/#track-options","text":"","title":"Track Options"},{"location":"resources/#hardware-options","text":"Raspberry Pi 3, 4, the Nvidia Nano, the Nvdia DX2, and the Intel Mobius Neural Stick The base DonkeyCar today uses the Raspberry Pi 3+ which has a list price of $35. This hardware is just barly able to process images in real-time. Small changes in lighting will throw the car off the track. The new Raspberry Pi 4 with 4GB RAM is a new option. The Nvidia Nano on the other hand has 128 CUDA core processors and has more than enough power to drive around a track in real time with varied lighting conditions. This is the hardware we have used for our first generation cars in the AI Racing League. There are also college-level autonomous driving teams that use the more expensive Nvidia DX2 hardware.","title":"Hardware Options"},{"location":"resources/#nvidia-nano","text":"Jetson Nano References Joseph Bastulli PyTorch Nano Nvidia Jetson Developer Kit Nvidia Jetson Nano Kaya Video Adding a Joystick to your DonkeyCar - From Dan McCreary's Blog","title":"Nvidia Nano"},{"location":"resources/#videos","text":"Video of Wide Track PID Theory and Steering - why using machine learning is easier than setting PID parameters. This is covered in control theory. Real time optimal control of an autonomous RC car with minimum-time maneuvers - nice video of optimization of driving algorithm using a \"U\" shaped track. Sparkfun Autonomous Vehicle Race from 2016 Ed Murphy on Maker Faire","title":"Videos"},{"location":"six-week-curriculum/","text":"Sample Six Week Curriculum This is a sample suggested curriculum for a six week AI Racing League summer school project. The students would all meet together for two hours, once a week. There are then homework assignments. The students don't need any prior experience. Week 1: Overview and Unboxing Slides: Welcome to the AI Racing League? Link to Slides What is the DonkeyCar? Lab: Unbox the car (requires tools such as cable tie cutter and screwdrivers) What is AI? What is Machine Learning? What is Python? Introduction to Python course Motors and servos (demo of car driving with the motors and servos being controlled by RC) Make sure students know how to turn on the ESC and listen for the startup beep sound See the suggested parts list for week 1 Week 2: Booting a Raspberry Pi, UNIX, Calibration, Intro to Python and Raspberry Pi Booting a Raspberry Pi from Micro SD card What is a Raspberry Pi? How much RAM do we need? Why is 4GB important for the AI Racing League? What is a micro SD card? How big a card do we need? 32GB vs 65GB vs 128GB Can we train our model on a Pi? Training vs. Inference - when do we need a GPU? What is an Operating System Image file? How do we create an image file? Download a Raspberry Pi image Raspberry Pi Imager Burn a microSD card with that image - include customization Use the microSD card to boot your Raspberry Pi (requires 4GB Raspberry Pi Pico, keyboard, mouse, power supply, monitor) Configure Pi desktop - learn how to use menus, add bookmarks to the web browser, manage bookmarks Start Python IDE Run \"hello world\" in Python Open a Terminal and type \"ls\" Download the DonkeyCar software Get familiar with the folder layout Verify the connections from the Pi to the PWM card and the DonkeyCar Run the calibration command, write down the numbers for throttle and steering Week 3: Python, Configuration, Drive More Python labs - get as far as possible through the Introduction to Python class Get familiar with the Donkey Car configuration file Focus on the key parameters for calibration Find the Drive command Discuss options for controlling the car: Joystick vs Web Application Backup Career Exploration: What is a Software Engineer? Backup Lab: Google Teachable Machines Week 4: Gather Image Data and Analyze Quality with Jupyter Notebooks Drive around the track and gather image data Look at the image data in the tubs Run a basic Python program to count the number of files Learn about a Jupyter Notebook Backup Career Exploration: What is a Data Scientist? Week 5: The GPU and Training Learn about the GPU - what are GPU cores? - Why is training time faster? What is a conda environment for Python? What is Miniconda Download here Activating conda environments Verifying that the GPU setting are correct Run a test program on the GPU Learn how to transfer files from the car's memory to the GPU (compress tubs, copy to jump drive) What is a model file? How big is your model? What are model parameters? Backup: What is Bias in AI? How to we detect it? How dow we measure it? Week 6: Using the Model to Drive Autonomously Put the model file on the Donkey car Run the drive with model command Change the configuration files Evaluate image quality","title":"Sample Six Week Course"},{"location":"six-week-curriculum/#sample-six-week-curriculum","text":"This is a sample suggested curriculum for a six week AI Racing League summer school project. The students would all meet together for two hours, once a week. There are then homework assignments. The students don't need any prior experience.","title":"Sample Six Week Curriculum"},{"location":"six-week-curriculum/#week-1-overview-and-unboxing","text":"Slides: Welcome to the AI Racing League? Link to Slides What is the DonkeyCar? Lab: Unbox the car (requires tools such as cable tie cutter and screwdrivers) What is AI? What is Machine Learning? What is Python? Introduction to Python course Motors and servos (demo of car driving with the motors and servos being controlled by RC) Make sure students know how to turn on the ESC and listen for the startup beep sound See the suggested parts list for week 1","title":"Week 1: Overview and Unboxing"},{"location":"six-week-curriculum/#week-2-booting-a-raspberry-pi-unix-calibration-intro-to-python-and-raspberry-pi","text":"Booting a Raspberry Pi from Micro SD card What is a Raspberry Pi? How much RAM do we need? Why is 4GB important for the AI Racing League? What is a micro SD card? How big a card do we need? 32GB vs 65GB vs 128GB Can we train our model on a Pi? Training vs. Inference - when do we need a GPU? What is an Operating System Image file? How do we create an image file? Download a Raspberry Pi image Raspberry Pi Imager Burn a microSD card with that image - include customization Use the microSD card to boot your Raspberry Pi (requires 4GB Raspberry Pi Pico, keyboard, mouse, power supply, monitor) Configure Pi desktop - learn how to use menus, add bookmarks to the web browser, manage bookmarks Start Python IDE Run \"hello world\" in Python Open a Terminal and type \"ls\" Download the DonkeyCar software Get familiar with the folder layout Verify the connections from the Pi to the PWM card and the DonkeyCar Run the calibration command, write down the numbers for throttle and steering","title":"Week 2: Booting a Raspberry Pi, UNIX, Calibration, Intro to Python and Raspberry Pi"},{"location":"six-week-curriculum/#week-3-python-configuration-drive","text":"More Python labs - get as far as possible through the Introduction to Python class Get familiar with the Donkey Car configuration file Focus on the key parameters for calibration Find the Drive command Discuss options for controlling the car: Joystick vs Web Application Backup Career Exploration: What is a Software Engineer? Backup Lab: Google Teachable Machines","title":"Week 3: Python, Configuration, Drive"},{"location":"six-week-curriculum/#week-4-gather-image-data-and-analyze-quality-with-jupyter-notebooks","text":"Drive around the track and gather image data Look at the image data in the tubs Run a basic Python program to count the number of files Learn about a Jupyter Notebook Backup Career Exploration: What is a Data Scientist?","title":"Week 4: Gather Image Data and Analyze Quality with Jupyter Notebooks"},{"location":"six-week-curriculum/#week-5-the-gpu-and-training","text":"Learn about the GPU - what are GPU cores? - Why is training time faster? What is a conda environment for Python? What is Miniconda Download here Activating conda environments Verifying that the GPU setting are correct Run a test program on the GPU Learn how to transfer files from the car's memory to the GPU (compress tubs, copy to jump drive) What is a model file? How big is your model? What are model parameters? Backup: What is Bias in AI? How to we detect it? How dow we measure it?","title":"Week 5: The GPU and Training"},{"location":"six-week-curriculum/#week-6-using-the-model-to-drive-autonomously","text":"Put the model file on the Donkey car Run the drive with model command Change the configuration files Evaluate image quality","title":"Week 6: Using the Model to Drive Autonomously"},{"location":"admin/01-intro/","text":"League Administers Page SD Image This section show how leage administrators can create their own SD image files.","title":"Introduction"},{"location":"admin/01-intro/#league-administers-page","text":"","title":"League Administers Page"},{"location":"admin/01-intro/#sd-image","text":"This section show how leage administrators can create their own SD image files.","title":"SD Image"},{"location":"admin/02-sd-image/","text":"Creating a League SD Image Many times teams will not have the time to build their own image during the time allocated for an event. It typically takes 2-4 hours to create a DonkeyCar image that is ready to drive. To get around this problem, leagues frequently create their own \"reference image\" that are given to teams. Checklist for the League Image Bookmark bar in the browser is visible (check Show Bookmarks Bar) Bookmark bar is populated with: You League Homepage (use GitHub Pages and mkdocs for best pratices) Links to Team pages with sample myconfig.py files for each team's car Link to the AI Racing League site (https://www.coderdojotc.org/ai-racing-league/) Links to the DonkeyCar site Links to the Raspberry Pi or NVIDIA Nano site All operating system files updates before match Auto-update disabled - you don't what gigabyte uploads when people insert their image All Python libraries Updates Verify Python release such as Python 3.7 using python --version Run \"pip freeze\" to get a list of the libraries you have tested on All DonkeyCar libraries updated In the \"donkeycar\" repo run a \"git pull\" to update the latest code Make sure you ONLY change the myconfig.py - changes to config.py will be lost ## Things to Remove from your Image Personal information (from your github file in .gitinfo) Change Hostname to be Generic (arl) Remove your home or school default wifi settings Burning an League Reference Image Take the image out of your car Take it to a Mac/PC and copy the image to your harddrive using the dd Command Insert a black MicroSD card Copy the image on your PC's Harddrive to the new MicroSD Using DD or a GUI tool like belana Etcher Using the GNome Partition Editor (gpartd) Reference DonkeyCar Release Process","title":"Custom Club Images"},{"location":"admin/02-sd-image/#creating-a-league-sd-image","text":"Many times teams will not have the time to build their own image during the time allocated for an event. It typically takes 2-4 hours to create a DonkeyCar image that is ready to drive. To get around this problem, leagues frequently create their own \"reference image\" that are given to teams.","title":"Creating a League SD Image"},{"location":"admin/02-sd-image/#checklist-for-the-league-image","text":"Bookmark bar in the browser is visible (check Show Bookmarks Bar) Bookmark bar is populated with: You League Homepage (use GitHub Pages and mkdocs for best pratices) Links to Team pages with sample myconfig.py files for each team's car Link to the AI Racing League site (https://www.coderdojotc.org/ai-racing-league/) Links to the DonkeyCar site Links to the Raspberry Pi or NVIDIA Nano site All operating system files updates before match Auto-update disabled - you don't what gigabyte uploads when people insert their image All Python libraries Updates Verify Python release such as Python 3.7 using python --version Run \"pip freeze\" to get a list of the libraries you have tested on All DonkeyCar libraries updated In the \"donkeycar\" repo run a \"git pull\" to update the latest code Make sure you ONLY change the myconfig.py - changes to config.py will be lost ## Things to Remove from your Image Personal information (from your github file in .gitinfo) Change Hostname to be Generic (arl) Remove your home or school default wifi settings","title":"Checklist for the League Image"},{"location":"admin/02-sd-image/#burning-an-league-reference-image","text":"Take the image out of your car Take it to a Mac/PC and copy the image to your harddrive using the dd Command Insert a black MicroSD card Copy the image on your PC's Harddrive to the new MicroSD Using DD or a GUI tool like belana Etcher","title":"Burning an League Reference Image"},{"location":"admin/02-sd-image/#using-the-gnome-partition-editor-gpartd","text":"","title":"Using the GNome Partition Editor (gpartd)"},{"location":"admin/02-sd-image/#reference","text":"DonkeyCar Release Process","title":"Reference"},{"location":"admin/03-purchasing-guide/","text":"AI Racing League Purchasing Guide DonkeyCars Microcontrollers Raspberry Pi NVIDIA Nano GPUs !!! Note The AI Racing League ONLY uses this for training our models. We don't need elaborate CPU overclocking and a water cooled CPU. We don't need powerful CPU and lots of RAM. We just need to be able to train a 20K image model within around 5-10 minutes. Most GPUs can do this. Portable Case We wanted a small lightweight case with a handle and tempered glass sides so our teams can see what is inside. The price is around $110.00. Lian Li TU150 Mini ITX Desktop Case Motherboard RAM GPU Solid State Drive","title":"Purchasing Guide"},{"location":"admin/03-purchasing-guide/#ai-racing-league-purchasing-guide","text":"","title":"AI Racing League Purchasing Guide"},{"location":"admin/03-purchasing-guide/#donkeycars","text":"","title":"DonkeyCars"},{"location":"admin/03-purchasing-guide/#microcontrollers","text":"","title":"Microcontrollers"},{"location":"admin/03-purchasing-guide/#raspberry-pi","text":"","title":"Raspberry Pi"},{"location":"admin/03-purchasing-guide/#nvidia-nano","text":"","title":"NVIDIA Nano"},{"location":"admin/03-purchasing-guide/#gpus","text":"!!! Note The AI Racing League ONLY uses this for training our models. We don't need elaborate CPU overclocking and a water cooled CPU. We don't need powerful CPU and lots of RAM. We just need to be able to train a 20K image model within around 5-10 minutes. Most GPUs can do this.","title":"GPUs"},{"location":"admin/03-purchasing-guide/#portable-case","text":"We wanted a small lightweight case with a handle and tempered glass sides so our teams can see what is inside. The price is around $110.00. Lian Li TU150 Mini ITX Desktop Case","title":"Portable Case"},{"location":"admin/03-purchasing-guide/#motherboard","text":"","title":"Motherboard"},{"location":"admin/03-purchasing-guide/#ram","text":"","title":"RAM"},{"location":"admin/03-purchasing-guide/#gpu","text":"","title":"GPU"},{"location":"admin/03-purchasing-guide/#solid-state-drive","text":"","title":"Solid State Drive"},{"location":"admin/04-tools/","text":"AI Racing League Tools Here are some handy tools to help you with your cars. USB Voltage Current Meter Many USB battery packs don't maintain a voltage of 5.1v when under load. To monitor this, you need to watch the voltage level that arrives at the single-board computer input voltage. This needs to include any voltage drop over the USB power cable. Sample item on e-Bay This item uses a four semi-double loop integral converter ICs, with a precision bandgap reference. The vendor claims the device has: range accuracy and high precision stable and reliable performance strong anti~interference ability tolarance for high temperatures Specifications: Current range: 0~5A Voltage range : 3 ~ 7.5V Resolution : Voltage 10mV, current 1mA Error: voltage <\u00b1 1% current <\u00b1 2% Voltage drop when measuring current: 200mV Note the voltage drop when current is passing through the device. In practice, the exact voltage drop varies with the current. For a typical project drawing 300 milliamps, the voltage drop is around 100vV.","title":"AI Racing League Tools"},{"location":"admin/04-tools/#ai-racing-league-tools","text":"Here are some handy tools to help you with your cars.","title":"AI Racing League Tools"},{"location":"admin/04-tools/#usb-voltage-current-meter","text":"Many USB battery packs don't maintain a voltage of 5.1v when under load. To monitor this, you need to watch the voltage level that arrives at the single-board computer input voltage. This needs to include any voltage drop over the USB power cable. Sample item on e-Bay This item uses a four semi-double loop integral converter ICs, with a precision bandgap reference. The vendor claims the device has: range accuracy and high precision stable and reliable performance strong anti~interference ability tolarance for high temperatures Specifications: Current range: 0~5A Voltage range : 3 ~ 7.5V Resolution : Voltage 10mV, current 1mA Error: voltage <\u00b1 1% current <\u00b1 2% Voltage drop when measuring current: 200mV Note the voltage drop when current is passing through the device. In practice, the exact voltage drop varies with the current. For a typical project drawing 300 milliamps, the voltage drop is around 100vV.","title":"USB Voltage Current Meter"},{"location":"admin/car-box-checklist/","text":"AI Racing League Car Box Checklist Donkey Car NVIDIA Kit Car Name: _ _ _ _ Mac Address: _ _ _ _ Static IP Address: _ _ _ [ ] RC Car with the following components [ ] RC Car Battery Charger (7.2v NiMh) [ ] Pi Camera Module V2 with 3D printed chassis [ ] 128GB micro SD card (inserted in Nvidia Nano) (2) [ ] RC Car Battery (7.2v NiMh) [ ] Nvidia Nano with 4GB RAM [ ] Ankar 5v 6800mHA battery with charging cable - note draws 900ma when charging so use a 1ft high current USB cable. [ ] 2.5 amp 5v barrel connector for desktop use of Nvidia [ ] Jumper for enabling the barrele connector [ ] WiFi Dongle (plugged into the car) or AC8265 [ ] Logitech F710 Joystick (dongle plugged into the car) - names on both Joystick and dongle [ ] Mouse Optional Accessories (not in the box) 1. [ ] Keyboard 1. [ ] External Monitor Nvidia Nano Serial Number: _ _ _ _____ Nvidia Nano Purchase Date: December 12, 2019 Raspberry Pi DonkeyCar Kit [ ] RC Car with separate ESC and Servo Connectors [ ] RC Car Battery Charger (7.2v NiMh) [ ] 7.2 volt battery with Tamiya connector [ ] 3D Printed Chassis [ ] with 3 large screws for roll-bar to base [ ] 8 smaller screws for attaching the PI and PWM board to base [ ] Raspberry Pi [ ] Board with 4GB RAM [ ] 3A Power with USB C Connector [ ] Keyboard [ ] Mouse [ ] VGA Connector [ ] PWM Board [ ] Camera [ ] 4 Female-Female Dupont Connectors [ ] USB Battery Pack (6,000 to 10,000 milliamp hours) [ ] 1 ft USB A to USB C [ ] Extra 7.2 volt battery for RC Car with mini Tamiya Miscellaneous Parts Digital Volt-Ohm Meter ($10) Extra Tamiya Style Connectors Amazon Link","title":"Car Box Checklist"},{"location":"admin/car-box-checklist/#ai-racing-league-car-box-checklist","text":"","title":"AI Racing League Car Box Checklist"},{"location":"admin/car-box-checklist/#donkey-car-nvidia-kit","text":"Car Name: _ _ _ _ Mac Address: _ _ _ _ Static IP Address: _ _ _ [ ] RC Car with the following components [ ] RC Car Battery Charger (7.2v NiMh) [ ] Pi Camera Module V2 with 3D printed chassis [ ] 128GB micro SD card (inserted in Nvidia Nano) (2) [ ] RC Car Battery (7.2v NiMh) [ ] Nvidia Nano with 4GB RAM [ ] Ankar 5v 6800mHA battery with charging cable - note draws 900ma when charging so use a 1ft high current USB cable. [ ] 2.5 amp 5v barrel connector for desktop use of Nvidia [ ] Jumper for enabling the barrele connector [ ] WiFi Dongle (plugged into the car) or AC8265 [ ] Logitech F710 Joystick (dongle plugged into the car) - names on both Joystick and dongle [ ] Mouse Optional Accessories (not in the box) 1. [ ] Keyboard 1. [ ] External Monitor Nvidia Nano Serial Number: _ _ _ _____ Nvidia Nano Purchase Date: December 12, 2019","title":"Donkey Car NVIDIA Kit"},{"location":"admin/car-box-checklist/#raspberry-pi-donkeycar-kit","text":"[ ] RC Car with separate ESC and Servo Connectors [ ] RC Car Battery Charger (7.2v NiMh) [ ] 7.2 volt battery with Tamiya connector [ ] 3D Printed Chassis [ ] with 3 large screws for roll-bar to base [ ] 8 smaller screws for attaching the PI and PWM board to base [ ] Raspberry Pi [ ] Board with 4GB RAM [ ] 3A Power with USB C Connector [ ] Keyboard [ ] Mouse [ ] VGA Connector [ ] PWM Board [ ] Camera [ ] 4 Female-Female Dupont Connectors [ ] USB Battery Pack (6,000 to 10,000 milliamp hours) [ ] 1 ft USB A to USB C [ ] Extra 7.2 volt battery for RC Car with mini Tamiya","title":"Raspberry Pi DonkeyCar Kit"},{"location":"admin/car-box-checklist/#miscellaneous-parts","text":"Digital Volt-Ohm Meter ($10) Extra Tamiya Style Connectors Amazon Link","title":"Miscellaneous Parts"},{"location":"admin/car-parts-list/","text":"DonkeyCar Nvidia Nano Parts List We have looked at many variations of parts and decided to go with the Nvidia Nano, a TP-Link WiFi dongle and the Logitech F710 Joystick. Here are our recomended parts. We are also looking into getting the wide-angle (160 degree) cameras but we have not tested these enough. Part Name Description Price Link Note 128GB microSD card Samsung 128GB 100MB/s (U3) MicroSDXC Evo Select Memory Card with Adapter (MB-ME128GA/AM) $20 https://www.amazon.com/Samsung-MicroSD-Adapter-MB-ME128GA-AM/dp/B06XWZWYVP MicroCenter in St. Louis Park has these for about 1/2 the prices Camera Raspberry Pi Camera Module V2-8 Megapixel,1080p $30 https://www.amazon.com/Raspberry-Pi-Camera-Module-Megapixel/dp/B01ER2SKFS MUST be Module V2. The V1 will NOT work with the Nano. Dupont Connectors (F-F) EDGELEC 120pcs 20cm Dupont Wire Female to Female Breadboard Jumper Wires 3.9 inch 1pin-1pin 2.54mm Connector Multicolored Ribbon Cables DIY Arduino Wires 10 15 20 30 40 50 100cm Optional $8 for 120 https://www.amazon.com/EDGELEC-Breadboard-1pin-1pin-Connector-Multicolored/dp/B07GCY6CH7 Only need one of these Nvidia Nano Single Board Computer NVIDIA Jetson Nano Developer Kit $99 https://www.amazon.com/NVIDIA-Jetson-Nano-Developer-Kit/dp/B07PZHBDKT Ships in two days Power for Pi - 6700mAh Anker [Upgraded to 6700mAh] Astro E1 Candy-Bar Sized Ultra Compact Portable Charger, External Battery Power Bank, with High-Speed Charging PowerIQ Technology $24 https://www.amazon.com/Anker-Upgraded-Candy-Bar-High-Speed-Technology/dp/B06XS9RMWS I like this one but there are other variations. Some are rated at 10,000 mAh Power Supply for Nano SMAKN DC 5V/4A 20W Switching Power Supply Adapter 100-240 Ac(US) $10 https://www.amazon.com/SMAKN-Switching-Supply-Adapter-100-240/dp/B01N4HYWAM Note that this is a 4A 12V power supply. RC Car 1/16 2.4Ghz Exceed RC Magnet Electric Powered RTR Off Road Truck Stripe Blue NEW $119 https://www.ebay.com/itm/1-16-2-4Ghz-Exceed-RC-Magnet-Electric-Powered-RTR-Off-Road-Truck-Stripe-Blue-NEW/223337258165 E-Bay Wifi USB Dongle N150 USB wireless WiFi network Adapter for PC with SoftAP Mode - Nano Size, Compatible with Linux Kernal 2.6.18~4.4.3 (TL-WN725N) $7 https://www.amazon.com/TP-Link-TL-WN725N-wireless-network-Adapter/dp/B008IFXQFU/ I purchased one at Microcenter and it worked out-of-the-box on the Nano. The Ubuntu drivers are pre-loaded! Servo Module HiLetgo 2pcs PCA9685 16 Channel 12-Bit PWM Servo Motor Driver IIC Module for Arduino Robot $10 for 2 https://www.amazon.com/gp/product/B07BRS249H/ref=ppx_yo_dt_b_asin_title_o00_s00?ie=UTF8&psc=1 Note the quantity is 2 USB Power Cable Anker [4-Pack] Powerline Micro USB (1ft) - Charging Cable $10 for 4 https://www.amazon.com/gp/product/B015XR60MQ/ref=ppx_yo_dt_b_asin_title_o02_s00 Note the quantity is 4 but you only need one Replacement Battery 7.2V 1100mAh 6x 2/3A Rechargeable Ni-MH RC Battery Pack w/Small Tamiya Connector 10cmX3cmX1.5cm $9.88 + $2.39 Shipping https://www.ebay.com/i/183877810537 Takes several weeks to ship from China. We are looking for a local supplier. Some replacements (Airsoft guns) have reverse polarity.","title":"Cars Parts List"},{"location":"admin/car-parts-list/#donkeycar-nvidia-nano-parts-list","text":"We have looked at many variations of parts and decided to go with the Nvidia Nano, a TP-Link WiFi dongle and the Logitech F710 Joystick. Here are our recomended parts. We are also looking into getting the wide-angle (160 degree) cameras but we have not tested these enough. Part Name Description Price Link Note 128GB microSD card Samsung 128GB 100MB/s (U3) MicroSDXC Evo Select Memory Card with Adapter (MB-ME128GA/AM) $20 https://www.amazon.com/Samsung-MicroSD-Adapter-MB-ME128GA-AM/dp/B06XWZWYVP MicroCenter in St. Louis Park has these for about 1/2 the prices Camera Raspberry Pi Camera Module V2-8 Megapixel,1080p $30 https://www.amazon.com/Raspberry-Pi-Camera-Module-Megapixel/dp/B01ER2SKFS MUST be Module V2. The V1 will NOT work with the Nano. Dupont Connectors (F-F) EDGELEC 120pcs 20cm Dupont Wire Female to Female Breadboard Jumper Wires 3.9 inch 1pin-1pin 2.54mm Connector Multicolored Ribbon Cables DIY Arduino Wires 10 15 20 30 40 50 100cm Optional $8 for 120 https://www.amazon.com/EDGELEC-Breadboard-1pin-1pin-Connector-Multicolored/dp/B07GCY6CH7 Only need one of these Nvidia Nano Single Board Computer NVIDIA Jetson Nano Developer Kit $99 https://www.amazon.com/NVIDIA-Jetson-Nano-Developer-Kit/dp/B07PZHBDKT Ships in two days Power for Pi - 6700mAh Anker [Upgraded to 6700mAh] Astro E1 Candy-Bar Sized Ultra Compact Portable Charger, External Battery Power Bank, with High-Speed Charging PowerIQ Technology $24 https://www.amazon.com/Anker-Upgraded-Candy-Bar-High-Speed-Technology/dp/B06XS9RMWS I like this one but there are other variations. Some are rated at 10,000 mAh Power Supply for Nano SMAKN DC 5V/4A 20W Switching Power Supply Adapter 100-240 Ac(US) $10 https://www.amazon.com/SMAKN-Switching-Supply-Adapter-100-240/dp/B01N4HYWAM Note that this is a 4A 12V power supply. RC Car 1/16 2.4Ghz Exceed RC Magnet Electric Powered RTR Off Road Truck Stripe Blue NEW $119 https://www.ebay.com/itm/1-16-2-4Ghz-Exceed-RC-Magnet-Electric-Powered-RTR-Off-Road-Truck-Stripe-Blue-NEW/223337258165 E-Bay Wifi USB Dongle N150 USB wireless WiFi network Adapter for PC with SoftAP Mode - Nano Size, Compatible with Linux Kernal 2.6.18~4.4.3 (TL-WN725N) $7 https://www.amazon.com/TP-Link-TL-WN725N-wireless-network-Adapter/dp/B008IFXQFU/ I purchased one at Microcenter and it worked out-of-the-box on the Nano. The Ubuntu drivers are pre-loaded! Servo Module HiLetgo 2pcs PCA9685 16 Channel 12-Bit PWM Servo Motor Driver IIC Module for Arduino Robot $10 for 2 https://www.amazon.com/gp/product/B07BRS249H/ref=ppx_yo_dt_b_asin_title_o00_s00?ie=UTF8&psc=1 Note the quantity is 2 USB Power Cable Anker [4-Pack] Powerline Micro USB (1ft) - Charging Cable $10 for 4 https://www.amazon.com/gp/product/B015XR60MQ/ref=ppx_yo_dt_b_asin_title_o02_s00 Note the quantity is 4 but you only need one Replacement Battery 7.2V 1100mAh 6x 2/3A Rechargeable Ni-MH RC Battery Pack w/Small Tamiya Connector 10cmX3cmX1.5cm $9.88 + $2.39 Shipping https://www.ebay.com/i/183877810537 Takes several weeks to ship from China. We are looking for a local supplier. Some replacements (Airsoft guns) have reverse polarity.","title":"DonkeyCar Nvidia Nano Parts List"},{"location":"admin/gpu-parts/","text":"AI Racing League GPU Components Design Goals We wanted to create a local training system that had fast training times but was portable so that we can easily carry it in a car and ship it to remote events. We can't assume any connectivity to the Internet for our events since some of them might be held in parking lots with no network access. Here are our design objectives. We also drive to remote events and the equipment needs to be outside overnight in freezing weather. This rules out using any water-cooled hardware which gets easily damaged in freezing weather. Fast Training Times We want students to be able to drive around a track 20 times (10 times clockwise and 10 times counterclockwise) and generate a reasonable sized data set of 20 frames per second and 224X224 images. This ends up being about 10,000 images. The sizes are a bit larger for larger tracks and slower drivers. Why We Like the NVIDIA RTX 2070 We want to train with this data set in under five minutes. This means that we want to use a GPU card that has about 2000 CUDA cores. An example of this is the Nvidia GeForce GTX graphic cards. The RTX 2070 which currently has a list price of around $500. There are many people that are upgrading their video game systems and are selling these GPUs used on eBay and Craigslist.com for a few hundred dollars. A higher cost option is the NVIDIA RTX 2080 which has a retail list price of around $1,200 USD. The benchmarks for image training for these two boards were done by Dr Donald Kinghorn in March of 2019. [His analysis] (https://www.pugetsystems.com/labs/hpc/TensorFlow-Performance-with-1-4-GPUs----RTX-Titan-2080Ti-2080-2070-GTX-1660Ti-1070-1080Ti-and-Titan-V-1386/) shows that a single GTX 2080 Ti can process about 293 images per second. The GTX 2070 only does about 191 images per second. But for about 1/3 of the price it is still a good value. Small and Lightweight We originally were \"gifted\" a somewhat old GPU server used in a data center for training deep learning models. Although the sever was \"free\", it was over 70 pounds and had far more capability for RAM and power then we needed at events. Based in this experience we opted to build a much smaller system using a mini enclosure with a handle. We selected the Mini ITX Desktop Case and determined that we could still fit the GPU in this case. Rugged Must be able to take the bumps of shipping and be able to be left out in a car overnight in freezing temperatures. This was a requirement for remote events in rural Minnesota communities. We opted for a full SSD drive to keep the moving parts to a minimum. Easy to ship to remote sites We had to be able to put the unit is a remote shipping case. We are still looking for low-cost cases that are lightweight but protective. Visibility We wanted students to be able to look into the case and see the parts. There is a trend to also purchase RGB LED versions of components which we thought we could program to change from RED to Green during the training process as the model converges. We have not found a good API for the parts so a simple $5 LED strip on a Arduino Nano might be a better idea. See the Moving Rainbow project for sample designs. We create these at the IoT hackthons each year. Sample Parts List 2023 Update PCPartPicker Part List $769 with Monitor by Neal Kelly Part Name Description Price Link CPU AMD Ryzen 5 3600 3.6 GHz 6-Core Processor $95.00 Amazon Motherboard MSI A520M-A PRO Micro ATX AM4 Motherboard $101.11 Amazon Memory Silicon Power SP016GBLFU320X02 16 GB (1 x 16 GB) DDR4-3200 CL22 Memory $23.99 Amazon Storage TEAMGROUP MP33 512 GB M.2-2280 PCIe 3.0 X4 NVME Solid State Drive $22.49 Amazon Video Card Asus Dual GeForce RTX 3060 V2 OC Edition GeForce RTX 3060 12GB 12 GB Video Card $299.99 Amazon Case Thermaltake Versa H18 MicroATX Mini Tower Case $49.99 Amazon Power Supply be quiet! Pure Power 11 400 W 80+ Gold Certified ATX Power Supply $89.69 Amazon Monitor Acer V227Q Abmix 21.5\" 1920 x 1080 75 Hz Monitor $87.29 Amazon Total $769.55 Part Name Description Price Link Note CPU AMD Ryzen 5 3600 3.6 GHz 6-Core Processor $189.99 Motherboard Gigabyte X570 I AORUS PRO WIFI Mini ITX AM4 $219.99 RAM Corsair Vengeance RGB Pro 32 GB (2 x 16 GB) DDR4-3200 Memory $162.99 Link Notes Storage Gigabyte AORUS NVMe Gen4 1 TB M.2-2280 NVME Solid State Drive $209.99 Link Notes Cooling Be quiet! Dark Rock Pro 4, BK022, 250W TDP $89.90 https://www.amazon.com/dp/B07BY6F8D9/ref=cm_sw_r_cp_api_i_PYp-DbFCY51CH Avoid liquid cooler GPU Card NVIDIA GeForce RTX 2070 Ti 8 GB $499.99 https://www.nvidia.com/en-us/geforce/graphics-cards/rtx-2070-super/ $500 price is a lower cost alternative Case Lian Li TU150 Mini ITX Desktop Case $109.99 Link We love the handle on this small case and the glass side panel. Power Supply Corsair SF 600W 80+ Gold SFX Power Supply $114.99 Link 600W is an overkill Note that this motherboard does come with builtin WiFi. The external antenna must be connected but it is easy to get lost in transport. You might want to get a few additional WiFi antennas like these RP-SMA Male Antenna We also think we could get buy with a smaller and lighter power supply, but the 600W model gives the system the opportunity to add external devices that might draw more power. Assembly There are several good videos on YouTube that show how to assemble custom systems. You can also use a search engine to find videos for each of the parts. The Liquid coolers can be tricky to install correctly if you don't have experience. We also recommend reading the user manauals for each of the parts. They are usually on line. Jon Herke's Tiny Monster Installing NVIDIA Drivers on Ubuntu Installing NVIDIA drivers on Ubuntu is notoriously painful and difficult. One mis-step and you can't get to the monitor and have to ssh in to fix things. Make sure to setup ssh before you install the NVIDIA drivers. We used the UNIX command line to install the NVIDIA drivers. The GUI tool on Ubuntu did not work for us in some settings. See NVIDIA Driver Install . A guide to do this is here: Installation of Nvidia Drivers on Ubuntu 18","title":"GPU Parts List"},{"location":"admin/gpu-parts/#ai-racing-league-gpu-components","text":"","title":"AI Racing League GPU Components"},{"location":"admin/gpu-parts/#design-goals","text":"We wanted to create a local training system that had fast training times but was portable so that we can easily carry it in a car and ship it to remote events. We can't assume any connectivity to the Internet for our events since some of them might be held in parking lots with no network access. Here are our design objectives. We also drive to remote events and the equipment needs to be outside overnight in freezing weather. This rules out using any water-cooled hardware which gets easily damaged in freezing weather.","title":"Design Goals"},{"location":"admin/gpu-parts/#fast-training-times","text":"We want students to be able to drive around a track 20 times (10 times clockwise and 10 times counterclockwise) and generate a reasonable sized data set of 20 frames per second and 224X224 images. This ends up being about 10,000 images. The sizes are a bit larger for larger tracks and slower drivers.","title":"Fast Training Times"},{"location":"admin/gpu-parts/#why-we-like-the-nvidia-rtx-2070","text":"We want to train with this data set in under five minutes. This means that we want to use a GPU card that has about 2000 CUDA cores. An example of this is the Nvidia GeForce GTX graphic cards. The RTX 2070 which currently has a list price of around $500. There are many people that are upgrading their video game systems and are selling these GPUs used on eBay and Craigslist.com for a few hundred dollars. A higher cost option is the NVIDIA RTX 2080 which has a retail list price of around $1,200 USD. The benchmarks for image training for these two boards were done by Dr Donald Kinghorn in March of 2019. [His analysis] (https://www.pugetsystems.com/labs/hpc/TensorFlow-Performance-with-1-4-GPUs----RTX-Titan-2080Ti-2080-2070-GTX-1660Ti-1070-1080Ti-and-Titan-V-1386/) shows that a single GTX 2080 Ti can process about 293 images per second. The GTX 2070 only does about 191 images per second. But for about 1/3 of the price it is still a good value.","title":"Why We Like the NVIDIA RTX 2070"},{"location":"admin/gpu-parts/#small-and-lightweight","text":"We originally were \"gifted\" a somewhat old GPU server used in a data center for training deep learning models. Although the sever was \"free\", it was over 70 pounds and had far more capability for RAM and power then we needed at events. Based in this experience we opted to build a much smaller system using a mini enclosure with a handle. We selected the Mini ITX Desktop Case and determined that we could still fit the GPU in this case.","title":"Small and Lightweight"},{"location":"admin/gpu-parts/#rugged","text":"Must be able to take the bumps of shipping and be able to be left out in a car overnight in freezing temperatures. This was a requirement for remote events in rural Minnesota communities. We opted for a full SSD drive to keep the moving parts to a minimum.","title":"Rugged"},{"location":"admin/gpu-parts/#easy-to-ship-to-remote-sites","text":"We had to be able to put the unit is a remote shipping case. We are still looking for low-cost cases that are lightweight but protective.","title":"Easy to ship to remote sites"},{"location":"admin/gpu-parts/#visibility","text":"We wanted students to be able to look into the case and see the parts. There is a trend to also purchase RGB LED versions of components which we thought we could program to change from RED to Green during the training process as the model converges. We have not found a good API for the parts so a simple $5 LED strip on a Arduino Nano might be a better idea. See the Moving Rainbow project for sample designs. We create these at the IoT hackthons each year.","title":"Visibility"},{"location":"admin/gpu-parts/#sample-parts-list","text":"","title":"Sample Parts List"},{"location":"admin/gpu-parts/#2023-update","text":"PCPartPicker Part List $769 with Monitor by Neal Kelly Part Name Description Price Link CPU AMD Ryzen 5 3600 3.6 GHz 6-Core Processor $95.00 Amazon Motherboard MSI A520M-A PRO Micro ATX AM4 Motherboard $101.11 Amazon Memory Silicon Power SP016GBLFU320X02 16 GB (1 x 16 GB) DDR4-3200 CL22 Memory $23.99 Amazon Storage TEAMGROUP MP33 512 GB M.2-2280 PCIe 3.0 X4 NVME Solid State Drive $22.49 Amazon Video Card Asus Dual GeForce RTX 3060 V2 OC Edition GeForce RTX 3060 12GB 12 GB Video Card $299.99 Amazon Case Thermaltake Versa H18 MicroATX Mini Tower Case $49.99 Amazon Power Supply be quiet! Pure Power 11 400 W 80+ Gold Certified ATX Power Supply $89.69 Amazon Monitor Acer V227Q Abmix 21.5\" 1920 x 1080 75 Hz Monitor $87.29 Amazon Total $769.55 Part Name Description Price Link Note CPU AMD Ryzen 5 3600 3.6 GHz 6-Core Processor $189.99 Motherboard Gigabyte X570 I AORUS PRO WIFI Mini ITX AM4 $219.99 RAM Corsair Vengeance RGB Pro 32 GB (2 x 16 GB) DDR4-3200 Memory $162.99 Link Notes Storage Gigabyte AORUS NVMe Gen4 1 TB M.2-2280 NVME Solid State Drive $209.99 Link Notes Cooling Be quiet! Dark Rock Pro 4, BK022, 250W TDP $89.90 https://www.amazon.com/dp/B07BY6F8D9/ref=cm_sw_r_cp_api_i_PYp-DbFCY51CH Avoid liquid cooler GPU Card NVIDIA GeForce RTX 2070 Ti 8 GB $499.99 https://www.nvidia.com/en-us/geforce/graphics-cards/rtx-2070-super/ $500 price is a lower cost alternative Case Lian Li TU150 Mini ITX Desktop Case $109.99 Link We love the handle on this small case and the glass side panel. Power Supply Corsair SF 600W 80+ Gold SFX Power Supply $114.99 Link 600W is an overkill Note that this motherboard does come with builtin WiFi. The external antenna must be connected but it is easy to get lost in transport. You might want to get a few additional WiFi antennas like these RP-SMA Male Antenna We also think we could get buy with a smaller and lighter power supply, but the 600W model gives the system the opportunity to add external devices that might draw more power.","title":"2023 Update"},{"location":"admin/gpu-parts/#assembly","text":"There are several good videos on YouTube that show how to assemble custom systems. You can also use a search engine to find videos for each of the parts. The Liquid coolers can be tricky to install correctly if you don't have experience. We also recommend reading the user manauals for each of the parts. They are usually on line. Jon Herke's Tiny Monster","title":"Assembly"},{"location":"admin/gpu-parts/#installing-nvidia-drivers-on-ubuntu","text":"Installing NVIDIA drivers on Ubuntu is notoriously painful and difficult. One mis-step and you can't get to the monitor and have to ssh in to fix things. Make sure to setup ssh before you install the NVIDIA drivers. We used the UNIX command line to install the NVIDIA drivers. The GUI tool on Ubuntu did not work for us in some settings. See NVIDIA Driver Install . A guide to do this is here: Installation of Nvidia Drivers on Ubuntu 18","title":"Installing NVIDIA Drivers on Ubuntu"},{"location":"admin/gpu-shell/","text":"Shell Commands for the GPU Server The following is a list of shell commands for the AI Racing League GPU Server. We have moved all the commands for setting up the NVIDIA GPU to the file NVIDIA Driver Install . The samples below are run if you are on the GPU running the Terminal shell or you have logged on using the secure shell program. Secure Shell Login ```linenums=\"0\" $ ssh arl@arl1.local 1 2 3 4 ## Check the Version of Ubuntu ```sh $ lsb_release -a Response: 1 2 3 4 5 No LSB modules are available. Distributor ID: Ubuntu Description: Ubuntu 18.04.3 LTS Release: 18.04 Codename: bionic List the CPU Information 1 lscpu Response: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 Architecture : x86_64 CPU op - mode ( s ): 32 - bit , 64 - bit Byte Order : Little Endian CPU ( s ): 12 On - line CPU ( s ) list : 0 - 11 Thread ( s ) per core : 2 Core ( s ) per socket : 6 Socket ( s ): 1 NUMA node ( s ): 1 Vendor ID : AuthenticAMD CPU family : 23 Model : 113 Model name : AMD Ryzen 5 3600 6 - Core Processor Stepping : 0 CPU MHz : 2195.902 CPU max MHz : 3600.0000 CPU min MHz : 2200.0000 BogoMIPS : 7187.07 Virtualization : AMD - V L1d cache : 32 K L1i cache : 32 K L2 cache : 512 K L3 cache : 16384 K NUMA node0 CPU ( s ): 0 - 11 Flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3 dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate sme ssbd mba sev ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif umip rdpid overflow_recov succor smca The key is that we have 12 CPUs and each CPU has two threads. That means that we have 24 threads that run concurrent operations on this server. This is plenty of capacity for our GPU server. RAM 1 free -m Response: 1 2 3 total used free shared buff/cache available Mem: 32124 1627 28879 75 1618 30019 Swap: 2047 0 2047 This indicates we have 32 GB RAM. The GPU server should have a minimum of 8 GB of RAM. Disk Space 1 df -h / Response: 1 2 Filesystem Size Used Avail Use% Mounted on /dev/nvme0n1p3 229G 178G 40G 82% / This shows we have a total of 229 gigabytes of RAM and we have 40 gigabytes available. We will need about 4 GB for each training set we store. Per User Disk Usage 1 du -hs /home/* 2 >/dev/null Response: 1 2 3 4 8.5 G / home / arl 1.4 G / home / dan 16 K / home / dan2 155 G / home / donkey Add A New GPU Server User 1 adduser donkey You can also allow the user to have \"sudo\" rights by using the following command: 1 sudo usermod -aG sudo donkey Change the Hostname 1 sudo vi hostname Change the name to \"gpu-server2\" or a similar name. NVIDIA GPU Monitor The runs similar to the UNIX top command, but for the GPU. 1 watch -d -n 0 .5 nvidia-smi NVIDIA GPU Utilization This shows the GPU running at 42% utilization during the training process. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 nvidia-smi Mon Jul 26 20:24:16 2021 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 470.57.02 Driver Version: 470.57.02 CUDA Version: 11.4 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... Off | 00000000:09:00.0 On | N/A | | 41% 49C P2 136W / 260W | 10892MiB / 11016MiB | 42% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 1327 G /usr/lib/xorg/Xorg 18MiB | | 0 N/A N/A 1398 G /usr/bin/gnome-shell 71MiB | | 0 N/A N/A 1574 G /usr/lib/xorg/Xorg 155MiB | | 0 N/A N/A 1705 G /usr/bin/gnome-shell 32MiB | | 0 N/A N/A 23722 G ...AAAAAAAAA= --shared-files 25MiB | | 0 N/A N/A 27071 G ...AAAAAAAAA= --shared-files 9MiB | | 0 N/A N/A 32486 C ...a3/envs/donkey/bin/python 10571MiB | +-----------------------------------------------------------------------------+","title":"GPU Shell Commands"},{"location":"admin/gpu-shell/#shell-commands-for-the-gpu-server","text":"The following is a list of shell commands for the AI Racing League GPU Server. We have moved all the commands for setting up the NVIDIA GPU to the file NVIDIA Driver Install . The samples below are run if you are on the GPU running the Terminal shell or you have logged on using the secure shell program.","title":"Shell Commands for the GPU Server"},{"location":"admin/gpu-shell/#secure-shell-login","text":"```linenums=\"0\" $ ssh arl@arl1.local 1 2 3 4 ## Check the Version of Ubuntu ```sh $ lsb_release -a Response: 1 2 3 4 5 No LSB modules are available. Distributor ID: Ubuntu Description: Ubuntu 18.04.3 LTS Release: 18.04 Codename: bionic","title":"Secure Shell Login"},{"location":"admin/gpu-shell/#list-the-cpu-information","text":"1 lscpu Response: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 Architecture : x86_64 CPU op - mode ( s ): 32 - bit , 64 - bit Byte Order : Little Endian CPU ( s ): 12 On - line CPU ( s ) list : 0 - 11 Thread ( s ) per core : 2 Core ( s ) per socket : 6 Socket ( s ): 1 NUMA node ( s ): 1 Vendor ID : AuthenticAMD CPU family : 23 Model : 113 Model name : AMD Ryzen 5 3600 6 - Core Processor Stepping : 0 CPU MHz : 2195.902 CPU max MHz : 3600.0000 CPU min MHz : 2200.0000 BogoMIPS : 7187.07 Virtualization : AMD - V L1d cache : 32 K L1i cache : 32 K L2 cache : 512 K L3 cache : 16384 K NUMA node0 CPU ( s ): 0 - 11 Flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3 dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate sme ssbd mba sev ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif umip rdpid overflow_recov succor smca The key is that we have 12 CPUs and each CPU has two threads. That means that we have 24 threads that run concurrent operations on this server. This is plenty of capacity for our GPU server.","title":"List the CPU Information"},{"location":"admin/gpu-shell/#ram","text":"1 free -m Response: 1 2 3 total used free shared buff/cache available Mem: 32124 1627 28879 75 1618 30019 Swap: 2047 0 2047 This indicates we have 32 GB RAM. The GPU server should have a minimum of 8 GB of RAM.","title":"RAM"},{"location":"admin/gpu-shell/#disk-space","text":"1 df -h / Response: 1 2 Filesystem Size Used Avail Use% Mounted on /dev/nvme0n1p3 229G 178G 40G 82% / This shows we have a total of 229 gigabytes of RAM and we have 40 gigabytes available. We will need about 4 GB for each training set we store.","title":"Disk Space"},{"location":"admin/gpu-shell/#per-user-disk-usage","text":"1 du -hs /home/* 2 >/dev/null Response: 1 2 3 4 8.5 G / home / arl 1.4 G / home / dan 16 K / home / dan2 155 G / home / donkey","title":"Per User Disk Usage"},{"location":"admin/gpu-shell/#add-a-new-gpu-server-user","text":"1 adduser donkey You can also allow the user to have \"sudo\" rights by using the following command: 1 sudo usermod -aG sudo donkey","title":"Add A New GPU Server User"},{"location":"admin/gpu-shell/#change-the-hostname","text":"1 sudo vi hostname Change the name to \"gpu-server2\" or a similar name.","title":"Change the Hostname"},{"location":"admin/gpu-shell/#nvidia-gpu-monitor","text":"The runs similar to the UNIX top command, but for the GPU. 1 watch -d -n 0 .5 nvidia-smi","title":"NVIDIA GPU Monitor"},{"location":"admin/gpu-shell/#nvidia-gpu-utilization","text":"This shows the GPU running at 42% utilization during the training process. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 nvidia-smi Mon Jul 26 20:24:16 2021 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 470.57.02 Driver Version: 470.57.02 CUDA Version: 11.4 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... Off | 00000000:09:00.0 On | N/A | | 41% 49C P2 136W / 260W | 10892MiB / 11016MiB | 42% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 1327 G /usr/lib/xorg/Xorg 18MiB | | 0 N/A N/A 1398 G /usr/bin/gnome-shell 71MiB | | 0 N/A N/A 1574 G /usr/lib/xorg/Xorg 155MiB | | 0 N/A N/A 1705 G /usr/bin/gnome-shell 32MiB | | 0 N/A N/A 23722 G ...AAAAAAAAA= --shared-files 25MiB | | 0 N/A N/A 27071 G ...AAAAAAAAA= --shared-files 9MiB | | 0 N/A N/A 32486 C ...a3/envs/donkey/bin/python 10571MiB | +-----------------------------------------------------------------------------+","title":"NVIDIA GPU Utilization"},{"location":"admin/joystick/","text":"Logitec F710 Game Controller for DonkeyCar https://docs.donkeycar.com/parts/controllers/ Testing to see if the Nano Recognizes the F710 USB Dongle You can use the \"lsusb\" UNIX shell command to list all the USB devices: $ lsusb Bus 002 Device 002: ID 0bda:0411 Realtek Semiconductor Corp. Bus 002 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub Bus 001 Device 004: ID 0bda:8179 Realtek Semiconductor Corp. RTL8188EUS 802.11n Wireless Network Adapter Bus 001 Device 005: ID 046d:c21f Logitech, Inc. F710 Wireless Gamepad [XInput Mode] Bus 001 Device 002: ID 0bda:5411 Realtek Semiconductor Corp. Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub Note that the USB device with an ID of 046d:c21f has been found in the 4th line above. The first ID before the colon is the device manufacturer (Logiteh) and the second is the id of their device (c21f). Linux looks this number up in their system and then loads the driver for this type of device. The driver will create a device file in the /dev/input directory called js0 $ ls -l /dev/input/js0 crw-rw-r--+ 1 root input 13, 0 Aug 16 19:30 /dev/input/js0 The \"c\" in the first letter says that this is a character I/O device. $ sudo apt-get install evtest [sudo] password for dan: Reading package lists... Done Building dependency tree Reading state information... Done The following packages were automatically installed and are no longer required: apt-clone archdetect-deb bogl-bterm busybox-static cryptsetup-bin dpkg-repack gir1.2-timezonemap-1.0 gir1.2-xkl-1.0 grub-common kde-window-manager kinit kio kpackagetool5 kwayland-data kwin-common kwin-data kwin-x11 libdebian-installer4 libkdecorations2-5v5 libkdecorations2private5v5 libkf5activities5 libkf5attica5 libkf5completion-data libkf5completion5 libkf5declarative-data libkf5declarative5 libkf5doctools5 libkf5globalaccel-data libkf5globalaccel5 libkf5globalaccelprivate5 libkf5idletime5 libkf5jobwidgets-data libkf5jobwidgets5 libkf5kcmutils-data libkf5kcmutils5 libkf5kiocore5 libkf5kiontlm5 libkf5kiowidgets5 libkf5newstuff-data libkf5newstuff5 libkf5newstuffcore5 libkf5package-data libkf5package5 libkf5plasma5 libkf5quickaddons5 libkf5solid5 libkf5solid5-data libkf5sonnet5-data libkf5sonnetcore5 libkf5sonnetui5 libkf5textwidgets-data libkf5textwidgets5 libkf5waylandclient5 libkf5waylandserver5 libkf5xmlgui-bin libkf5xmlgui-data libkf5xmlgui5 libkscreenlocker5 libkwin4-effect-builtins1 libkwineffects11 libkwinglutils11 libkwinxrenderutils11 libqgsttools-p1 libqt5designer5 libqt5help5 libqt5multimedia5 libqt5multimedia5-plugins libqt5multimediaquick-p5 libqt5multimediawidgets5 libqt5opengl5 libqt5positioning5 libqt5printsupport5 libqt5qml5 libqt5quick5 libqt5quickwidgets5 libqt5sensors5 libqt5sql5 libqt5test5 libqt5webchannel5 libqt5webkit5 libxcb-composite0 libxcb-cursor0 libxcb-damage0 os-prober python3-dbus.mainloop.pyqt5 python3-icu python3-pam python3-pyqt5 python3-pyqt5.qtsvg python3-pyqt5.qtwebkit python3-sip qml-module-org-kde-kquickcontrolsaddons qml-module-qtmultimedia qml-module-qtquick2 rdate tasksel tasksel-data Use 'sudo apt autoremove' to remove them. The following additional packages will be installed: evemu-tools libevemu3 The following NEW packages will be installed: evemu-tools evtest libevemu3 0 upgraded, 3 newly installed, 0 to remove and 7 not upgraded. Need to get 38.2 kB of archives. After this operation, 191 kB of additional disk space will be used. Do you want to continue? [Y/n] y Get:1 http://ports.ubuntu.com/ubuntu-ports bionic/universe arm64 libevemu3 arm64 2.6.0-0.1 [11.0 kB] Get:2 http://ports.ubuntu.com/ubuntu-ports bionic/universe arm64 evemu-tools arm64 2.6.0-0.1 [12.3 kB] Get:3 http://ports.ubuntu.com/ubuntu-ports bionic/universe arm64 evtest arm64 1:1.33-1build1 [14.9 kB] Fetched 38.2 kB in 1s (56.1 kB/s) debconf: delaying package configuration, since apt-utils is not installed Selecting previously unselected package libevemu3:arm64. (Reading database ... 140149 files and directories currently installed.) Preparing to unpack .../libevemu3_2.6.0-0.1_arm64.deb ... Unpacking libevemu3:arm64 (2.6.0-0.1) ... Selecting previously unselected package evemu-tools. Preparing to unpack .../evemu-tools_2.6.0-0.1_arm64.deb ... Unpacking evemu-tools (2.6.0-0.1) ... Selecting previously unselected package evtest. Preparing to unpack .../evtest_1%3a1.33-1build1_arm64.deb ... Unpacking evtest (1:1.33-1build1) ... Setting up evtest (1:1.33-1build1) ... Processing triggers for libc-bin (2.27-3ubuntu1) ... Processing triggers for man-db (2.8.3-2ubuntu0.1) ... Setting up libevemu3:arm64 (2.6.0-0.1) ... Setting up evemu-tools (2.6.0-0.1) ... Processing triggers for libc-bin (2.27-3ubuntu1) ... dan@danm-nano:~$ Now run it: $ evtest No device specified, trying to scan all of /dev/input/event* Not running as root, no devices may be available. Available devices: /dev/input/event2: Logitech Gamepad F710 Select the device event number [0-2]: 2 Logitech Gamepad F710 Input driver version is 1.0.1 Input device ID: bus 0x3 vendor 0x46d product 0xc21f version 0x305 Input device name: \"Logitech Gamepad F710\" Supported events: Event type 0 (EV_SYN) Event type 1 (EV_KEY) Event code 304 (BTN_SOUTH) Event code 305 (BTN_EAST) Event code 307 (BTN_NORTH) Event code 308 (BTN_WEST) Event code 310 (BTN_TL) Event code 311 (BTN_TR) Event code 314 (BTN_SELECT) Event code 315 (BTN_START) Event code 316 (BTN_MODE) Event code 317 (BTN_THUMBL) Event code 318 (BTN_THUMBR) Event type 3 (EV_ABS) Event code 0 (ABS_X) Value 128 Min -32768 Max 32767 Fuzz 16 Flat 128 Event code 1 (ABS_Y) Value -129 Min -32768 Max 32767 Fuzz 16 Flat 128 Event code 2 (ABS_Z) Value 0 Min 0 Max 255 Event code 3 (ABS_RX) Value 128 Min -32768 Max 32767 Fuzz 16 Flat 128 Event code 4 (ABS_RY) Value -129 Min -32768 Max 32767 Fuzz 16 Flat 128 Event code 5 (ABS_RZ) Value 0 Min 0 Max 255 Event code 16 (ABS_HAT0X) Value 0 Min -1 Max 1 Event code 17 (ABS_HAT0Y) Value 0 Min -1 Max 1 Properties: Testing ... (interrupt to exit) Now as you press any key or move any joystick you will see the events. When I press the yellow Y we see: Event: time 1566006064.962158, type 1 (EV_KEY), code 308 (BTN_WEST), value 1 Event: time 1566006064.962158, -------------- SYN_REPORT ------------ Event: time 1566006065.129981, type 1 (EV_KEY), code 308 (BTN_WEST), value 0 Event: time 1566006065.129981, -------------- SYN_REPORT ------------ Blue X Event: time 1566006110.047015, type 1 (EV_KEY), code 307 (BTN_NORTH), value 1 Event: time 1566006110.047015, -------------- SYN_REPORT ------------ Event: time 1566006110.182606, type 1 (EV_KEY), code 307 (BTN_NORTH), value 0 Event: time 1566006110.182606, -------------- SYN_REPORT ------------ Red B Event: time 1566006143.423217, type 1 (EV_KEY), code 305 (BTN_EAST), value 1 Event: time 1566006143.423217, -------------- SYN_REPORT ------------ Event: time 1566006143.499642, type 1 (EV_KEY), code 305 (BTN_EAST), value 0 Event: time 1566006143.499642, -------------- SYN_REPORT ------------ Green A Event: time 1566006184.060282, type 1 (EV_KEY), code 304 (BTN_SOUTH), value 1 Event: time 1566006184.060282, -------------- SYN_REPORT ------------ Event: time 1566006184.128408, type 1 (EV_KEY), code 304 (BTN_SOUTH), value 0 Event: time 1566006184.128408, -------------- SYN_REPORT ------------ Moving the joystick generates: Event: time 1566006255.549652, -------------- SYN_REPORT ------------ Event: time 1566006255.553650, type 3 (EV_ABS), code 1 (ABS_Y), value -10923 Event: time 1566006255.553650, -------------- SYN_REPORT ------------ Event: time 1566006255.557650, type 3 (EV_ABS), code 1 (ABS_Y), value -14264 Event: time 1566006255.557650, -------------- SYN_REPORT ------------ Event: time 1566006255.561652, type 3 (EV_ABS), code 1 (ABS_Y), value -18633","title":"Joystick"},{"location":"admin/nvidia-driver-install/","text":"Install the NVIDIA Driver Ideally you should be able to use the Ubuntu \"Software and Updates\" tool to install the NIVIDA driver. This usually works, but if you get errors, you may need to use the unix shell. NVIDIA Card Verification You can first verify that the GPU card has been installed and powered up. We can use the \"list hardware\" command with the display option: 1 $ sudo lshw -C display 1 2 3 4 5 6 7 8 9 10 11 12 *- display UNCLAIMED description : VGA compatible controller product : GV102 vendor : NVIDIA Corporation physical id : 0 bus info : pci @0000 : 09 : 00.0 version : a1 width : 64 bits clock : 33 MHz capabilities : pm msi pciexpress vga_controller bus_master cap_list configuration : latency = 0 resources : memory : f6000000 - f6ffffff memory : e0000000 - efffffff memory : f0000000 - f1ffffff ioport : e000 ( size = 128 ) memory : c0000 - dffff This shows that there is a GPU card installed but not claimed by the display. NVIDIA Devices You can then use the ubuntu-drivers command to see the devices. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 $ ubuntu-drivers devices == /sys/devices/pci0000:00/0000:00:03.1/0000:09:00.0 == modalias : pci:v000010DEd00001E07sv000010DEsd000012A4bc03sc00i00 vendor : NVIDIA Corporation driver : nvidia-driver-470 - distro non-free recommended driver : nvidia-driver-460-server - distro non-free driver : nvidia-driver-418-server - distro non-free driver : nvidia-driver-460 - distro non-free driver : nvidia-driver-450-server - distro non-free driver : xserver-xorg-video-nouveau - distro free builtin == /sys/devices/pci0000:00/0000:00:01.2/0000:02:00.0/0000:03:04.0/0000:05:00.0 == modalias : pci:v00008086d00002723sv00008086sd00000084bc02sc80i00 vendor : Intel Corporation manual_install: True driver : backport-iwlwifi-dkms - distro free Ubuntu Drivers Autoinstall 1 sudo ubuntu-drivers autoinstall This tool will tell you what drivers you need to install. 1 sudo apt-get install nvidia-driver-470 This will often generate errors but it will indicate what other libraries need to be installed for the 470 driver to work. Final Test Now we are ready to probe the full GPU and get all the statistics of what is in the GPU. 1 nvidia-smi 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Thu Jul 22 22:59:36 2021 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 470.57.02 Driver Version: 470.57.02 CUDA Version: 11.4 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... Off | 00000000:09:00.0 Off | N/A | | 41% 36C P8 2W / 260W | 283MiB / 11016MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 1327 G /usr/lib/xorg/Xorg 18MiB | | 0 N/A N/A 1398 G /usr/bin/gnome-shell 71MiB | | 0 N/A N/A 1574 G /usr/lib/xorg/Xorg 98MiB | | 0 N/A N/A 1705 G /usr/bin/gnome-shell 91MiB | +-----------------------------------------------------------------------------+ If you don't get this or a similar display, you must continue to search for installation instructions. After you get this screen you can reboot. CUDA Version 1 nvcc --version Results: 1 2 3 4 nvcc : NVIDIA ( R ) Cuda compiler driver Copyright ( c ) 2005 - 2017 NVIDIA Corporation Built on Fri_Nov__3_21 : 07 : 56 _CDT_2017 Cuda compilation tools , release 9.1 , V9 . 1.85 CUDA Tookkit Install for PyTorch 1 conda install cudatoolkit = <CUDA Version> -c pytorch 1 conda install cudatoolkit = 11 -c pytorch","title":"NVIDIA Driver Installation"},{"location":"admin/nvidia-driver-install/#install-the-nvidia-driver","text":"Ideally you should be able to use the Ubuntu \"Software and Updates\" tool to install the NIVIDA driver. This usually works, but if you get errors, you may need to use the unix shell.","title":"Install the NVIDIA Driver"},{"location":"admin/nvidia-driver-install/#nvidia-card-verification","text":"You can first verify that the GPU card has been installed and powered up. We can use the \"list hardware\" command with the display option: 1 $ sudo lshw -C display 1 2 3 4 5 6 7 8 9 10 11 12 *- display UNCLAIMED description : VGA compatible controller product : GV102 vendor : NVIDIA Corporation physical id : 0 bus info : pci @0000 : 09 : 00.0 version : a1 width : 64 bits clock : 33 MHz capabilities : pm msi pciexpress vga_controller bus_master cap_list configuration : latency = 0 resources : memory : f6000000 - f6ffffff memory : e0000000 - efffffff memory : f0000000 - f1ffffff ioport : e000 ( size = 128 ) memory : c0000 - dffff This shows that there is a GPU card installed but not claimed by the display.","title":"NVIDIA Card Verification"},{"location":"admin/nvidia-driver-install/#nvidia-devices","text":"You can then use the ubuntu-drivers command to see the devices. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 $ ubuntu-drivers devices == /sys/devices/pci0000:00/0000:00:03.1/0000:09:00.0 == modalias : pci:v000010DEd00001E07sv000010DEsd000012A4bc03sc00i00 vendor : NVIDIA Corporation driver : nvidia-driver-470 - distro non-free recommended driver : nvidia-driver-460-server - distro non-free driver : nvidia-driver-418-server - distro non-free driver : nvidia-driver-460 - distro non-free driver : nvidia-driver-450-server - distro non-free driver : xserver-xorg-video-nouveau - distro free builtin == /sys/devices/pci0000:00/0000:00:01.2/0000:02:00.0/0000:03:04.0/0000:05:00.0 == modalias : pci:v00008086d00002723sv00008086sd00000084bc02sc80i00 vendor : Intel Corporation manual_install: True driver : backport-iwlwifi-dkms - distro free","title":"NVIDIA Devices"},{"location":"admin/nvidia-driver-install/#ubuntu-drivers-autoinstall","text":"1 sudo ubuntu-drivers autoinstall This tool will tell you what drivers you need to install. 1 sudo apt-get install nvidia-driver-470 This will often generate errors but it will indicate what other libraries need to be installed for the 470 driver to work.","title":"Ubuntu Drivers Autoinstall"},{"location":"admin/nvidia-driver-install/#final-test","text":"Now we are ready to probe the full GPU and get all the statistics of what is in the GPU. 1 nvidia-smi 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Thu Jul 22 22:59:36 2021 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 470.57.02 Driver Version: 470.57.02 CUDA Version: 11.4 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... Off | 00000000:09:00.0 Off | N/A | | 41% 36C P8 2W / 260W | 283MiB / 11016MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 1327 G /usr/lib/xorg/Xorg 18MiB | | 0 N/A N/A 1398 G /usr/bin/gnome-shell 71MiB | | 0 N/A N/A 1574 G /usr/lib/xorg/Xorg 98MiB | | 0 N/A N/A 1705 G /usr/bin/gnome-shell 91MiB | +-----------------------------------------------------------------------------+ If you don't get this or a similar display, you must continue to search for installation instructions. After you get this screen you can reboot.","title":"Final Test"},{"location":"admin/nvidia-driver-install/#cuda-version","text":"1 nvcc --version Results: 1 2 3 4 nvcc : NVIDIA ( R ) Cuda compiler driver Copyright ( c ) 2005 - 2017 NVIDIA Corporation Built on Fri_Nov__3_21 : 07 : 56 _CDT_2017 Cuda compilation tools , release 9.1 , V9 . 1.85","title":"CUDA Version"},{"location":"admin/nvidia-driver-install/#cuda-tookkit-install-for-pytorch","text":"1 conda install cudatoolkit = <CUDA Version> -c pytorch 1 conda install cudatoolkit = 11 -c pytorch","title":"CUDA Tookkit Install for PyTorch"},{"location":"admin/tensorflow-gpu-install/","text":"$ conda install tensorflow-gpu==2.2.0 Collecting package metadata (current_repodata.json): done Solving environment: failed with initial frozen solve. Retrying with flexible solve. Collecting package metadata (repodata.json): done Solving environment: done Package Plan environment location: /home/arl/miniconda3/envs/donkey added / updated specs: - tensorflow-gpu==2.2.0 The following packages will be downloaded: 1 2 3 4 5 6 7 8 9 10 package | build ---------------------------|----------------- cudatoolkit-10.1.243 | h6bb024c_0 347.4 MB cudnn-7.6.5 | cuda10.1_0 179.9 MB cupti-10.1.168 | 0 1.4 MB tensorflow-2.2.0 |gpu_py37h1a511ff_0 4 KB tensorflow-base-2.2.0 |gpu_py37h8a81be8_0 181.7 MB tensorflow-gpu-2.2.0 | h0d30ee6_0 3 KB ------------------------------------------------------------ Total: 710.4 MB The following NEW packages will be INSTALLED: cudatoolkit pkgs/main/linux-64::cudatoolkit-10.1.243-h6bb024c_0 cudnn pkgs/main/linux-64::cudnn-7.6.5-cuda10.1_0 cupti pkgs/main/linux-64::cupti-10.1.168-0 tensorflow-gpu pkgs/main/linux-64::tensorflow-gpu-2.2.0-h0d30ee6_0 The following packages will be DOWNGRADED: _tflow_select 2.3.0-mkl --> 2.1.0-gpu tensorflow 2.2.0-mkl_py37h6e9ce2d_0 --> 2.2.0-gpu_py37h1a511ff_0 tensorflow-base 2.2.0-mkl_py37hd506778_0 --> 2.2.0-gpu_py37h8a81be8_0 Proceed ([y]/n)? Y Downloading and Extracting Packages tensorflow-base-2.2. | 181.7 MB | ################################################################################################################################################################ | 100% cudnn-7.6.5 | 179.9 MB | ################################################################################################################################################################ | 100% cupti-10.1.168 | 1.4 MB | ################################################################################################################################################################ | 100% tensorflow-2.2.0 | 4 KB | ################################################################################################################################################################ | 100% tensorflow-gpu-2.2.0 | 3 KB | ################################################################################################################################################################ | 100% cudatoolkit-10.1.243 | 347.4 MB | ################################################################################################################################################################ | 100% Preparing transaction: done Verifying transaction: done Executing transaction: done","title":"Tensorflow GPU Software"},{"location":"admin/tensorflow-gpu-install/#package-plan","text":"environment location: /home/arl/miniconda3/envs/donkey added / updated specs: - tensorflow-gpu==2.2.0 The following packages will be downloaded: 1 2 3 4 5 6 7 8 9 10 package | build ---------------------------|----------------- cudatoolkit-10.1.243 | h6bb024c_0 347.4 MB cudnn-7.6.5 | cuda10.1_0 179.9 MB cupti-10.1.168 | 0 1.4 MB tensorflow-2.2.0 |gpu_py37h1a511ff_0 4 KB tensorflow-base-2.2.0 |gpu_py37h8a81be8_0 181.7 MB tensorflow-gpu-2.2.0 | h0d30ee6_0 3 KB ------------------------------------------------------------ Total: 710.4 MB The following NEW packages will be INSTALLED: cudatoolkit pkgs/main/linux-64::cudatoolkit-10.1.243-h6bb024c_0 cudnn pkgs/main/linux-64::cudnn-7.6.5-cuda10.1_0 cupti pkgs/main/linux-64::cupti-10.1.168-0 tensorflow-gpu pkgs/main/linux-64::tensorflow-gpu-2.2.0-h0d30ee6_0 The following packages will be DOWNGRADED: _tflow_select 2.3.0-mkl --> 2.1.0-gpu tensorflow 2.2.0-mkl_py37h6e9ce2d_0 --> 2.2.0-gpu_py37h1a511ff_0 tensorflow-base 2.2.0-mkl_py37hd506778_0 --> 2.2.0-gpu_py37h8a81be8_0 Proceed ([y]/n)? Y Downloading and Extracting Packages tensorflow-base-2.2. | 181.7 MB | ################################################################################################################################################################ | 100% cudnn-7.6.5 | 179.9 MB | ################################################################################################################################################################ | 100% cupti-10.1.168 | 1.4 MB | ################################################################################################################################################################ | 100% tensorflow-2.2.0 | 4 KB | ################################################################################################################################################################ | 100% tensorflow-gpu-2.2.0 | 3 KB | ################################################################################################################################################################ | 100% cudatoolkit-10.1.243 | 347.4 MB | ################################################################################################################################################################ | 100% Preparing transaction: done Verifying transaction: done Executing transaction: done","title":"Package Plan"},{"location":"lesson-plans/car-structure/","text":"RC Car Structure Using GPTChat To Help Generate Block Diagrams Generating architecture diagrams can be a slow and painful process. But ChatGPT can help. here is a process of generating a block diagram using ChatGPT. We can use the Mermaid Live program to view the diagram. ChatGPT also has a diagram plugin that will render the image within ChatGPT. GPTChat Prompt ```linenums=\"0\" Create a mermaid block diagram for the structure of a Donkey Car. Make sure to label all the components and their connection types. Return the results in raw mermaid format. Include the following structures: The main computer (Raspberry Pi or NVIDIA Nano) The main computer has an I2C connection to the Servo Controller board The Servo Controller connects to the Steering Servo and the Electronic Speed Controller (ESC) via PWM connections The ESC connects to the RC Car Battery THE ESC connects to the Car Motor The 5v battery powers the main computer There are two batteries: a 5V battery and a 7V RC Car Battery. The Servo controller is a PCA9685 board. 1 ## ChatGPT Response This code will create a block diagram where: The \"Main Computer\" (A) connects to the \"Servo Controller\" (B) via an I2C connection. The \"Servo Controller\" (B) connects to both the \"Steering Servo\" (C) and \"Electronic Speed Controller\" (D) via PWM connections. The \"Electronic Speed Controller\" (D) connects to both the \"RC Car Battery\" (E) and the \"Car Motor\" (F). The \"5V Battery\" (G) connects to the \"Main Computer\" (A) as well as to the \"Positive Connection\" (H) and \"Negative Connection\" (I). The \"RC Car Battery\" (E) also connects to both the \"Positive Connection\" (H) and \"Negative Connection\" (I). Please note that this is a simplified diagram and does not necessarily represent the physical arrangement of the components in an actual Donkey Car setup. This diagram should be taken as an abstract representation of how the components interact. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## Mermaid Diagram ``` mermaid graph TB A [ \"Main Computer<br>Raspberry Pi or NVIDIA Nano\" ] B [ \"Servo Controller<br>PCA9685 board\" ] C [ \"Steering Servo\" ] D [ \"Electronic Speed Controller<br>(ESC)\" ] E [ \"RC Car Battery (7V)\" ] F [ \"Car Motor\" ] G [ \"5V Battery\" ] A -- I2C --> B B -- PWM --> C B -- PWM --> D D --- E D --- F G --- A","title":"Generating Block Diagrams"},{"location":"lesson-plans/car-structure/#rc-car-structure","text":"","title":"RC Car Structure"},{"location":"lesson-plans/car-structure/#using-gptchat-to-help-generate-block-diagrams","text":"Generating architecture diagrams can be a slow and painful process. But ChatGPT can help. here is a process of generating a block diagram using ChatGPT. We can use the Mermaid Live program to view the diagram. ChatGPT also has a diagram plugin that will render the image within ChatGPT.","title":"Using GPTChat To Help Generate Block Diagrams"},{"location":"lesson-plans/car-structure/#gptchat-prompt","text":"```linenums=\"0\" Create a mermaid block diagram for the structure of a Donkey Car. Make sure to label all the components and their connection types. Return the results in raw mermaid format. Include the following structures: The main computer (Raspberry Pi or NVIDIA Nano) The main computer has an I2C connection to the Servo Controller board The Servo Controller connects to the Steering Servo and the Electronic Speed Controller (ESC) via PWM connections The ESC connects to the RC Car Battery THE ESC connects to the Car Motor The 5v battery powers the main computer There are two batteries: a 5V battery and a 7V RC Car Battery. The Servo controller is a PCA9685 board. 1 ## ChatGPT Response This code will create a block diagram where: The \"Main Computer\" (A) connects to the \"Servo Controller\" (B) via an I2C connection. The \"Servo Controller\" (B) connects to both the \"Steering Servo\" (C) and \"Electronic Speed Controller\" (D) via PWM connections. The \"Electronic Speed Controller\" (D) connects to both the \"RC Car Battery\" (E) and the \"Car Motor\" (F). The \"5V Battery\" (G) connects to the \"Main Computer\" (A) as well as to the \"Positive Connection\" (H) and \"Negative Connection\" (I). The \"RC Car Battery\" (E) also connects to both the \"Positive Connection\" (H) and \"Negative Connection\" (I). Please note that this is a simplified diagram and does not necessarily represent the physical arrangement of the components in an actual Donkey Car setup. This diagram should be taken as an abstract representation of how the components interact. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## Mermaid Diagram ``` mermaid graph TB A [ \"Main Computer<br>Raspberry Pi or NVIDIA Nano\" ] B [ \"Servo Controller<br>PCA9685 board\" ] C [ \"Steering Servo\" ] D [ \"Electronic Speed Controller<br>(ESC)\" ] E [ \"RC Car Battery (7V)\" ] F [ \"Car Motor\" ] G [ \"5V Battery\" ] A -- I2C --> B B -- PWM --> C B -- PWM --> D D --- E D --- F G --- A","title":"GPTChat Prompt"},{"location":"lesson-plans/computer-vision/","text":"AI Racing League Computer Vision Table Raspberry Pi and the NVIDIA Nano are popular systems for demonstrating various computer vision applications due to their affordability and flexibility. Requirements For these lessons, you just need a Raspberry Pi (or Nano) and the attached Camera that we use for all our cars. Sample Labs Here are several demos we show to high school students using OpenCV and Raspberry Pi: Face Detection and Recognition We can use the built-in Haar cascades in OpenCV for face and eyes detection. For the face recognition part, you can use either OpenCV's built-in algorithms or deep learning-based models such as FaceNet. Object Detection Use pre-trained models from OpenCV's DNN module or TensorFlow's model zoo to recognize multiple objects in real-time. Optical Character Recognition (OCR): Combine OpenCV for image processing and Tesseract for character recognition to demonstrate how a device can read text from images or real-time video feed. Color Detection Write a simple program that detects specific colors in real-time. This can be used as a stepping stone to more advanced object-tracking projects. We can also combine this lab with our Raspberry Pi Pico color detection sensors. Motion Detection and Tracking Implement a simple surveillance system that detects motion and tracks moving objects. This can be a good introduction to video analysis. Augmented Reality Show how to overlay graphics on a real-time video feed based on detected features. For example, you can use OpenCV's capabilities for feature detection (like SIFT, SURF, ORB) and perspective transformation to overlay 3D objects on a marker. Hand Gesture Recognition Create a program that recognizes hand gestures and associates them with commands. You could use this to control a game or navigate a user interface. License Plate Recognition You can implement a simple Automatic Number Plate Recognition (ANPR) system using image processing techniques in OpenCV and OCR. QR Code and Barcode Scanner Use OpenCV for real-time detection and decoding of QR codes and bar codes. Most of these demonstrations will require additional Python libraries beyond just OpenCV, like NumPy, Pillow, or TensorFlow. For hardware, you will need the Raspberry Pi 3 with 4GB RAM, a camera module, and potentially additional items like a monitor, mouse, and keyboard for a fully interactive setup.","title":"Introduction"},{"location":"lesson-plans/computer-vision/#ai-racing-league-computer-vision-table","text":"Raspberry Pi and the NVIDIA Nano are popular systems for demonstrating various computer vision applications due to their affordability and flexibility.","title":"AI Racing League Computer Vision Table"},{"location":"lesson-plans/computer-vision/#requirements","text":"For these lessons, you just need a Raspberry Pi (or Nano) and the attached Camera that we use for all our cars.","title":"Requirements"},{"location":"lesson-plans/computer-vision/#sample-labs","text":"Here are several demos we show to high school students using OpenCV and Raspberry Pi:","title":"Sample Labs"},{"location":"lesson-plans/computer-vision/#face-detection-and-recognition","text":"We can use the built-in Haar cascades in OpenCV for face and eyes detection. For the face recognition part, you can use either OpenCV's built-in algorithms or deep learning-based models such as FaceNet.","title":"Face Detection and Recognition"},{"location":"lesson-plans/computer-vision/#object-detection","text":"Use pre-trained models from OpenCV's DNN module or TensorFlow's model zoo to recognize multiple objects in real-time.","title":"Object Detection"},{"location":"lesson-plans/computer-vision/#optical-character-recognition-ocr","text":"Combine OpenCV for image processing and Tesseract for character recognition to demonstrate how a device can read text from images or real-time video feed.","title":"Optical Character Recognition (OCR):"},{"location":"lesson-plans/computer-vision/#color-detection","text":"Write a simple program that detects specific colors in real-time. This can be used as a stepping stone to more advanced object-tracking projects. We can also combine this lab with our Raspberry Pi Pico color detection sensors.","title":"Color Detection"},{"location":"lesson-plans/computer-vision/#motion-detection-and-tracking","text":"Implement a simple surveillance system that detects motion and tracks moving objects. This can be a good introduction to video analysis.","title":"Motion Detection and Tracking"},{"location":"lesson-plans/computer-vision/#augmented-reality","text":"Show how to overlay graphics on a real-time video feed based on detected features. For example, you can use OpenCV's capabilities for feature detection (like SIFT, SURF, ORB) and perspective transformation to overlay 3D objects on a marker.","title":"Augmented Reality"},{"location":"lesson-plans/computer-vision/#hand-gesture-recognition","text":"Create a program that recognizes hand gestures and associates them with commands. You could use this to control a game or navigate a user interface.","title":"Hand Gesture Recognition"},{"location":"lesson-plans/computer-vision/#license-plate-recognition","text":"You can implement a simple Automatic Number Plate Recognition (ANPR) system using image processing techniques in OpenCV and OCR.","title":"License Plate Recognition"},{"location":"lesson-plans/computer-vision/#qr-code-and-barcode-scanner","text":"Use OpenCV for real-time detection and decoding of QR codes and bar codes. Most of these demonstrations will require additional Python libraries beyond just OpenCV, like NumPy, Pillow, or TensorFlow. For hardware, you will need the Raspberry Pi 3 with 4GB RAM, a camera module, and potentially additional items like a monitor, mouse, and keyboard for a fully interactive setup.","title":"QR Code and Barcode Scanner"},{"location":"lesson-plans/computer-vision/face-filters/","text":"OpenCV Lab: Fun Face Filters** Objective: Learn to detect faces using OpenCV and apply fun filters like cat whiskers and ears. Tools & Libraries Python OpenCV Haar Cascades Understanding Haar Cascades in OpenCV Haar Cascades are a type of machine learning object detection method used to identify objects in images or video. In OpenCV, Haar Cascades are mainly used to detect faces, but they can also identify other objects like eyes, smiles, and more. There were named after Hungarian mathematician Alfr\u00e9d Haar who make key contributions to the mathematics of transformation of matrix data in the 1920s. Imagine you have a magic magnifying glass that you move across a photo. Whenever this magnifying glass sees a face, it lights up! That's kind of what Haar Cascades does in OpenCV. How Do Harr Cascades Work? Features : Haar Cascades work by looking at simple features in an area of the image, like the bridge of the nose being brighter than the eyes on either side because it sticks out and catches the light. Training : To make Haar Cascades 'learn' these features, it's shown many pictures of faces and non-faces. Through this, it learns what a face typically looks like. Cascade : The term 'cascade' is used because the algorithm employs a series of increasingly complex features to determine if a particular region is a face. Building Face-Filters Using Haar Cascades: Creating a Snapchat-like face filter using Haar Cascades involves two main steps: Face Detection : Detecting the location of the face in an image or video stream. Overlaying the Filter : Once we know where the face is, we can overlay our filter (like cat ears or sunglasses) at the correct position. Example Python Code : ```python import cv2 Load the Haar cascade for face detection face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml') Read the image img = cv2.imread('your_photo.jpg') gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) Detect faces faces = face_cascade.detectMultiScale(gray, 1.1, 4) for (x, y, w, h) in faces: # Here you'd typically overlay your filter, e.g., cat ears at coordinates (x, y) cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2) # Drawing a rectangle around the detected face for now cv2.imshow('Face Detection', img) cv2.waitKey() Resources: - Haarcascades for face and eye detection (provided by OpenCV). Steps: Setup & Installation Make sure you have Python and OpenCV installed. bash pip install opencv-python Face Detection Before adding filters, students should understand face detection. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ``` python import cv2 face_cascade = cv2 . CascadeClassifier ( cv2 . data . haarcascades + 'haarcascade_frontalface_default.xml' ) img = cv2 . imread ( 'path_to_image.jpg' ) gray = cv2 . cvtColor ( img , cv2 . COLOR_BGR2GRAY ) faces = face_cascade . detectMultiScale ( gray , 1.1 , 4 ) for ( x , y , w , h ) in faces : cv2 . rectangle ( img , ( x , y ), ( x + w , y + h ), ( 255 , 0 , 0 ), 2 ) cv2 . imshow ( 'Detected Faces' , img ) cv2 . waitKey () ``` Designing the Filters Use any graphic software (like GIMP or Photoshop) to design transparent PNG images of cat whiskers and ears. Ensure the filter graphics are adjustable in size or make several versions to fit different face sizes. Applying the Cat Ears Filter Given the coordinates (x, y, w, h) of the detected face: ```python ear_image = cv2.imread('path_to_ear_image.png', -1) # The -1 reads the alpha channel Resize the cat ear image to fit the width of the face ear_width = w aspect_ratio = ear_image.shape[1] / float(ear_image.shape[0]) ear_height = int(ear_width / aspect_ratio) ear_image = cv2.resize(ear_image, (ear_width, ear_height)) Region where we want to place the cat ears (taking care not to exceed image dimensions) for c in range(0, 3): img[y:y+ear_image.shape[0], x:x+ear_image.shape[1], c] = img[y:y+ear_image.shape[0], x:x+ear_image.shape[1], c] * (1 - ear_image[:, :, 3] / 255.0) + ear_image[:, :, c] * (ear_image[:, :, 3] / 255.0) ``` Applying the Whiskers Filter Similar to the cat ears, you can position whiskers on the cheeks using the (x, y, w, h) of the face. Real-time Filters Application Capture video from the webcam and apply the filters in real-time. ```python cap = cv2.VideoCapture(0) while True: ret, frame = cap.read() gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) 1 2 3 4 5 6 7 faces = face_cascade . detectMultiScale ( gray , 1 . 1 , 4 ) for ( x , y , w , h ) in faces : # Apply cat ears and whiskers filter here cv2 . imshow ( ' Filters in Action ' , frame ) if cv2 . waitKey ( 1 ) & 0 xFF == ord ( ' q ' ) : break cap.release() cv2.destroyAllWindows() ``` Challenge & Extension Multiple faces: Modify the code to handle multiple faces in one frame. Other Filters: Allow students to design and implement their own filters. Interactivity: Add buttons to change filters or toggle them on/off. Assessment: Can students successfully detect faces in different images? How accurately do the filters get applied to the face? How creative and effective are student-designed filters? Note: This is a basic version and doesn't incorporate sophisticated technologies like deep learning which Snapchat uses. It's aimed at high school students to give a foundational understanding.","title":"Face Filters"},{"location":"lesson-plans/computer-vision/face-filters/#opencv-lab-fun-face-filters","text":"Objective: Learn to detect faces using OpenCV and apply fun filters like cat whiskers and ears.","title":"OpenCV Lab: Fun Face Filters**"},{"location":"lesson-plans/computer-vision/face-filters/#tools-libraries","text":"Python OpenCV","title":"Tools &amp; Libraries"},{"location":"lesson-plans/computer-vision/face-filters/#haar-cascades","text":"","title":"Haar Cascades"},{"location":"lesson-plans/computer-vision/face-filters/#understanding-haar-cascades-in-opencv","text":"Haar Cascades are a type of machine learning object detection method used to identify objects in images or video. In OpenCV, Haar Cascades are mainly used to detect faces, but they can also identify other objects like eyes, smiles, and more. There were named after Hungarian mathematician Alfr\u00e9d Haar who make key contributions to the mathematics of transformation of matrix data in the 1920s. Imagine you have a magic magnifying glass that you move across a photo. Whenever this magnifying glass sees a face, it lights up! That's kind of what Haar Cascades does in OpenCV.","title":"Understanding Haar Cascades in OpenCV"},{"location":"lesson-plans/computer-vision/face-filters/#how-do-harr-cascades-work","text":"Features : Haar Cascades work by looking at simple features in an area of the image, like the bridge of the nose being brighter than the eyes on either side because it sticks out and catches the light. Training : To make Haar Cascades 'learn' these features, it's shown many pictures of faces and non-faces. Through this, it learns what a face typically looks like. Cascade : The term 'cascade' is used because the algorithm employs a series of increasingly complex features to determine if a particular region is a face.","title":"How Do Harr Cascades Work?"},{"location":"lesson-plans/computer-vision/face-filters/#building-face-filters-using-haar-cascades","text":"Creating a Snapchat-like face filter using Haar Cascades involves two main steps: Face Detection : Detecting the location of the face in an image or video stream. Overlaying the Filter : Once we know where the face is, we can overlay our filter (like cat ears or sunglasses) at the correct position. Example Python Code : ```python import cv2","title":"Building Face-Filters Using Haar Cascades:"},{"location":"lesson-plans/computer-vision/face-filters/#load-the-haar-cascade-for-face-detection","text":"face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')","title":"Load the Haar cascade for face detection"},{"location":"lesson-plans/computer-vision/face-filters/#read-the-image","text":"img = cv2.imread('your_photo.jpg') gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)","title":"Read the image"},{"location":"lesson-plans/computer-vision/face-filters/#detect-faces","text":"faces = face_cascade.detectMultiScale(gray, 1.1, 4) for (x, y, w, h) in faces: # Here you'd typically overlay your filter, e.g., cat ears at coordinates (x, y) cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2) # Drawing a rectangle around the detected face for now cv2.imshow('Face Detection', img) cv2.waitKey() Resources: - Haarcascades for face and eye detection (provided by OpenCV). Steps: Setup & Installation Make sure you have Python and OpenCV installed. bash pip install opencv-python Face Detection Before adding filters, students should understand face detection. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ``` python import cv2 face_cascade = cv2 . CascadeClassifier ( cv2 . data . haarcascades + 'haarcascade_frontalface_default.xml' ) img = cv2 . imread ( 'path_to_image.jpg' ) gray = cv2 . cvtColor ( img , cv2 . COLOR_BGR2GRAY ) faces = face_cascade . detectMultiScale ( gray , 1.1 , 4 ) for ( x , y , w , h ) in faces : cv2 . rectangle ( img , ( x , y ), ( x + w , y + h ), ( 255 , 0 , 0 ), 2 ) cv2 . imshow ( 'Detected Faces' , img ) cv2 . waitKey () ``` Designing the Filters Use any graphic software (like GIMP or Photoshop) to design transparent PNG images of cat whiskers and ears. Ensure the filter graphics are adjustable in size or make several versions to fit different face sizes. Applying the Cat Ears Filter Given the coordinates (x, y, w, h) of the detected face: ```python ear_image = cv2.imread('path_to_ear_image.png', -1) # The -1 reads the alpha channel","title":"Detect faces"},{"location":"lesson-plans/computer-vision/face-filters/#resize-the-cat-ear-image-to-fit-the-width-of-the-face","text":"ear_width = w aspect_ratio = ear_image.shape[1] / float(ear_image.shape[0]) ear_height = int(ear_width / aspect_ratio) ear_image = cv2.resize(ear_image, (ear_width, ear_height))","title":"Resize the cat ear image to fit the width of the face"},{"location":"lesson-plans/computer-vision/face-filters/#region-where-we-want-to-place-the-cat-ears-taking-care-not-to-exceed-image-dimensions","text":"for c in range(0, 3): img[y:y+ear_image.shape[0], x:x+ear_image.shape[1], c] = img[y:y+ear_image.shape[0], x:x+ear_image.shape[1], c] * (1 - ear_image[:, :, 3] / 255.0) + ear_image[:, :, c] * (ear_image[:, :, 3] / 255.0) ``` Applying the Whiskers Filter Similar to the cat ears, you can position whiskers on the cheeks using the (x, y, w, h) of the face. Real-time Filters Application Capture video from the webcam and apply the filters in real-time. ```python cap = cv2.VideoCapture(0) while True: ret, frame = cap.read() gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) 1 2 3 4 5 6 7 faces = face_cascade . detectMultiScale ( gray , 1 . 1 , 4 ) for ( x , y , w , h ) in faces : # Apply cat ears and whiskers filter here cv2 . imshow ( ' Filters in Action ' , frame ) if cv2 . waitKey ( 1 ) & 0 xFF == ord ( ' q ' ) : break cap.release() cv2.destroyAllWindows() ``` Challenge & Extension Multiple faces: Modify the code to handle multiple faces in one frame. Other Filters: Allow students to design and implement their own filters. Interactivity: Add buttons to change filters or toggle them on/off. Assessment: Can students successfully detect faces in different images? How accurately do the filters get applied to the face? How creative and effective are student-designed filters? Note: This is a basic version and doesn't incorporate sophisticated technologies like deep learning which Snapchat uses. It's aimed at high school students to give a foundational understanding.","title":"Region where we want to place the cat ears (taking care not to exceed image dimensions)"},{"location":"lesson-plans/computer-vision/open-cv/","text":"OpenCV Labs Lesson Plan: Introduction to OpenCV in Python Setup Students should have access to a computer with a webcam and Python installed. We use Rasberry Pi or NIVIDA Nanos with cameras. Objective: By the end of the lesson, students will be able to explain the purpose and basic functions of OpenCV, and implement some basic image processing tasks. 1. Introduction (10 minutes) Discussion : Ask students if they've ever used Instagram or Snapchat filters, or how computers recognize faces or objects. Explanation : Introduce OpenCV as one of the most powerful libraries used for computer vision tasks. 2. Brief History & Applications (10 minutes) Mention OpenCV's origins and its significance in AI and robotics. Showcase a few applications, e.g., facial recognition, self-driving cars, AR filters. 3. Basics of Image Representation (10 minutes) Discuss how computers see images as matrices of numbers. Quick overview: Images are made of pixels; each pixel has values that determine its color. 4. Installation & Setup (10 minutes) Demo : How to install OpenCV using pip . pip install opencv-python Quick navigation of the OpenCV documentation to encourage self-learning. 5. Hands-on Lab 1: Reading, Displaying, and Saving Images (20 minutes) Exercise : Use OpenCV to read an image. Display the image in a window. Convert the image to grayscale. Save the grayscale image. 6. Hands-on Lab 2: Playing with Webcam Feed (20 minutes) Exercise : Access the webcam using OpenCV. Display the live video feed. Apply a grayscale filter to the feed. Bonus: Add a button or a keypress event to capture and save a snapshot from the feed. 7. Introduction to Basic Image Processing Techniques (15 minutes) Explanation : Discuss image thresholding, blurring, and edge detection. Showcase examples of each technique. 8. Hands-on Lab 3: Basic Image Processing (30 minutes) Exercise : Use a sample image (or one they choose). Apply and display thresholding. Apply and display blurring. Apply and display edge detection using the Canny edge detector. 9. Fun Lab: Snapchat-like Filters (45 minutes) Objective : The goal of this lab is to have students use OpenCV to create basic filters for a live webcam feed. Exercise : Access the webcam feed. Overlay cartoon glasses or hats on the user's face using OpenCV's face detection (Haar cascades). Bonus: Let the students get creative, e.g., adding mustaches, changing background, etc. 10. Discussion & Wrap-up (15 minutes) Reflect on the potential of computer vision and its real-world applications. Discuss the ethical implications, such as privacy concerns with facial recognition. Introduce more advanced topics in OpenCV for those interested (like object recognition, machine learning with OpenCV, etc.) Q&A session. Additional Resources & Take-home Assignments: Explore More : Give students links to OpenCV tutorials and documentation for further reading. Project : Ask students to work on a mini-project, like a basic digital photo editor using OpenCV, allowing them to apply filters, rotate, and crop images. Challenge : For advanced students, introduce them to object detection using pre-trained models in OpenCV. Notes for the Instructor: Make sure all students have Python installed and guide them in setting up a virtual environment. Visual aids, like slides with images representing pixel values, will help in explaining image representation. Encourage students to collaborate and share their findings or issues during labs. Pair programming can be useful. Make sure to have a few sample images ready for labs, preferably with varying complexities.","title":"Open Computer Vision"},{"location":"lesson-plans/computer-vision/open-cv/#opencv-labs","text":"Lesson Plan: Introduction to OpenCV in Python","title":"OpenCV Labs"},{"location":"lesson-plans/computer-vision/open-cv/#setup","text":"Students should have access to a computer with a webcam and Python installed. We use Rasberry Pi or NIVIDA Nanos with cameras. Objective: By the end of the lesson, students will be able to explain the purpose and basic functions of OpenCV, and implement some basic image processing tasks.","title":"Setup"},{"location":"lesson-plans/computer-vision/open-cv/#1-introduction-10-minutes","text":"Discussion : Ask students if they've ever used Instagram or Snapchat filters, or how computers recognize faces or objects. Explanation : Introduce OpenCV as one of the most powerful libraries used for computer vision tasks.","title":"1. Introduction (10 minutes)"},{"location":"lesson-plans/computer-vision/open-cv/#2-brief-history-applications-10-minutes","text":"Mention OpenCV's origins and its significance in AI and robotics. Showcase a few applications, e.g., facial recognition, self-driving cars, AR filters.","title":"2. Brief History &amp; Applications (10 minutes)"},{"location":"lesson-plans/computer-vision/open-cv/#3-basics-of-image-representation-10-minutes","text":"Discuss how computers see images as matrices of numbers. Quick overview: Images are made of pixels; each pixel has values that determine its color.","title":"3. Basics of Image Representation (10 minutes)"},{"location":"lesson-plans/computer-vision/open-cv/#4-installation-setup-10-minutes","text":"Demo : How to install OpenCV using pip . pip install opencv-python Quick navigation of the OpenCV documentation to encourage self-learning.","title":"4. Installation &amp; Setup (10 minutes)"},{"location":"lesson-plans/computer-vision/open-cv/#5-hands-on-lab-1-reading-displaying-and-saving-images-20-minutes","text":"Exercise : Use OpenCV to read an image. Display the image in a window. Convert the image to grayscale. Save the grayscale image.","title":"5. Hands-on Lab 1: Reading, Displaying, and Saving Images (20 minutes)"},{"location":"lesson-plans/computer-vision/open-cv/#6-hands-on-lab-2-playing-with-webcam-feed-20-minutes","text":"Exercise : Access the webcam using OpenCV. Display the live video feed. Apply a grayscale filter to the feed. Bonus: Add a button or a keypress event to capture and save a snapshot from the feed.","title":"6. Hands-on Lab 2: Playing with Webcam Feed (20 minutes)"},{"location":"lesson-plans/computer-vision/open-cv/#7-introduction-to-basic-image-processing-techniques-15-minutes","text":"Explanation : Discuss image thresholding, blurring, and edge detection. Showcase examples of each technique.","title":"7. Introduction to Basic Image Processing Techniques (15 minutes)"},{"location":"lesson-plans/computer-vision/open-cv/#8-hands-on-lab-3-basic-image-processing-30-minutes","text":"Exercise : Use a sample image (or one they choose). Apply and display thresholding. Apply and display blurring. Apply and display edge detection using the Canny edge detector.","title":"8. Hands-on Lab 3: Basic Image Processing (30 minutes)"},{"location":"lesson-plans/computer-vision/open-cv/#9-fun-lab-snapchat-like-filters-45-minutes","text":"Objective : The goal of this lab is to have students use OpenCV to create basic filters for a live webcam feed. Exercise : Access the webcam feed. Overlay cartoon glasses or hats on the user's face using OpenCV's face detection (Haar cascades). Bonus: Let the students get creative, e.g., adding mustaches, changing background, etc.","title":"9. Fun Lab: Snapchat-like Filters (45 minutes)"},{"location":"lesson-plans/computer-vision/open-cv/#10-discussion-wrap-up-15-minutes","text":"Reflect on the potential of computer vision and its real-world applications. Discuss the ethical implications, such as privacy concerns with facial recognition. Introduce more advanced topics in OpenCV for those interested (like object recognition, machine learning with OpenCV, etc.) Q&A session.","title":"10. Discussion &amp; Wrap-up (15 minutes)"},{"location":"lesson-plans/computer-vision/open-cv/#additional-resources-take-home-assignments","text":"Explore More : Give students links to OpenCV tutorials and documentation for further reading. Project : Ask students to work on a mini-project, like a basic digital photo editor using OpenCV, allowing them to apply filters, rotate, and crop images. Challenge : For advanced students, introduce them to object detection using pre-trained models in OpenCV.","title":"Additional Resources &amp; Take-home Assignments:"},{"location":"lesson-plans/computer-vision/open-cv/#notes-for-the-instructor","text":"Make sure all students have Python installed and guide them in setting up a virtual environment. Visual aids, like slides with images representing pixel values, will help in explaining image representation. Encourage students to collaborate and share their findings or issues during labs. Pair programming can be useful. Make sure to have a few sample images ready for labs, preferably with varying complexities.","title":"Notes for the Instructor:"},{"location":"lesson-plans/data-analysis/","text":"Data Analysis In these lessons, we learn how to write some basic data analysis Python programs. In the real world, you are often given some data and people ask us \"Tell me what insights you can give me about this data.\" This forms the basis of a field of data science called \"EDA\" for \"Electronic Data Analysis\". For example, say you are on a project to get cars to drive using machine learning. What insights can you derive from the sample images and driving data? Numpy Profiler TBD","title":"Introduction"},{"location":"lesson-plans/data-analysis/#data-analysis","text":"In these lessons, we learn how to write some basic data analysis Python programs. In the real world, you are often given some data and people ask us \"Tell me what insights you can give me about this data.\" This forms the basis of a field of data science called \"EDA\" for \"Electronic Data Analysis\". For example, say you are on a project to get cars to drive using machine learning. What insights can you derive from the sample images and driving data?","title":"Data Analysis"},{"location":"lesson-plans/data-analysis/#numpy-profiler","text":"TBD","title":"Numpy Profiler"},{"location":"lesson-plans/data-analysis/01-intro/","text":"AI Racing League Data Analysis Why Analysis? Data analysis is a core part of building accurate models that create high quality predictions. Here are some sample analytics tasks: Understand what data we have Browse data sets Run metrics that count the number of items in a dataset Open sample items and view sample images Look for groupings of data Look at averages of values Remove poor training data Tools: Python and Jupyter Notebooks Libraries: os, image, numpy, dataframes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import os from IPython.display import Image image_dir = \"/home/arl/mycar/data/dans-msp/data/images\" files = os . listdir ( image_dir ) # last basement image is 1710 n = 1710 file_n = files [ n ] file_2 = files [ n + 1 ] print ( n , file_n ) file_path1 = image_dir + '/' + file_n file_path2 = image_dir + '/' + file_2 i1 = Image ( file_path1 ) i2 = Image ( file_path2 ) print ( n + 1 , file_2 ) display ( i1 , i2 )","title":"Intro Part 2"},{"location":"lesson-plans/data-analysis/01-intro/#ai-racing-league-data-analysis","text":"","title":"AI Racing League Data Analysis"},{"location":"lesson-plans/data-analysis/01-intro/#why-analysis","text":"Data analysis is a core part of building accurate models that create high quality predictions. Here are some sample analytics tasks: Understand what data we have Browse data sets Run metrics that count the number of items in a dataset Open sample items and view sample images Look for groupings of data Look at averages of values Remove poor training data","title":"Why Analysis?"},{"location":"lesson-plans/data-analysis/01-intro/#tools-python-and-jupyter-notebooks","text":"","title":"Tools: Python and Jupyter Notebooks"},{"location":"lesson-plans/data-analysis/01-intro/#libraries-os-image-numpy-dataframes","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import os from IPython.display import Image image_dir = \"/home/arl/mycar/data/dans-msp/data/images\" files = os . listdir ( image_dir ) # last basement image is 1710 n = 1710 file_n = files [ n ] file_2 = files [ n + 1 ] print ( n , file_n ) file_path1 = image_dir + '/' + file_n file_path2 = image_dir + '/' + file_2 i1 = Image ( file_path1 ) i2 = Image ( file_path2 ) print ( n + 1 , file_2 ) display ( i1 , i2 )","title":"Libraries: os, image, numpy, dataframes"},{"location":"lesson-plans/data-analysis/02-listing-files/","text":"Working with Files Listing Files with the OS library Python provides a powerful library for working with Operating System resources like file systems. We will start out with the listdir() function that lists the files in a directory. Here is program that lists all the tub files in our mycar/data directory: 1 2 3 4 5 6 import os data_dir = \"/home/arl/mycar/data/\" files = os . listdir ( data_dir ) for file in files : print ( file ) returns: 1 2 3 4 5 a-test-tub my-test-tub junk-tub production-run tub-47 Listing Files in a 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import os from IPython.display import Image image_dir = \"/home/arl/mycar/data/dans-msp/data/images\" files = os . listdir ( image_dir ) # last basement image is 1710 n = 1710 file_n = files [ n ] file_2 = files [ n + 1 ] print ( n , file_n ) file_path1 = image_dir + '/' + file_n file_path2 = image_dir + '/' + file_2 i1 = Image ( file_path1 ) i2 = Image ( file_path2 ) print ( n + 1 , file_2 ) display ( i1 , i2 ) List Random Files In Images Directory 1 2 3 4 5 6 7 8 9 10 import os import matplotlib.pyplot as plt from IPython.display import Image image_dir = \"/home/arl/mycar/data/dans-msp/data/images\" image_file_name_list = os . listdir ( image_dir ) for index in range ( 0 , 10 ): file_name = image_file_name_list [ index ] print ( file_name ) returns: 1 2 3 4 5 6 7 8 9 10 10263 _cam_image_array_ . jpg 6257 _cam_image_array_ . jpg 15248 _cam_image_array_ . jpg 3916 _cam_image_array_ . jpg 5223 _cam_image_array_ . jpg 15765 _cam_image_array_ . jpg 8437 _cam_image_array_ . jpg 5871 _cam_image_array_ . jpg 16681 _cam_image_array_ . jpg 15281 _cam_image_array_ . jpg Note that the files are not in any specific order. Show Images for 10 Random Files 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import glob import matplotlib.pyplot as plt import matplotlib.image as mpimg % matplotlib inline images = [] for img_path in glob . glob ( '/home/arl/mycar/data/dans-msp/data/images/*.jpg' ): images . append ( mpimg . imread ( img_path )) plt . figure ( figsize = ( 20 , 10 )) columns = 5 for i , image in enumerate ( images ): plt . subplot ( len ( images ) / columns + 1 , columns , i + 1 ) plt . imshow ( image ) Sorting Images By File Name We can add an additional step if we want to sort the images by the file name: 1 2 image_file_name_list = os . listdir ( image_dir ) image_file_name_list . sort () Return Images Based On Creation Date 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import os import matplotlib.pyplot as plt from IPython.display import Image from pathlib import Path image_dir = \"/home/arl/mycar/data/dans-msp/data/images\" paths = sorted ( Path ( image_dir ) . iterdir (), key = os . path . getmtime ) images = [] # just get the first 10 items in the list of images for path in paths [: 10 ]: print ( path ) images . append ( mpimg . imread ( path )) plt . figure ( figsize = ( 20 , 10 )) columns = 5 for i , image in enumerate ( images ): plt . subplot ( len ( images ) / columns + 1 , columns , i + 1 ) plt . imshow ( image )","title":"Listing Files"},{"location":"lesson-plans/data-analysis/02-listing-files/#working-with-files","text":"","title":"Working with Files"},{"location":"lesson-plans/data-analysis/02-listing-files/#listing-files-with-the-os-library","text":"Python provides a powerful library for working with Operating System resources like file systems. We will start out with the listdir() function that lists the files in a directory. Here is program that lists all the tub files in our mycar/data directory: 1 2 3 4 5 6 import os data_dir = \"/home/arl/mycar/data/\" files = os . listdir ( data_dir ) for file in files : print ( file ) returns: 1 2 3 4 5 a-test-tub my-test-tub junk-tub production-run tub-47","title":"Listing Files with the OS library"},{"location":"lesson-plans/data-analysis/02-listing-files/#listing-files-in-a","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import os from IPython.display import Image image_dir = \"/home/arl/mycar/data/dans-msp/data/images\" files = os . listdir ( image_dir ) # last basement image is 1710 n = 1710 file_n = files [ n ] file_2 = files [ n + 1 ] print ( n , file_n ) file_path1 = image_dir + '/' + file_n file_path2 = image_dir + '/' + file_2 i1 = Image ( file_path1 ) i2 = Image ( file_path2 ) print ( n + 1 , file_2 ) display ( i1 , i2 )","title":"Listing Files in a"},{"location":"lesson-plans/data-analysis/02-listing-files/#list-random-files-in-images-directory","text":"1 2 3 4 5 6 7 8 9 10 import os import matplotlib.pyplot as plt from IPython.display import Image image_dir = \"/home/arl/mycar/data/dans-msp/data/images\" image_file_name_list = os . listdir ( image_dir ) for index in range ( 0 , 10 ): file_name = image_file_name_list [ index ] print ( file_name ) returns: 1 2 3 4 5 6 7 8 9 10 10263 _cam_image_array_ . jpg 6257 _cam_image_array_ . jpg 15248 _cam_image_array_ . jpg 3916 _cam_image_array_ . jpg 5223 _cam_image_array_ . jpg 15765 _cam_image_array_ . jpg 8437 _cam_image_array_ . jpg 5871 _cam_image_array_ . jpg 16681 _cam_image_array_ . jpg 15281 _cam_image_array_ . jpg Note that the files are not in any specific order.","title":"List Random Files In Images Directory"},{"location":"lesson-plans/data-analysis/02-listing-files/#show-images-for-10-random-files","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 import glob import matplotlib.pyplot as plt import matplotlib.image as mpimg % matplotlib inline images = [] for img_path in glob . glob ( '/home/arl/mycar/data/dans-msp/data/images/*.jpg' ): images . append ( mpimg . imread ( img_path )) plt . figure ( figsize = ( 20 , 10 )) columns = 5 for i , image in enumerate ( images ): plt . subplot ( len ( images ) / columns + 1 , columns , i + 1 ) plt . imshow ( image )","title":"Show Images for 10 Random Files"},{"location":"lesson-plans/data-analysis/02-listing-files/#sorting-images-by-file-name","text":"We can add an additional step if we want to sort the images by the file name: 1 2 image_file_name_list = os . listdir ( image_dir ) image_file_name_list . sort ()","title":"Sorting Images By File Name"},{"location":"lesson-plans/data-analysis/02-listing-files/#return-images-based-on-creation-date","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import os import matplotlib.pyplot as plt from IPython.display import Image from pathlib import Path image_dir = \"/home/arl/mycar/data/dans-msp/data/images\" paths = sorted ( Path ( image_dir ) . iterdir (), key = os . path . getmtime ) images = [] # just get the first 10 items in the list of images for path in paths [: 10 ]: print ( path ) images . append ( mpimg . imread ( path )) plt . figure ( figsize = ( 20 , 10 )) columns = 5 for i , image in enumerate ( images ): plt . subplot ( len ( images ) / columns + 1 , columns , i + 1 ) plt . imshow ( image )","title":"Return Images Based On Creation Date"},{"location":"lesson-plans/data-analysis/03-viewing-images/","text":"Viewing Images Viewing a Single JPG Image","title":"Viewing Images"},{"location":"lesson-plans/data-analysis/03-viewing-images/#viewing-images","text":"","title":"Viewing Images"},{"location":"lesson-plans/data-analysis/03-viewing-images/#viewing-a-single-jpg-image","text":"","title":"Viewing a Single JPG Image"},{"location":"lesson-plans/data-analysis/04-viewing-catalog-files/","text":"Viewing Catalog Files The data about each image, sometimes called the image \"metadata\", is stored in a file that ends with the file extension .catalog. If you open these files, you will see a simple layout that looks like the following: 1 2 3 {'_index': 16000, '_session_id': '21-07-20_0', '_timestamp_ms': 1626797545360, 'cam/image_array': '16000_cam_image_array_.jpg', 'user/angle': 1.0, 'user/mode': 'user', 'user/throttle': 0.5} {'_index': 16001, '_session_id': '21-07-20_0', '_timestamp_ms': 1626797545411, 'cam/image_array': '16001_cam_image_array_.jpg', 'user/angle': 0.37, 'user/mode': 'user', 'user/throttle': 0.7} {'_index': 16002, '_session_id': '21-07-20_0', '_timestamp_ms': 1626797545460, 'cam/image_array': '16002_cam_image_array_.jpg', 'user/angle': -0.23, 'user/mode': 'user', 'user/throttle': 0.25} This file consists of multiple lines, each line starts and ends with curly braces \"{\" and \"}\". Within these curly braces are a set of key-value pairs where the label is a string in single quotes followed by a colon, the value and a comma. This file uses newlines to separate records and a JSON object format within each single line. Note this is NOT a full JSON file format so you can't just use a standard JSON library to read the catalog file. Here is that format with a the key and value on separate lines to make the line easier to read. 1 2 3 4 5 6 7 8 9 10 { '_i n dex' : 16000 , '_sessio n _id' : ' 21-07-20 _ 0 ' , '_ t imes ta mp_ms' : 1626797545360 , 'cam/image_array' : ' 16000 _cam_image_array_.jpg' , 'user/a n gle' : 1.0 , 'user/mode' : 'user' , 'user/ t hro ttle ' : 0.5 } This format is very similar to a JSON file with the following exceptions: There is no root element to tell us when the JSON file starts and ends There are no commas between the item Here is what a properly formatted JSON file would look like: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 { \"driveData\" : [ { \"_index\" : 16000 , \"_session_id\" : \"21-07-20_0\" , \"_timestamp_ms\" : 1626797545360 , \"cam/image_array\" : \"16000_cam_image_array_.jpg\" , \"user/angle\" : 1.0 , \"user/mode\" : \"user\" , \"user/throttle\" : 0.5 }, { \"_index\" : 16001 , \"_session_id\" : \"21-07-20_0\" , \"_timestamp_ms\" : 1626797545411 , \"cam/image_array\" : \"16001_cam_image_array_.jpg\" , \"user/angle\" : 0.37 , \"user/mode\" : \"user\" , \"user/throttle\" : 0.70 }, { \"_index\" : 16002 , \"_session_id\" : \"21-07-20_0\" , \"_timestamp_ms\" : 1626797545460 , \"cam/image_array\" : \"16002_cam_image_array_.jpg\" , \"user/angle\" : -0.23 , \"user/mode\" : \"user\" , \"user/throttle\" : 0.25 } ] } Here is a sample JSON file reader that would read this file: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # program to read a DonkeyCar Catalog File import os , json # this program assumes that test.json is in the same directory as this script # get the direcotry that this script is running script_dir = os . path . dirname ( __file__ ) # get a relative path to the script dir path_to_json_file = script_dir + '/test.json' # Open the JSON test file for read only f = open ( path_to_json_file , 'r' ) # returns JSON object as a dictionary data = json . load ( f ) # Iterating through the json file for the items in the drive data dictionary for i in data [ 'driveData' ]: print ( i ) # Close the JSON file f . close () Note that the open() function reads the file with the \"r\" option which indicates read-only mode. Although this format would make reading the file simple, there are some disadvantages. The key is that individual lines in the new catalog format are atomic units of storage and the files can be easily split and joined using line-by-line tools. Reading Catalog Lines to JSON Objects To read in the values of the catalog file we will open using a line-oriented data structure assuming that there is a newline at the end of each record. We can then just the json library's loads() function which will convert each line to a JSON object. Sample Objects.json file: 1 2 3 4 {\"name\":\"Ann\",\"age\":15} {\"name\":\"Peggy\",\"age\":16} {\"name\":\"Rima\",\"age\":13} {\"name\":\"Sue\",\"age\":14} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import os , json json_file = \"objects.json\" script_dir = os . path . dirname ( __file__ ) # get a relative path to the script dir path_to_catalog_file = script_dir + '/' + json_file f = open ( path_to_catalog_file ) lines = f . readlines () count = 0 # Convert each line to a JSON object for line in lines : line_in_json = json . loads ( line ) count += 1 print ( count , ' ' , end = '' ) print ( line_in_json ) # the result is a Python dictionary print ( line_in_json [ 'name' ]) print ( \"Name:\" , line_to_json [ \"name\" ] ) print ( \"Age:\" , line_to_json [ \"age\" ] ) Returns 1 2 3 4 5 6 7 8 9 10 11 12 1 {' name ' : ' Ann ' , ' age ' : 15 } Name : Ann Age : 15 2 {' name ' : ' Peggy ' , ' age ' : 16 } Name : Peggy Age : 16 3 {' name ' : ' Rima ' , ' age ' : 13 } Name : Rima Age : 13 4 {' name ' : ' Sue ' , ' age ' : 14 } Name : Sue Age : 14 Sample CataLog Reader Program 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # program to read a DonkeyCar Catalog File import os , json # this program assumes that test.catalog is in the same directory as this script # get the direcotry that this script is running script_dir = os . path . dirname ( __file__ ) # get a relative path to the script dir path_to_catalog_file = script_dir + '/test.catalog' f = open ( path_to_catalog_file ) lines = f . readlines () count = 0 # Convert each line to a JSON object for line in lines : # each line as a JSON dictionary object j = json . loads ( line ) count += 1 print ( ' \\n\\n line:' , count ) # print(j) print ( \"Index:\" , j [ \"_index\" ] ) print ( \"Session:\" , j [ \"_session_id\" ] ) print ( \"Timestamp:\" , j [ \"_timestamp_ms\" ] ) print ( \"cam/image_array:\" , j [ \"cam/image_array\" ] ) print ( \"user/angle:\" , j [ \"user/angle\" ] ) print ( \"user/mode:\" , j [ \"user/mode\" ] ) print ( \"user/throttle:\" , j [ \"user/throttle\" ] ) returns: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 line : 1 Index : 16000 Session : 21 - 07 - 20 _0 Timestamp : 1626797545360 cam / image_array : 16000 _cam_image_array_ . jpg user / angle : 1.0 user / mode : user user / throttle : 0.31 line : 2 Index : 16001 Session : 21 - 07 - 20 _0 Timestamp : 1626797545411 cam / image_array : 16001 _cam_image_array_ . jpg user / angle : 0.3715323343607898 user / mode : user user / throttle : 0.31 line : 3 Index : 16002 Session : 21 - 07 - 20 _0 Timestamp : 1626797545460 cam / image_array : 16002 _cam_image_array_ . jpg user / angle : 0.2371288186284982 user / mode : user user / throttle : 0.31 Reference The Python class that creates version 2 of the catalog files is here","title":"Viewing Catalog Files"},{"location":"lesson-plans/data-analysis/04-viewing-catalog-files/#viewing-catalog-files","text":"The data about each image, sometimes called the image \"metadata\", is stored in a file that ends with the file extension .catalog. If you open these files, you will see a simple layout that looks like the following: 1 2 3 {'_index': 16000, '_session_id': '21-07-20_0', '_timestamp_ms': 1626797545360, 'cam/image_array': '16000_cam_image_array_.jpg', 'user/angle': 1.0, 'user/mode': 'user', 'user/throttle': 0.5} {'_index': 16001, '_session_id': '21-07-20_0', '_timestamp_ms': 1626797545411, 'cam/image_array': '16001_cam_image_array_.jpg', 'user/angle': 0.37, 'user/mode': 'user', 'user/throttle': 0.7} {'_index': 16002, '_session_id': '21-07-20_0', '_timestamp_ms': 1626797545460, 'cam/image_array': '16002_cam_image_array_.jpg', 'user/angle': -0.23, 'user/mode': 'user', 'user/throttle': 0.25} This file consists of multiple lines, each line starts and ends with curly braces \"{\" and \"}\". Within these curly braces are a set of key-value pairs where the label is a string in single quotes followed by a colon, the value and a comma. This file uses newlines to separate records and a JSON object format within each single line. Note this is NOT a full JSON file format so you can't just use a standard JSON library to read the catalog file. Here is that format with a the key and value on separate lines to make the line easier to read. 1 2 3 4 5 6 7 8 9 10 { '_i n dex' : 16000 , '_sessio n _id' : ' 21-07-20 _ 0 ' , '_ t imes ta mp_ms' : 1626797545360 , 'cam/image_array' : ' 16000 _cam_image_array_.jpg' , 'user/a n gle' : 1.0 , 'user/mode' : 'user' , 'user/ t hro ttle ' : 0.5 } This format is very similar to a JSON file with the following exceptions: There is no root element to tell us when the JSON file starts and ends There are no commas between the item Here is what a properly formatted JSON file would look like: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 { \"driveData\" : [ { \"_index\" : 16000 , \"_session_id\" : \"21-07-20_0\" , \"_timestamp_ms\" : 1626797545360 , \"cam/image_array\" : \"16000_cam_image_array_.jpg\" , \"user/angle\" : 1.0 , \"user/mode\" : \"user\" , \"user/throttle\" : 0.5 }, { \"_index\" : 16001 , \"_session_id\" : \"21-07-20_0\" , \"_timestamp_ms\" : 1626797545411 , \"cam/image_array\" : \"16001_cam_image_array_.jpg\" , \"user/angle\" : 0.37 , \"user/mode\" : \"user\" , \"user/throttle\" : 0.70 }, { \"_index\" : 16002 , \"_session_id\" : \"21-07-20_0\" , \"_timestamp_ms\" : 1626797545460 , \"cam/image_array\" : \"16002_cam_image_array_.jpg\" , \"user/angle\" : -0.23 , \"user/mode\" : \"user\" , \"user/throttle\" : 0.25 } ] } Here is a sample JSON file reader that would read this file: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # program to read a DonkeyCar Catalog File import os , json # this program assumes that test.json is in the same directory as this script # get the direcotry that this script is running script_dir = os . path . dirname ( __file__ ) # get a relative path to the script dir path_to_json_file = script_dir + '/test.json' # Open the JSON test file for read only f = open ( path_to_json_file , 'r' ) # returns JSON object as a dictionary data = json . load ( f ) # Iterating through the json file for the items in the drive data dictionary for i in data [ 'driveData' ]: print ( i ) # Close the JSON file f . close () Note that the open() function reads the file with the \"r\" option which indicates read-only mode. Although this format would make reading the file simple, there are some disadvantages. The key is that individual lines in the new catalog format are atomic units of storage and the files can be easily split and joined using line-by-line tools.","title":"Viewing Catalog Files"},{"location":"lesson-plans/data-analysis/04-viewing-catalog-files/#reading-catalog-lines-to-json-objects","text":"To read in the values of the catalog file we will open using a line-oriented data structure assuming that there is a newline at the end of each record. We can then just the json library's loads() function which will convert each line to a JSON object. Sample Objects.json file: 1 2 3 4 {\"name\":\"Ann\",\"age\":15} {\"name\":\"Peggy\",\"age\":16} {\"name\":\"Rima\",\"age\":13} {\"name\":\"Sue\",\"age\":14} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import os , json json_file = \"objects.json\" script_dir = os . path . dirname ( __file__ ) # get a relative path to the script dir path_to_catalog_file = script_dir + '/' + json_file f = open ( path_to_catalog_file ) lines = f . readlines () count = 0 # Convert each line to a JSON object for line in lines : line_in_json = json . loads ( line ) count += 1 print ( count , ' ' , end = '' ) print ( line_in_json ) # the result is a Python dictionary print ( line_in_json [ 'name' ]) print ( \"Name:\" , line_to_json [ \"name\" ] ) print ( \"Age:\" , line_to_json [ \"age\" ] ) Returns 1 2 3 4 5 6 7 8 9 10 11 12 1 {' name ' : ' Ann ' , ' age ' : 15 } Name : Ann Age : 15 2 {' name ' : ' Peggy ' , ' age ' : 16 } Name : Peggy Age : 16 3 {' name ' : ' Rima ' , ' age ' : 13 } Name : Rima Age : 13 4 {' name ' : ' Sue ' , ' age ' : 14 } Name : Sue Age : 14","title":"Reading Catalog Lines to JSON Objects"},{"location":"lesson-plans/data-analysis/04-viewing-catalog-files/#sample-catalog-reader-program","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # program to read a DonkeyCar Catalog File import os , json # this program assumes that test.catalog is in the same directory as this script # get the direcotry that this script is running script_dir = os . path . dirname ( __file__ ) # get a relative path to the script dir path_to_catalog_file = script_dir + '/test.catalog' f = open ( path_to_catalog_file ) lines = f . readlines () count = 0 # Convert each line to a JSON object for line in lines : # each line as a JSON dictionary object j = json . loads ( line ) count += 1 print ( ' \\n\\n line:' , count ) # print(j) print ( \"Index:\" , j [ \"_index\" ] ) print ( \"Session:\" , j [ \"_session_id\" ] ) print ( \"Timestamp:\" , j [ \"_timestamp_ms\" ] ) print ( \"cam/image_array:\" , j [ \"cam/image_array\" ] ) print ( \"user/angle:\" , j [ \"user/angle\" ] ) print ( \"user/mode:\" , j [ \"user/mode\" ] ) print ( \"user/throttle:\" , j [ \"user/throttle\" ] ) returns: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 line : 1 Index : 16000 Session : 21 - 07 - 20 _0 Timestamp : 1626797545360 cam / image_array : 16000 _cam_image_array_ . jpg user / angle : 1.0 user / mode : user user / throttle : 0.31 line : 2 Index : 16001 Session : 21 - 07 - 20 _0 Timestamp : 1626797545411 cam / image_array : 16001 _cam_image_array_ . jpg user / angle : 0.3715323343607898 user / mode : user user / throttle : 0.31 line : 3 Index : 16002 Session : 21 - 07 - 20 _0 Timestamp : 1626797545460 cam / image_array : 16002 _cam_image_array_ . jpg user / angle : 0.2371288186284982 user / mode : user user / throttle : 0.31","title":"Sample CataLog Reader Program"},{"location":"lesson-plans/data-analysis/04-viewing-catalog-files/#reference","text":"The Python class that creates version 2 of the catalog files is here","title":"Reference"},{"location":"lesson-plans/data-analysis/05-catalog-statistics/","text":"Catalog Statistics Now that we know how to reach each item in the tub catalog, we can now do some simple statistics on this data. For example we can calculate the average throttle and steering angle and create some plots of the distribution of these values. Calculating Average Throttle and Angle When we drive around the track each image records both the throttle and steering values at the instant the image was taken by the camera. Although the values sent to the Electronic Speed Controller (ESC) and the servo are unique to every car, instead we store values that have been converted to a range between 0 and 1. Both these values are Normalized to values of between 0 and 1. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # program to read a DonkeyCar Catalog File import os , json # this program assumes that test.catalog is in the same directory as this script # get the direcotry that this script is running script_dir = os . path . dirname ( __file__ ) # get a relative path to the script dir path_to_catalog_file = script_dir + '/test.catalog' f = open ( path_to_catalog_file ) lines = f . readlines () # create a dictionary object dict = {} count = 0 total_throttle = 0 total_angle = 0 # Add each line to our dictionary for line in lines : # each line as a JSON dictionary object j = json . loads ( line ) count += 1 dict . update ( json . loads ( line )) total_throttle += j [ \"user/throttle\" ] total_angle += j [ \"user/angle\" ] print ( count , \"items in dictionary\" ) print ( \"Average throttle: \" , round ( total_throttle / count , 3 )) print ( \"Average angle:\" , round ( total_angle / count , 3 )) returns: 1 2 3 100 items in dictionary Average throttle : 0.31 Average angle : 0.53 These values look reasonable. Our throttle should be between 0 and 1 and our average steering should be around 0.5. If we drive in a pure circle only in a single direction the average angle will be offset from the 0.5 center value. Viewing Min and Max values 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 # program to read a DonkeyCar Catalog File import os , json # this program assumes that test.catalog is in the same directory as this script # get the direcotry that this script is running script_dir = os . path . dirname ( __file__ ) # get a relative path to the script dir path_to_catalog_file = script_dir + '/test.catalog' f = open ( path_to_catalog_file ) lines = f . readlines () # create a dictionary object dict = {} count = 0 total_throttle = 0 min_throttle = 1 max_throttle = 0 total_angle = 0 min_angle = 1 max_angle = 0 # Add each line to our dictionary for line in lines : # each line as a JSON dictionary object j = json . loads ( line ) count += 1 dict . update ( json . loads ( line )) total_throttle += j [ \"user/throttle\" ] total_angle += j [ \"user/angle\" ] # check for min and max throttle if j [ \"user/throttle\" ] < min_throttle : min_throttle = j [ \"user/throttle\" ] if j [ \"user/throttle\" ] > max_throttle : max_throttle = j [ \"user/throttle\" ] if j [ \"user/angle\" ] < min_angle : min_angle = j [ \"user/angle\" ] if j [ \"user/angle\" ] > max_angle : max_angle = j [ \"user/angle\" ] print ( ' \\n ' ) print ( count , \"items in catalog\" ) print ( \"Min throttle:\" , round ( min_throttle , 3 )) print ( \"Average throttle: \" , round ( total_throttle / count , 3 )) print ( \"Max throttle:\" , round ( max_throttle , 3 )) print ( \"Min angle:\" , round ( min_throttle , 3 )) print ( \"Average angle:\" , round ( total_angle / count , 3 )) print ( \"Max angle:\" , round ( max_angle , 3 )) print ( ' \\n ' ) returns: 1 2 3 4 5 6 7 100 items in catalog Min throttle : - 0.31 Average throttle : 0.308 Max throttle : 0.5 Min angle : - 0.31 Average angle : 0.534 Max angle : 1.0 Converting the Dictionary to a DataFrame 1 2 df = pd . DataFrame ( list ( dict . items ())) print ( df ) returns 1 2 3 4 5 6 7 8 0 1 0 _index 16099 1 _session_id 21-07-20_1 2 _timestamp_ms 1626797880229 3 cam/image_array 16099_cam_image_array_.jpg 4 user/angle 0.56914 5 user/mode user 6 user/throttle 0.0632649 Plotting Steering Distributions","title":"Catalog Statistics"},{"location":"lesson-plans/data-analysis/05-catalog-statistics/#catalog-statistics","text":"Now that we know how to reach each item in the tub catalog, we can now do some simple statistics on this data. For example we can calculate the average throttle and steering angle and create some plots of the distribution of these values.","title":"Catalog Statistics"},{"location":"lesson-plans/data-analysis/05-catalog-statistics/#calculating-average-throttle-and-angle","text":"When we drive around the track each image records both the throttle and steering values at the instant the image was taken by the camera. Although the values sent to the Electronic Speed Controller (ESC) and the servo are unique to every car, instead we store values that have been converted to a range between 0 and 1. Both these values are Normalized to values of between 0 and 1. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # program to read a DonkeyCar Catalog File import os , json # this program assumes that test.catalog is in the same directory as this script # get the direcotry that this script is running script_dir = os . path . dirname ( __file__ ) # get a relative path to the script dir path_to_catalog_file = script_dir + '/test.catalog' f = open ( path_to_catalog_file ) lines = f . readlines () # create a dictionary object dict = {} count = 0 total_throttle = 0 total_angle = 0 # Add each line to our dictionary for line in lines : # each line as a JSON dictionary object j = json . loads ( line ) count += 1 dict . update ( json . loads ( line )) total_throttle += j [ \"user/throttle\" ] total_angle += j [ \"user/angle\" ] print ( count , \"items in dictionary\" ) print ( \"Average throttle: \" , round ( total_throttle / count , 3 )) print ( \"Average angle:\" , round ( total_angle / count , 3 )) returns: 1 2 3 100 items in dictionary Average throttle : 0.31 Average angle : 0.53 These values look reasonable. Our throttle should be between 0 and 1 and our average steering should be around 0.5. If we drive in a pure circle only in a single direction the average angle will be offset from the 0.5 center value.","title":"Calculating Average Throttle and Angle"},{"location":"lesson-plans/data-analysis/05-catalog-statistics/#viewing-min-and-max-values","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 # program to read a DonkeyCar Catalog File import os , json # this program assumes that test.catalog is in the same directory as this script # get the direcotry that this script is running script_dir = os . path . dirname ( __file__ ) # get a relative path to the script dir path_to_catalog_file = script_dir + '/test.catalog' f = open ( path_to_catalog_file ) lines = f . readlines () # create a dictionary object dict = {} count = 0 total_throttle = 0 min_throttle = 1 max_throttle = 0 total_angle = 0 min_angle = 1 max_angle = 0 # Add each line to our dictionary for line in lines : # each line as a JSON dictionary object j = json . loads ( line ) count += 1 dict . update ( json . loads ( line )) total_throttle += j [ \"user/throttle\" ] total_angle += j [ \"user/angle\" ] # check for min and max throttle if j [ \"user/throttle\" ] < min_throttle : min_throttle = j [ \"user/throttle\" ] if j [ \"user/throttle\" ] > max_throttle : max_throttle = j [ \"user/throttle\" ] if j [ \"user/angle\" ] < min_angle : min_angle = j [ \"user/angle\" ] if j [ \"user/angle\" ] > max_angle : max_angle = j [ \"user/angle\" ] print ( ' \\n ' ) print ( count , \"items in catalog\" ) print ( \"Min throttle:\" , round ( min_throttle , 3 )) print ( \"Average throttle: \" , round ( total_throttle / count , 3 )) print ( \"Max throttle:\" , round ( max_throttle , 3 )) print ( \"Min angle:\" , round ( min_throttle , 3 )) print ( \"Average angle:\" , round ( total_angle / count , 3 )) print ( \"Max angle:\" , round ( max_angle , 3 )) print ( ' \\n ' ) returns: 1 2 3 4 5 6 7 100 items in catalog Min throttle : - 0.31 Average throttle : 0.308 Max throttle : 0.5 Min angle : - 0.31 Average angle : 0.534 Max angle : 1.0","title":"Viewing Min and Max values"},{"location":"lesson-plans/data-analysis/05-catalog-statistics/#converting-the-dictionary-to-a-dataframe","text":"1 2 df = pd . DataFrame ( list ( dict . items ())) print ( df ) returns 1 2 3 4 5 6 7 8 0 1 0 _index 16099 1 _session_id 21-07-20_1 2 _timestamp_ms 1626797880229 3 cam/image_array 16099_cam_image_array_.jpg 4 user/angle 0.56914 5 user/mode user 6 user/throttle 0.0632649","title":"Converting the Dictionary to a DataFrame"},{"location":"lesson-plans/data-analysis/05-catalog-statistics/#plotting-steering-distributions","text":"","title":"Plotting Steering Distributions"},{"location":"lesson-plans/data-analysis/06-cleaning-datasets/","text":"Cleaning Datasets Up until now, we have only been viewing metrics and files. These are all read-only operations. Now we will write our first programs that change the tub datasets. Splitting Datasets In this lab we will assume that we want to break our data into two distinct subsets and place them in different \"tubs\", which are just directories that contain both the catalogs and images for a dataset. You can begin by taking a single dataset in a tub and then duplicating that tub. You can then selectively remove the data from the two tubs to effectively split the tubs. The UNIX shell command to copy an entire directly is the \"cp\" command with the \"-r\" option for recursive copy. 1 cp -r from-dir to-dir You can also add the \"-v\" option to see what files are being copied.","title":"Cleaning Datasets"},{"location":"lesson-plans/data-analysis/06-cleaning-datasets/#cleaning-datasets","text":"Up until now, we have only been viewing metrics and files. These are all read-only operations. Now we will write our first programs that change the tub datasets.","title":"Cleaning Datasets"},{"location":"lesson-plans/data-analysis/06-cleaning-datasets/#splitting-datasets","text":"In this lab we will assume that we want to break our data into two distinct subsets and place them in different \"tubs\", which are just directories that contain both the catalogs and images for a dataset. You can begin by taking a single dataset in a tub and then duplicating that tub. You can then selectively remove the data from the two tubs to effectively split the tubs. The UNIX shell command to copy an entire directly is the \"cp\" command with the \"-r\" option for recursive copy. 1 cp -r from-dir to-dir You can also add the \"-v\" option to see what files are being copied.","title":"Splitting Datasets"},{"location":"lesson-plans/jupyter-notebooks/","text":"Introduction to Jupyter Notebooks and Basic Data Analysis Learning Objectives By the end of this lesson, students should be able to: Understand what Jupyter Notebooks are and how to use them effectively. Load and inspect data from CSV and JSON files in a Jupyter Notebook. Perform basic data analysis using Python libraries such as pandas and matplotlib. Apply their knowledge to analyze data from Donkey Car projects. Required Materials and Preparation Access to a computer with Python, Jupyter Notebooks, pandas, and matplotlib installed. Sample data sets from Donkey Car projects in both CSV and JSON formats. Previous knowledge in Python programming. Lesson Breakdown Lesson 1: Introduction to Jupyter Notebooks (2 hours) 1.1 Lecture: What is a Jupyter Notebook? (30 mins) Definition and purpose Features and benefits of using Jupyter Notebooks 1.2 Hands-on Activity: Getting Started with Jupyter Notebook (90 mins) Launching a Jupyter Notebook Familiarizing with the interface Creating, editing, and executing cells Markdown syntax and use Saving and sharing Jupyter Notebooks Lesson 2: Data Loading and Inspection in Jupyter Notebooks (2 hours) 2.1 Lecture: Basics of pandas (30 mins) Overview of pandas Creating dataframes Basic dataframe operations 2.2 Hands-on Activity: Loading and Inspecting Data (90 mins) Reading data from CSV and JSON files with pandas Inspecting data: checking the dimensions, viewing the first/last few rows, data types Data summary statistics: using describe() Lesson 3: Basic Data Analysis in Jupyter Notebooks (3 hours) 3.1 Lecture: Data Analysis with pandas (30 mins) Filtering and selecting data Grouping and aggregation Basic plotting with pandas 3.2 Hands-on Activity: Basic Data Analysis (150 mins) Practical exercises for data selection, filtering, and aggregation Creating basic plots to visualize data insights Exploring the data to answer exploratory questions Lesson 4: Data Analysis of Donkey Car Project Data (3 hours) 4.1 Recap: Overview of the Donkey Car project (30 mins) Overview of the Donkey Car project and the associated datasets 4.2 Hands-on Activity: Donkey Car Data Analysis (150 mins) Loading and inspecting Donkey Car project datasets Performing exploratory data analysis: answering specific questions, making plots, extracting insights Discussion: Sharing insights, potential improvements for the Donkey Car project based on the data Evaluation Students' understanding will be evaluated through their participation in the hands-on activities and the insights they generate from the Donkey Car project's data analysis. An end-of-unit quiz will also be provided to assess their theoretical understanding and practical skills in Jupyter Notebooks and data analysis. Extension Activities Advanced Data Analysis: Introduce students to more advanced data analysis techniques such as correlation analysis, data normalization, and pivot tables. Data Visualization: Teach students about more complex visualizations using libraries such as seaborn or plotly. Machine Learning Introduction: Provide a brief overview of how the data they have analyzed could be used to train a machine learning model.","title":"Jupyter Notebooks"},{"location":"lesson-plans/jupyter-notebooks/#introduction-to-jupyter-notebooks-and-basic-data-analysis","text":"","title":"Introduction to Jupyter Notebooks and Basic Data Analysis"},{"location":"lesson-plans/jupyter-notebooks/#learning-objectives","text":"By the end of this lesson, students should be able to: Understand what Jupyter Notebooks are and how to use them effectively. Load and inspect data from CSV and JSON files in a Jupyter Notebook. Perform basic data analysis using Python libraries such as pandas and matplotlib. Apply their knowledge to analyze data from Donkey Car projects.","title":"Learning Objectives"},{"location":"lesson-plans/jupyter-notebooks/#required-materials-and-preparation","text":"Access to a computer with Python, Jupyter Notebooks, pandas, and matplotlib installed. Sample data sets from Donkey Car projects in both CSV and JSON formats. Previous knowledge in Python programming.","title":"Required Materials and Preparation"},{"location":"lesson-plans/jupyter-notebooks/#lesson-breakdown","text":"Lesson 1: Introduction to Jupyter Notebooks (2 hours) 1.1 Lecture: What is a Jupyter Notebook? (30 mins) Definition and purpose Features and benefits of using Jupyter Notebooks 1.2 Hands-on Activity: Getting Started with Jupyter Notebook (90 mins) Launching a Jupyter Notebook Familiarizing with the interface Creating, editing, and executing cells Markdown syntax and use Saving and sharing Jupyter Notebooks Lesson 2: Data Loading and Inspection in Jupyter Notebooks (2 hours) 2.1 Lecture: Basics of pandas (30 mins) Overview of pandas Creating dataframes Basic dataframe operations 2.2 Hands-on Activity: Loading and Inspecting Data (90 mins) Reading data from CSV and JSON files with pandas Inspecting data: checking the dimensions, viewing the first/last few rows, data types Data summary statistics: using describe() Lesson 3: Basic Data Analysis in Jupyter Notebooks (3 hours) 3.1 Lecture: Data Analysis with pandas (30 mins) Filtering and selecting data Grouping and aggregation Basic plotting with pandas 3.2 Hands-on Activity: Basic Data Analysis (150 mins) Practical exercises for data selection, filtering, and aggregation Creating basic plots to visualize data insights Exploring the data to answer exploratory questions Lesson 4: Data Analysis of Donkey Car Project Data (3 hours) 4.1 Recap: Overview of the Donkey Car project (30 mins) Overview of the Donkey Car project and the associated datasets 4.2 Hands-on Activity: Donkey Car Data Analysis (150 mins) Loading and inspecting Donkey Car project datasets Performing exploratory data analysis: answering specific questions, making plots, extracting insights Discussion: Sharing insights, potential improvements for the Donkey Car project based on the data","title":"Lesson Breakdown"},{"location":"lesson-plans/jupyter-notebooks/#evaluation","text":"Students' understanding will be evaluated through their participation in the hands-on activities and the insights they generate from the Donkey Car project's data analysis. An end-of-unit quiz will also be provided to assess their theoretical understanding and practical skills in Jupyter Notebooks and data analysis.","title":"Evaluation"},{"location":"lesson-plans/jupyter-notebooks/#extension-activities","text":"Advanced Data Analysis: Introduce students to more advanced data analysis techniques such as correlation analysis, data normalization, and pivot tables. Data Visualization: Teach students about more complex visualizations using libraries such as seaborn or plotly. Machine Learning Introduction: Provide a brief overview of how the data they have analyzed could be used to train a machine learning model.","title":"Extension Activities"},{"location":"quizzes/gpu-commands/","text":"","title":"GPU Commands"},{"location":"quizzes/gpu-shell-commands/","text":"GPU Shell Commands Quiz When you use a new GPU server at an AI Racing League event there are many questions you need to have answered about your GPU server. Here is a handy quiz you can use to check your knowledge. The answers to the questions are listed below. Questions Question 1: How would you log into the GPU server using the secure shell program? A) $ login arl@arl1.local B) $ ssh arl@arl1.local C) $ enter arl@arl1.local D) $ connect arl@arl1.local Question 2: How would you check the version of Ubuntu on the GPU server? A) $ version -a B) $ lsb_release -a C) $ ubuntu_version -all D) $ check_ubuntu -a Question 3: What information does the lscpu command provide? A) It provides the CPU information. B) It lists the amount of RAM on the server. C) It checks the disk space. D) It shows per-user disk usage. Question 4: Which command is used to check the total RAM on the GPU server? A) $ free -m B) $ checkram -m C) $ listram -m D) $ raminfo -m Question 5: What does the command df -h / provide? A) It lists per user disk usage. B) It adds a new GPU server user. C) It checks the disk space. D) It monitors the NVIDIA GPU. Question 6: How can a new GPU server user be added? A) $ adduser <username> B) $ newuser <username> C) $ createuser <username> D) $ useradd <username> Question 7: How can you give a user \"sudo\" rights? A) $ sudo usermod -aG sudo <username> B) $ sudo addrights -aG sudo <username> C) $ sudo giverights -aG sudo <username> D) $ sudo addrules -aG sudo <username> Question 8: How can the hostname be changed? A) $ sudo vi hostname B) $ sudo edit hostname C) $ sudo change hostname D) $ sudo alter hostname Question 9: What does the command watch -d -n 0.5 nvidia-smi do? A) It adds a new GPU server user. B) It runs similar to the UNIX top command, but for the GPU. C) It checks the version of Ubuntu. D) It lists the CPU information. Question 10: How would you check the NVIDIA GPU utilization? A) $ checkgpu B) $ nvidia-smi C) $ gpu-utilization D) $ utilization nvidia Answers B) $ ssh arl@arl1.local B) $ lsb_release -a A) It provides the CPU information. A) $ free -m C) It checks the disk space. A) $ adduser <username> A) $ sudo usermod -aG sudo <username> A) $ sudo vi hostname B) It runs similar to the UNIX top command, but for the GPU. B) $ nvidia-smi","title":"GPU Shell Commands Quiz"},{"location":"quizzes/gpu-shell-commands/#gpu-shell-commands-quiz","text":"When you use a new GPU server at an AI Racing League event there are many questions you need to have answered about your GPU server. Here is a handy quiz you can use to check your knowledge. The answers to the questions are listed below.","title":"GPU Shell Commands Quiz"},{"location":"quizzes/gpu-shell-commands/#questions","text":"Question 1: How would you log into the GPU server using the secure shell program? A) $ login arl@arl1.local B) $ ssh arl@arl1.local C) $ enter arl@arl1.local D) $ connect arl@arl1.local Question 2: How would you check the version of Ubuntu on the GPU server? A) $ version -a B) $ lsb_release -a C) $ ubuntu_version -all D) $ check_ubuntu -a Question 3: What information does the lscpu command provide? A) It provides the CPU information. B) It lists the amount of RAM on the server. C) It checks the disk space. D) It shows per-user disk usage. Question 4: Which command is used to check the total RAM on the GPU server? A) $ free -m B) $ checkram -m C) $ listram -m D) $ raminfo -m Question 5: What does the command df -h / provide? A) It lists per user disk usage. B) It adds a new GPU server user. C) It checks the disk space. D) It monitors the NVIDIA GPU. Question 6: How can a new GPU server user be added? A) $ adduser <username> B) $ newuser <username> C) $ createuser <username> D) $ useradd <username> Question 7: How can you give a user \"sudo\" rights? A) $ sudo usermod -aG sudo <username> B) $ sudo addrights -aG sudo <username> C) $ sudo giverights -aG sudo <username> D) $ sudo addrules -aG sudo <username> Question 8: How can the hostname be changed? A) $ sudo vi hostname B) $ sudo edit hostname C) $ sudo change hostname D) $ sudo alter hostname Question 9: What does the command watch -d -n 0.5 nvidia-smi do? A) It adds a new GPU server user. B) It runs similar to the UNIX top command, but for the GPU. C) It checks the version of Ubuntu. D) It lists the CPU information. Question 10: How would you check the NVIDIA GPU utilization? A) $ checkgpu B) $ nvidia-smi C) $ gpu-utilization D) $ utilization nvidia","title":"Questions"},{"location":"quizzes/gpu-shell-commands/#answers","text":"B) $ ssh arl@arl1.local B) $ lsb_release -a A) It provides the CPU information. A) $ free -m C) It checks the disk space. A) $ adduser <username> A) $ sudo usermod -aG sudo <username> A) $ sudo vi hostname B) It runs similar to the UNIX top command, but for the GPU. B) $ nvidia-smi","title":"Answers"},{"location":"setup/battery-options/","text":"Battery Options Dual Batteries In general, most of our cars use two separate batteries: One battery for the power to the drive motors and steering servos One battery to power the single-board computer and power the sensors like the camera Getting batteries charged before each event requires some strong organizational skills. Although the LiPo batteries retain a charge for a long time, the RC car batteries must be fully charged the night before each event. 18650 Standards The 18650 is a standard cell size for 3.7 volt Lithium batteries (18mmX65mm). The problem is that if these batteries go below 3.2 volts they become damaged and can't be recharged. This is why you should never use these batteries without charge protection circuits. A typical 18650 battery produces 4.2 V when fully charged with a nominal voltage of 3.7 V. The TP4056 is a low-cost charger chip for one (1) cell. If you plan to charge multiple cells simultaneously, each charging slot for a cell needs its own TP4056. !!! Warning Never connect a 5V power supply directly to a lithium cell. Even though the cells have built-in protection, they are not chargers. It is the last line of protection to prevent catastrophic failures by disconnecting the cell under abnormal conditions - unless the protection does not work. Boost and Buck Converters These are low-cost DC-to-DC voltage converters that allow you to use standard 3.7-volt cells that drive a 5-volt system. Boost converters increase the voltage and buck converters lower the voltage. You can purchase basic devices for $2, but fancier versions have displays that show both the input and output voltages. Boost Converters on eBay Buck Converters on eBay Computer Batteries We initially used several Ankar 2,000 milliamp-hour batteries for powering the cars. The batteries would last for the entire single-day events as long as they were charged before the event and not used to power the cars when not running on the tracks. The batteries typically could be recharged over 100 times before they had to be replaced. Many teams use larger (and heavier) 10,000 milliamp-hour Lithium batteries. These are especially useful when learning to drive around the track. Teams also loved the fact that the INIU batteries showed the percent power remaining. The INIU Portable Charger, Slimmest 10000mAh 5V/3A Power Bank is also a popular choice since they only cost about $19. RC Car Batteries Battery Cables Several participants used long battery cables with a small wire gauge. These cables caused voltage drops that made the cars stop working. We told all teams to use short 8-inch battery cables and most of these problems went away. We strongly suggest you build your own cables and use 18-gauge wire. Sample 1ft Charging Cable Connectors and Crimping Tool Ferrule Crimping Tool Kit Camera Batteries The camera industry has also developed standard 7.2-volt battery packs that enclose low-cost 18650 battery cells. These standards have electronics built in to protect from drainage. SmallRig NP-F Battery Plate Adapter Generic NP-F 550 2 Cell Battery Generic NP-F 750 4 Cell Battery Antigravity Micro-Start XP-1 Battery Related References NVIDIA Jetson on Battery Power - a nice video by","title":"Battery Options"},{"location":"setup/battery-options/#battery-options","text":"","title":"Battery Options"},{"location":"setup/battery-options/#dual-batteries","text":"In general, most of our cars use two separate batteries: One battery for the power to the drive motors and steering servos One battery to power the single-board computer and power the sensors like the camera Getting batteries charged before each event requires some strong organizational skills. Although the LiPo batteries retain a charge for a long time, the RC car batteries must be fully charged the night before each event.","title":"Dual Batteries"},{"location":"setup/battery-options/#18650-standards","text":"The 18650 is a standard cell size for 3.7 volt Lithium batteries (18mmX65mm). The problem is that if these batteries go below 3.2 volts they become damaged and can't be recharged. This is why you should never use these batteries without charge protection circuits. A typical 18650 battery produces 4.2 V when fully charged with a nominal voltage of 3.7 V. The TP4056 is a low-cost charger chip for one (1) cell. If you plan to charge multiple cells simultaneously, each charging slot for a cell needs its own TP4056. !!! Warning Never connect a 5V power supply directly to a lithium cell. Even though the cells have built-in protection, they are not chargers. It is the last line of protection to prevent catastrophic failures by disconnecting the cell under abnormal conditions - unless the protection does not work.","title":"18650 Standards"},{"location":"setup/battery-options/#boost-and-buck-converters","text":"These are low-cost DC-to-DC voltage converters that allow you to use standard 3.7-volt cells that drive a 5-volt system. Boost converters increase the voltage and buck converters lower the voltage. You can purchase basic devices for $2, but fancier versions have displays that show both the input and output voltages. Boost Converters on eBay Buck Converters on eBay","title":"Boost and Buck Converters"},{"location":"setup/battery-options/#computer-batteries","text":"We initially used several Ankar 2,000 milliamp-hour batteries for powering the cars. The batteries would last for the entire single-day events as long as they were charged before the event and not used to power the cars when not running on the tracks. The batteries typically could be recharged over 100 times before they had to be replaced. Many teams use larger (and heavier) 10,000 milliamp-hour Lithium batteries. These are especially useful when learning to drive around the track. Teams also loved the fact that the INIU batteries showed the percent power remaining. The INIU Portable Charger, Slimmest 10000mAh 5V/3A Power Bank is also a popular choice since they only cost about $19.","title":"Computer Batteries"},{"location":"setup/battery-options/#rc-car-batteries","text":"","title":"RC Car Batteries"},{"location":"setup/battery-options/#battery-cables","text":"Several participants used long battery cables with a small wire gauge. These cables caused voltage drops that made the cars stop working. We told all teams to use short 8-inch battery cables and most of these problems went away. We strongly suggest you build your own cables and use 18-gauge wire. Sample 1ft Charging Cable","title":"Battery Cables"},{"location":"setup/battery-options/#connectors-and-crimping-tool","text":"Ferrule Crimping Tool Kit","title":"Connectors and Crimping Tool"},{"location":"setup/battery-options/#camera-batteries","text":"The camera industry has also developed standard 7.2-volt battery packs that enclose low-cost 18650 battery cells. These standards have electronics built in to protect from drainage. SmallRig NP-F Battery Plate Adapter Generic NP-F 550 2 Cell Battery Generic NP-F 750 4 Cell Battery Antigravity Micro-Start XP-1 Battery Related","title":"Camera Batteries"},{"location":"setup/battery-options/#references","text":"NVIDIA Jetson on Battery Power - a nice video by","title":"References"},{"location":"setup/building-arl-image/","text":"Building an AI Racing League Image for the Car This process takes about four hours and requires close attention to detail. Install The Latest NVIDIA image Enable SSH This allows you to do a remote login to the car 1 2 sudo systemctl enable ssh sudo systemctl start ssh Test ssh: 1 ssh arl@arl-01 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Welcome to Ubuntu 18 . 04 . 6 LTS ( GNU / Linux 4 . 9 . 253 - tegra aarch64 ) * Documentation : https : // help . ubuntu . com * Management : https : // landscape . canonical . com * Support : https : // ubuntu . com / advantage This system has been minimized by removing packages and content that are not required on a system that users do not log into . To restore this content , you can run the ' unminimize ' command . Expanded Security Maintenance for Infrastructure is not enabled . 0 updates can be applied immediately . Enable ESM Infra to receive additional future security updates . See https : // ubuntu . com / esm or run : sudo pro status Add a swap file The Jetson Hacks GitHub repo contains a nice UNIX script that will add a swap file. 1 2 3 git clone https://github.com/JetsonHacksNano/installSwapfile cd installSwapfile ./installSwapfile.sh You should reboot the Nano after you add this file. Add HTOP The standard UNIX top command does not display the load across multiple cores. The much more modern htop is preferred and has better use of color. 1 sudo apt-get install htop Customize Browser Add the Bookmark Bar AI Racing League Donkey Car Docs Remove Unused Software Ubuntu comes with several \"Office\" applications for doing word-processing, spreadsheets and presentations. These applications take up a lot of disk space so we will remove them. 1 2 3 sudo apt-get remove --purge libreoffice* sudo apt-get clean sudo apt-get autoremove Upgrade Local Libraries Download Donkey Car Dependencies sudo apt-get update -y sudo apt-get upgrade -y sudo apt-get install -y libhdf5-serial-dev hdf5-tools libhdf5-dev zlib1g-dev zip libjpeg8-dev liblapack-dev libblas-dev gfortran sudo apt-get install -y python3-dev python3-pip sudo apt-get install -y libxslt1-dev libxml2-dev libffi-dev libcurl4-openssl-dev libssl-dev libpng-dev libopenblas-dev sudo apt-get install -y git nano sudo apt-get install -y openmpi-doc openmpi-bin libopenmpi-dev libopenblas-dev Monitoring Activity Version Checks 1 pip3 --version 1 pip 21.3.1 from /home/arl/env/lib/python3.6/site-packages/pip (python 3.6)","title":"Building ARL Image"},{"location":"setup/building-arl-image/#building-an-ai-racing-league-image-for-the-car","text":"This process takes about four hours and requires close attention to detail.","title":"Building an AI Racing League Image for the Car"},{"location":"setup/building-arl-image/#install-the-latest-nvidia-image","text":"","title":"Install The Latest NVIDIA image"},{"location":"setup/building-arl-image/#enable-ssh","text":"This allows you to do a remote login to the car 1 2 sudo systemctl enable ssh sudo systemctl start ssh Test ssh: 1 ssh arl@arl-01 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Welcome to Ubuntu 18 . 04 . 6 LTS ( GNU / Linux 4 . 9 . 253 - tegra aarch64 ) * Documentation : https : // help . ubuntu . com * Management : https : // landscape . canonical . com * Support : https : // ubuntu . com / advantage This system has been minimized by removing packages and content that are not required on a system that users do not log into . To restore this content , you can run the ' unminimize ' command . Expanded Security Maintenance for Infrastructure is not enabled . 0 updates can be applied immediately . Enable ESM Infra to receive additional future security updates . See https : // ubuntu . com / esm or run : sudo pro status","title":"Enable SSH"},{"location":"setup/building-arl-image/#add-a-swap-file","text":"The Jetson Hacks GitHub repo contains a nice UNIX script that will add a swap file. 1 2 3 git clone https://github.com/JetsonHacksNano/installSwapfile cd installSwapfile ./installSwapfile.sh You should reboot the Nano after you add this file.","title":"Add a swap file"},{"location":"setup/building-arl-image/#add-htop","text":"The standard UNIX top command does not display the load across multiple cores. The much more modern htop is preferred and has better use of color. 1 sudo apt-get install htop","title":"Add HTOP"},{"location":"setup/building-arl-image/#customize-browser-add-the-bookmark-bar","text":"AI Racing League Donkey Car Docs","title":"Customize Browser Add the Bookmark Bar"},{"location":"setup/building-arl-image/#remove-unused-software","text":"Ubuntu comes with several \"Office\" applications for doing word-processing, spreadsheets and presentations. These applications take up a lot of disk space so we will remove them. 1 2 3 sudo apt-get remove --purge libreoffice* sudo apt-get clean sudo apt-get autoremove","title":"Remove Unused Software"},{"location":"setup/building-arl-image/#upgrade-local-libraries","text":"","title":"Upgrade Local Libraries"},{"location":"setup/building-arl-image/#download-donkey-car-dependencies","text":"sudo apt-get update -y sudo apt-get upgrade -y sudo apt-get install -y libhdf5-serial-dev hdf5-tools libhdf5-dev zlib1g-dev zip libjpeg8-dev liblapack-dev libblas-dev gfortran sudo apt-get install -y python3-dev python3-pip sudo apt-get install -y libxslt1-dev libxml2-dev libffi-dev libcurl4-openssl-dev libssl-dev libpng-dev libopenblas-dev sudo apt-get install -y git nano sudo apt-get install -y openmpi-doc openmpi-bin libopenmpi-dev libopenblas-dev","title":"Download Donkey Car Dependencies"},{"location":"setup/building-arl-image/#monitoring-activity","text":"","title":"Monitoring Activity"},{"location":"setup/building-arl-image/#version-checks","text":"1 pip3 --version 1 pip 21.3.1 from /home/arl/env/lib/python3.6/site-packages/pip (python 3.6)","title":"Version Checks"},{"location":"setup/calibrate/","text":"Calibrate 1 $ donkey calibrate --channel 0 --bus = 1 Result 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 ________ ______ _________ ___ __ \\ _______________ / ___________ __ __ ____ / _____ ________ __ / / / __ \\ _ __ \\ _ // _ / _ \\ _ / / / _ / _ __ ` / _ ___ / _ / _ / // / _ / / / / / , < / __ / / _ / / / / ___ / / _ / / _ / / _____ / \\ ____ // _ / / _ // _ /| _ | \\ ___ / _ \\ __ , / \\ ____ / \\ __ , _ / / _ / / ____ / using donkey v4 . 2.1 ... sombrero enabled init PCA9685 on channel 0 address 0x40 bus 1 Using PWM freq : 60 Traceback ( most recent call last ): File \"/home/pi/env/bin/donkey\" , line 33 , in < module > sys . exit ( load_entry_point ( 'donkeycar' , 'console_scripts' , 'donkey' )()) File \"/home/pi/projects/donkeycar/donkeycar/management/base.py\" , line 500 , in execute_from_command_line c . run ( args [ 2 :]) File \"/home/pi/projects/donkeycar/donkeycar/management/base.py\" , line 219 , in run c = PCA9685 ( channel , address = address , busnum = busnum , frequency = freq ) File \"/home/pi/projects/donkeycar/donkeycar/parts/actuator.py\" , line 30 , in __init__ self . pwm = Adafruit_PCA9685 . PCA9685 ( address = address ) File \"/home/pi/env/lib/python3.7/site-packages/Adafruit_PCA9685/PCA9685.py\" , line 75 , in __init__ self . set_all_pwm ( 0 , 0 ) File \"/home/pi/env/lib/python3.7/site-packages/Adafruit_PCA9685/PCA9685.py\" , line 111 , in set_all_pwm self . _device . write8 ( ALL_LED_ON_L , on & 0xFF ) File \"/home/pi/env/lib/python3.7/site-packages/Adafruit_GPIO/I2C.py\" , line 114 , in write8 self . _bus . write_byte_data ( self . _address , register , value ) File \"/home/pi/env/lib/python3.7/site-packages/Adafruit_PureIO/smbus.py\" , line 327 , in write_byte_data self . _device . write ( data ) OSError : [ Errno 121 ] Remote I / O error sombrero disabled Diagnostics I2C Detect 1 2 3 4 5 6 7 8 9 10 i2cdetect - y 1 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- -- 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: 40 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: 70 -- -- -- -- -- -- -- I2C Device File 1 2 ls -ld /dev/i2* crw-rw---- 1 root i2c 89, 1 Jul 3 13:17 /dev/i2c-1 I2C Functions Enabled 1 i2cdetect -F 1 returns: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Functionalities implemented by / dev / i2c - 1 : I2C yes SMBus Quick Command yes SMBus Send Byte yes SMBus Receive Byte yes SMBus Write Byte yes SMBus Read Byte yes SMBus Write Word yes SMBus Read Word yes SMBus Process Call yes SMBus Block Write yes SMBus Block Read no SMBus Block Process Call no SMBus PEC yes I2C Block Write yes I2C Block Read yes Note that both SMBus Block Read and SMBus Block Process Call are set to no. The rest are yes. Upgrade to Python 3.70 1 python3 -m virtualenv -p python3.7 env --system-site-packages 1 2 3 4 5 created virtual environment CPython3 . 7.3 . final . 0 - 32 in 2535 ms creator CPython3Posix ( dest =/ home / pi / env , clear = False , no_vcs_ignore = False , global = True ) seeder FromAppData ( download = False , pip = bundle , setuptools = bundle , wheel = bundle , via = copy , app_data_dir =/ home / pi /. local / share / virtualenv ) added seed packages : pip == 21.1 . 2 , setuptools == 57.0 . 0 , wheel == 0.36 . 2 activators BashActivator , CShellActivator , FishActivator , PowerShellActivator , PythonActivator , XonshActivator 1 python --version 1 Python 3.7.3","title":"Calibrate"},{"location":"setup/calibrate/#calibrate","text":"1 $ donkey calibrate --channel 0 --bus = 1 Result 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 ________ ______ _________ ___ __ \\ _______________ / ___________ __ __ ____ / _____ ________ __ / / / __ \\ _ __ \\ _ // _ / _ \\ _ / / / _ / _ __ ` / _ ___ / _ / _ / // / _ / / / / / , < / __ / / _ / / / / ___ / / _ / / _ / / _____ / \\ ____ // _ / / _ // _ /| _ | \\ ___ / _ \\ __ , / \\ ____ / \\ __ , _ / / _ / / ____ / using donkey v4 . 2.1 ... sombrero enabled init PCA9685 on channel 0 address 0x40 bus 1 Using PWM freq : 60 Traceback ( most recent call last ): File \"/home/pi/env/bin/donkey\" , line 33 , in < module > sys . exit ( load_entry_point ( 'donkeycar' , 'console_scripts' , 'donkey' )()) File \"/home/pi/projects/donkeycar/donkeycar/management/base.py\" , line 500 , in execute_from_command_line c . run ( args [ 2 :]) File \"/home/pi/projects/donkeycar/donkeycar/management/base.py\" , line 219 , in run c = PCA9685 ( channel , address = address , busnum = busnum , frequency = freq ) File \"/home/pi/projects/donkeycar/donkeycar/parts/actuator.py\" , line 30 , in __init__ self . pwm = Adafruit_PCA9685 . PCA9685 ( address = address ) File \"/home/pi/env/lib/python3.7/site-packages/Adafruit_PCA9685/PCA9685.py\" , line 75 , in __init__ self . set_all_pwm ( 0 , 0 ) File \"/home/pi/env/lib/python3.7/site-packages/Adafruit_PCA9685/PCA9685.py\" , line 111 , in set_all_pwm self . _device . write8 ( ALL_LED_ON_L , on & 0xFF ) File \"/home/pi/env/lib/python3.7/site-packages/Adafruit_GPIO/I2C.py\" , line 114 , in write8 self . _bus . write_byte_data ( self . _address , register , value ) File \"/home/pi/env/lib/python3.7/site-packages/Adafruit_PureIO/smbus.py\" , line 327 , in write_byte_data self . _device . write ( data ) OSError : [ Errno 121 ] Remote I / O error sombrero disabled","title":"Calibrate"},{"location":"setup/calibrate/#diagnostics","text":"","title":"Diagnostics"},{"location":"setup/calibrate/#i2c-detect","text":"1 2 3 4 5 6 7 8 9 10 i2cdetect - y 1 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- -- 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: 40 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: 70 -- -- -- -- -- -- --","title":"I2C Detect"},{"location":"setup/calibrate/#i2c-device-file","text":"1 2 ls -ld /dev/i2* crw-rw---- 1 root i2c 89, 1 Jul 3 13:17 /dev/i2c-1","title":"I2C Device File"},{"location":"setup/calibrate/#i2c-functions-enabled","text":"1 i2cdetect -F 1 returns: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Functionalities implemented by / dev / i2c - 1 : I2C yes SMBus Quick Command yes SMBus Send Byte yes SMBus Receive Byte yes SMBus Write Byte yes SMBus Read Byte yes SMBus Write Word yes SMBus Read Word yes SMBus Process Call yes SMBus Block Write yes SMBus Block Read no SMBus Block Process Call no SMBus PEC yes I2C Block Write yes I2C Block Read yes Note that both SMBus Block Read and SMBus Block Process Call are set to no. The rest are yes.","title":"I2C Functions Enabled"},{"location":"setup/calibrate/#upgrade-to-python-370","text":"1 python3 -m virtualenv -p python3.7 env --system-site-packages 1 2 3 4 5 created virtual environment CPython3 . 7.3 . final . 0 - 32 in 2535 ms creator CPython3Posix ( dest =/ home / pi / env , clear = False , no_vcs_ignore = False , global = True ) seeder FromAppData ( download = False , pip = bundle , setuptools = bundle , wheel = bundle , via = copy , app_data_dir =/ home / pi /. local / share / virtualenv ) added seed packages : pip == 21.1 . 2 , setuptools == 57.0 . 0 , wheel == 0.36 . 2 activators BashActivator , CShellActivator , FishActivator , PowerShellActivator , PythonActivator , XonshActivator 1 python --version 1 Python 3.7.3","title":"Upgrade to Python 3.70"},{"location":"setup/camera-testing/","text":"Testing the Camera To test the camera and cable, we need a command that captures video from a CSI camera connected to an NVIDIA Jetson Nano, converts the video format and resolution, and then displays the video on the screen. We will use the GStreamer command first. GStreamer Test on the Nano 1 2 3 4 5 6 7 gst-launch-1.0 nvarguscamerasrc ! \\ 'video/x-raw(memory:NVMM),width=3820, height=2464, framerate=21/1, format=NV12' ! \\ nvvidconv flip-method = 0 ! \\ 'video/x-raw,width=960, height=616' ! nvvidconv ! \\ nvegltransform ! \\ nveglglessink -e This command is a GStreamer command used to test the functionality of a camera on a NVIDIA Jetson Nano device. GStreamer is a multimedia framework that provides a pipeline for media data. The gst-launch-1.0 utility is used to build and run basic GStreamer pipelines. On newer systems with two cameras you can add: sensor-id=0 to make sure you are getting camera 0. This guide explains how to clone a Raspberry Pi microSD card using the dd command on UNIX and UNIX-like systems. Table of Contents Testing the Camera GStreamer Test on the Nano Table of Contents Identify the microSD Card Device Unmount the microSD Card Create a Backup Image Copy the Backup Image to New microSD Cards Expand the Filesystem (Optional) Why \"!\" and not \"|\"? Flip Modes Resources Dan's Blog Jetson Hacks Blog Identify the microSD Card Device First, identify the device name for your microSD card. Use either lsblk or df to list devices and their mount points. bash lsblk Or: bash df -h Look for the device corresponding to your microSD card. It's generally something like /dev/sdX or /dev/mmcblkX , where X is a letter. \u26a0\ufe0f Caution : Be very careful to identify the correct device, as choosing the wrong one could result in data loss. Unmount the microSD Card Before copying data, unmount the partitions of the microSD card to ensure that no data is being read or written during the cloning process. bash sudo umount /dev/sdX* Create a Backup Image Use the dd command to create an image file of the microSD card. bash sudo dd if=/dev/sdX of=raspberrypi_backup.img bs=4M status=progress if : Input File \u2014 the device you are copying from (your microSD card). of : Output File \u2014 the image file you are creating. bs : Block Size \u2014 specifies how much data should be read at each iteration. 4M is usually a good size. status=progress : shows the progress during the copy. Copy the Backup Image to New microSD Cards To clone the image onto a new microSD card, insert the new card and identify it just like you did in the first step. bash sudo dd if=raspberrypi_backup.img of=/dev/sdY bs=4M status=progress Replace /dev/sdY with the device name of your new microSD card. \u26a0\ufe0f Caution : Again, be very careful to identify the correct device to avoid data loss. Expand the Filesystem (Optional) If your new microSD card is larger than the original, you might need to expand the filesystem to use the additional space. You can do this using raspi-config on the Raspberry Pi. bash sudo raspi-config Navigate to Advanced Options > Expand Filesystem . After following these steps, you should have successfully cloned your Raspberry Pi's microSD card. Note : The dd command can be very dangerous if misused. Always double-check your device names and ensure you understand the commands you're running. ``` Here's a breakdown of the command: nvarguscamerasrc sensor_mode=0 : This is a GStreamer plugin specific to the NVIDIA platform that provides support for the Camera Serial Interface (CSI) cameras. sensor_mode=0 indicates that the command should use the first sensor mode of the camera. The sensor mode usually defines properties such as the resolution and frame rate that the camera supports. 'video/x-raw(memory:NVMM),width=3820, height=2464, framerate=21/1, format=NV12' : This part of the command specifies the desired output from the camera source. The properties indicate that the video should be in NV12 format, with a resolution of 3820x2464 pixels and a frame rate of 21 frames per second. NVMM refers to NVIDIA's proprietary multimedia memory. nvvidconv flip-method=0 : This is another NVIDIA specific GStreamer plugin that converts video from one format to another. The flip-method=0 option means that no flipping operation should be performed on the frames. 'video/x-raw,width=960, height=616' : This specifies the desired output format and resolution after the conversion. The resolution is downscaled to 960x616 pixels. nvvidconv ! nvegltransform ! nveglglessink -e : This part of the pipeline takes the video from the conversion, applies an EGLStream transformation ( nvegltransform ) and then sends it to a EGL/GLES-based render sink ( nveglglessink ). This sink displays the video on the device's screen. The -e flag at the end of the command tells GStreamer to send an end-of-stream signal when the source stops, which will properly close down the pipeline. Why \"!\" and not \"|\"? In the context of a GStreamer command, the \"!\" (aka bang) character is used to connect different elements of a GStreamer pipeline together. It serves a similar role to the UNIX \"|\" (pipe) character in a regular UNIX shell command, where it's used to pipe the output from one command into another. However, there's an important difference between the two. In a UNIX shell command, the | character sends the standard output (stdout) of one command to the standard input (stdin) of another. In a GStreamer pipeline, the ! character doesn't simply pipe data from one element to the next. Instead, it establishes a connection between two GStreamer elements, allowing them to negotiate formats, buffer management, and other details. This negotiation process can involve more complex operations like format conversion, and it happens before any data is actually transferred. So, in summary, while | and ! might seem similar, the latter is used in GStreamer to create more complex, negotiated connections between different multimedia processing elements. Flip Modes The flip-method property of the nvvidconv (NVIDIA Video Converter) plugin controls the orientation of the output video in the NVIDIA Jetson platform. This is useful for handling scenarios where the camera could be mounted in various orientations. Here are the possible values for the flip-method parameter: 0 (Identity) - No rotation, no vertical flip. 1 (Counterclockwise) - Rotate counter-clockwise 90 degrees. 2 (Rotate 180) - Rotate 180 degrees. 3 (Clockwise) - Rotate clockwise 90 degrees. 4 (Horizontal Flip) - Flip horizontally. 5 (Upper Right Diagonal) - Flip across upper right/lower left diagonal. 6 (Vertical Flip) - Flip vertically. 7 (Upper Left Diagonal) - Flip across upper left/lower right diagonal. Each number corresponds to a specific operation on the video frames. The specific operation will be applied to each frame of the video before it's sent to the next element in the GStreamer pipeline. Resources Dan's Blog NVIDIA CSI Camera GitHub Repo Jetson Hacks Blog https://jetsonhacks.com/2019/04/02/jetson-nano-raspberry-pi-camera/","title":"Camera Testing"},{"location":"setup/camera-testing/#testing-the-camera","text":"To test the camera and cable, we need a command that captures video from a CSI camera connected to an NVIDIA Jetson Nano, converts the video format and resolution, and then displays the video on the screen. We will use the GStreamer command first.","title":"Testing the Camera"},{"location":"setup/camera-testing/#gstreamer-test-on-the-nano","text":"1 2 3 4 5 6 7 gst-launch-1.0 nvarguscamerasrc ! \\ 'video/x-raw(memory:NVMM),width=3820, height=2464, framerate=21/1, format=NV12' ! \\ nvvidconv flip-method = 0 ! \\ 'video/x-raw,width=960, height=616' ! nvvidconv ! \\ nvegltransform ! \\ nveglglessink -e This command is a GStreamer command used to test the functionality of a camera on a NVIDIA Jetson Nano device. GStreamer is a multimedia framework that provides a pipeline for media data. The gst-launch-1.0 utility is used to build and run basic GStreamer pipelines. On newer systems with two cameras you can add: sensor-id=0 to make sure you are getting camera 0. This guide explains how to clone a Raspberry Pi microSD card using the dd command on UNIX and UNIX-like systems.","title":"GStreamer Test on the Nano"},{"location":"setup/camera-testing/#table-of-contents","text":"Testing the Camera GStreamer Test on the Nano Table of Contents Identify the microSD Card Device Unmount the microSD Card Create a Backup Image Copy the Backup Image to New microSD Cards Expand the Filesystem (Optional) Why \"!\" and not \"|\"? Flip Modes Resources Dan's Blog Jetson Hacks Blog","title":"Table of Contents"},{"location":"setup/camera-testing/#identify-the-microsd-card-device","text":"First, identify the device name for your microSD card. Use either lsblk or df to list devices and their mount points. bash lsblk Or: bash df -h Look for the device corresponding to your microSD card. It's generally something like /dev/sdX or /dev/mmcblkX , where X is a letter. \u26a0\ufe0f Caution : Be very careful to identify the correct device, as choosing the wrong one could result in data loss.","title":"Identify the microSD Card Device"},{"location":"setup/camera-testing/#unmount-the-microsd-card","text":"Before copying data, unmount the partitions of the microSD card to ensure that no data is being read or written during the cloning process. bash sudo umount /dev/sdX*","title":"Unmount the microSD Card"},{"location":"setup/camera-testing/#create-a-backup-image","text":"Use the dd command to create an image file of the microSD card. bash sudo dd if=/dev/sdX of=raspberrypi_backup.img bs=4M status=progress if : Input File \u2014 the device you are copying from (your microSD card). of : Output File \u2014 the image file you are creating. bs : Block Size \u2014 specifies how much data should be read at each iteration. 4M is usually a good size. status=progress : shows the progress during the copy.","title":"Create a Backup Image"},{"location":"setup/camera-testing/#copy-the-backup-image-to-new-microsd-cards","text":"To clone the image onto a new microSD card, insert the new card and identify it just like you did in the first step. bash sudo dd if=raspberrypi_backup.img of=/dev/sdY bs=4M status=progress Replace /dev/sdY with the device name of your new microSD card. \u26a0\ufe0f Caution : Again, be very careful to identify the correct device to avoid data loss.","title":"Copy the Backup Image to New microSD Cards"},{"location":"setup/camera-testing/#expand-the-filesystem-optional","text":"If your new microSD card is larger than the original, you might need to expand the filesystem to use the additional space. You can do this using raspi-config on the Raspberry Pi. bash sudo raspi-config Navigate to Advanced Options > Expand Filesystem . After following these steps, you should have successfully cloned your Raspberry Pi's microSD card. Note : The dd command can be very dangerous if misused. Always double-check your device names and ensure you understand the commands you're running. ``` Here's a breakdown of the command: nvarguscamerasrc sensor_mode=0 : This is a GStreamer plugin specific to the NVIDIA platform that provides support for the Camera Serial Interface (CSI) cameras. sensor_mode=0 indicates that the command should use the first sensor mode of the camera. The sensor mode usually defines properties such as the resolution and frame rate that the camera supports. 'video/x-raw(memory:NVMM),width=3820, height=2464, framerate=21/1, format=NV12' : This part of the command specifies the desired output from the camera source. The properties indicate that the video should be in NV12 format, with a resolution of 3820x2464 pixels and a frame rate of 21 frames per second. NVMM refers to NVIDIA's proprietary multimedia memory. nvvidconv flip-method=0 : This is another NVIDIA specific GStreamer plugin that converts video from one format to another. The flip-method=0 option means that no flipping operation should be performed on the frames. 'video/x-raw,width=960, height=616' : This specifies the desired output format and resolution after the conversion. The resolution is downscaled to 960x616 pixels. nvvidconv ! nvegltransform ! nveglglessink -e : This part of the pipeline takes the video from the conversion, applies an EGLStream transformation ( nvegltransform ) and then sends it to a EGL/GLES-based render sink ( nveglglessink ). This sink displays the video on the device's screen. The -e flag at the end of the command tells GStreamer to send an end-of-stream signal when the source stops, which will properly close down the pipeline.","title":"Expand the Filesystem (Optional)"},{"location":"setup/camera-testing/#why-and-not","text":"In the context of a GStreamer command, the \"!\" (aka bang) character is used to connect different elements of a GStreamer pipeline together. It serves a similar role to the UNIX \"|\" (pipe) character in a regular UNIX shell command, where it's used to pipe the output from one command into another. However, there's an important difference between the two. In a UNIX shell command, the | character sends the standard output (stdout) of one command to the standard input (stdin) of another. In a GStreamer pipeline, the ! character doesn't simply pipe data from one element to the next. Instead, it establishes a connection between two GStreamer elements, allowing them to negotiate formats, buffer management, and other details. This negotiation process can involve more complex operations like format conversion, and it happens before any data is actually transferred. So, in summary, while | and ! might seem similar, the latter is used in GStreamer to create more complex, negotiated connections between different multimedia processing elements.","title":"Why \"!\" and not \"|\"?"},{"location":"setup/camera-testing/#flip-modes","text":"The flip-method property of the nvvidconv (NVIDIA Video Converter) plugin controls the orientation of the output video in the NVIDIA Jetson platform. This is useful for handling scenarios where the camera could be mounted in various orientations. Here are the possible values for the flip-method parameter: 0 (Identity) - No rotation, no vertical flip. 1 (Counterclockwise) - Rotate counter-clockwise 90 degrees. 2 (Rotate 180) - Rotate 180 degrees. 3 (Clockwise) - Rotate clockwise 90 degrees. 4 (Horizontal Flip) - Flip horizontally. 5 (Upper Right Diagonal) - Flip across upper right/lower left diagonal. 6 (Vertical Flip) - Flip vertically. 7 (Upper Left Diagonal) - Flip across upper left/lower right diagonal. Each number corresponds to a specific operation on the video frames. The specific operation will be applied to each frame of the video before it's sent to the next element in the GStreamer pipeline.","title":"Flip Modes"},{"location":"setup/camera-testing/#resources","text":"","title":"Resources"},{"location":"setup/camera-testing/#dans-blog","text":"NVIDIA CSI Camera GitHub Repo","title":"Dan's Blog"},{"location":"setup/camera-testing/#jetson-hacks-blog","text":"https://jetsonhacks.com/2019/04/02/jetson-nano-raspberry-pi-camera/","title":"Jetson Hacks Blog"},{"location":"setup/clone-sd/","text":"Cloning a microSD Card using UNIX dd Command This guide explains how to clone a microSD card using the dd command on UNIX and UNIX-like systems. Table of Contents Cloning a microSD Card using UNIX dd Command Table of Contents Identify the microSD Card Device Unmount the microSD Card Create a Backup Image Copy the Backup Image to New microSD Cards Expand the Filesystem (Optional) Identify the microSD Card Device First, identify the device name for your microSD card. Use either lsblk or df to list devices and their mount points. bash lsblk Or: bash df -h Look for the device corresponding to your microSD card. It's generally something like /dev/sdX or /dev/mmcblkX , where X is a letter. \u26a0\ufe0f Caution : Be very careful to identify the correct device, as choosing the wrong one could result in data loss. Unmount the microSD Card Before copying data, unmount the partitions of the microSD card to ensure that no data is being read or written during the cloning process. bash sudo umount /dev/sdX* Create a Backup Image Use the dd command to create an image file of the microSD card. bash sudo dd if=/dev/sdX of=raspberrypi_backup.img bs=4M status=progress if : Input File \u2014 the device you are copying from (your microSD card). of : Output File \u2014 the image file you are creating. bs : Block Size \u2014 specifies how much data should be read at each iteration. 4M is usually a good size. status=progress : shows the progress during the copy. Copy the Backup Image to New microSD Cards To clone the image onto a new microSD card, insert the new card and identify it just like you did in the first step. bash sudo dd if=raspberrypi_backup.img of=/dev/sdY bs=4M status=progress Replace /dev/sdY with the device name of your new microSD card. \u26a0\ufe0f Caution : Again, be very careful to identify the correct device to avoid data loss. Expand the Filesystem (Optional) If your new microSD card is larger than the original, you might need to expand the filesystem to use the additional space. You can do this using raspi-config on the Raspberry Pi. bash sudo raspi-config Navigate to Advanced Options > Expand Filesystem . After following these steps, you should have successfully cloned your Raspberry Pi's microSD card. Note : The dd command can be very dangerous if misused. Always double-check your device names and ensure you understand the commands you're running.","title":"Clone an SD Card"},{"location":"setup/clone-sd/#cloning-a-microsd-card-using-unix-dd-command","text":"This guide explains how to clone a microSD card using the dd command on UNIX and UNIX-like systems.","title":"Cloning a microSD Card using UNIX dd Command"},{"location":"setup/clone-sd/#table-of-contents","text":"Cloning a microSD Card using UNIX dd Command Table of Contents Identify the microSD Card Device Unmount the microSD Card Create a Backup Image Copy the Backup Image to New microSD Cards Expand the Filesystem (Optional)","title":"Table of Contents"},{"location":"setup/clone-sd/#identify-the-microsd-card-device","text":"First, identify the device name for your microSD card. Use either lsblk or df to list devices and their mount points. bash lsblk Or: bash df -h Look for the device corresponding to your microSD card. It's generally something like /dev/sdX or /dev/mmcblkX , where X is a letter. \u26a0\ufe0f Caution : Be very careful to identify the correct device, as choosing the wrong one could result in data loss.","title":"Identify the microSD Card Device"},{"location":"setup/clone-sd/#unmount-the-microsd-card","text":"Before copying data, unmount the partitions of the microSD card to ensure that no data is being read or written during the cloning process. bash sudo umount /dev/sdX*","title":"Unmount the microSD Card"},{"location":"setup/clone-sd/#create-a-backup-image","text":"Use the dd command to create an image file of the microSD card. bash sudo dd if=/dev/sdX of=raspberrypi_backup.img bs=4M status=progress if : Input File \u2014 the device you are copying from (your microSD card). of : Output File \u2014 the image file you are creating. bs : Block Size \u2014 specifies how much data should be read at each iteration. 4M is usually a good size. status=progress : shows the progress during the copy.","title":"Create a Backup Image"},{"location":"setup/clone-sd/#copy-the-backup-image-to-new-microsd-cards","text":"To clone the image onto a new microSD card, insert the new card and identify it just like you did in the first step. bash sudo dd if=raspberrypi_backup.img of=/dev/sdY bs=4M status=progress Replace /dev/sdY with the device name of your new microSD card. \u26a0\ufe0f Caution : Again, be very careful to identify the correct device to avoid data loss.","title":"Copy the Backup Image to New microSD Cards"},{"location":"setup/clone-sd/#expand-the-filesystem-optional","text":"If your new microSD card is larger than the original, you might need to expand the filesystem to use the additional space. You can do this using raspi-config on the Raspberry Pi. bash sudo raspi-config Navigate to Advanced Options > Expand Filesystem . After following these steps, you should have successfully cloned your Raspberry Pi's microSD card. Note : The dd command can be very dangerous if misused. Always double-check your device names and ensure you understand the commands you're running.","title":"Expand the Filesystem (Optional)"},{"location":"setup/conda-pi-setup/","text":"Raspberry Pi Setup Install Conda for the ARM Processor When asked: Do you wish the installer to prepend the Miniconda3 install location to PATH in your /root/.bashrc? Answer: yes 1 2 3 4 cd /tmp wget http://repo.continuum.io/miniconda/Miniconda3-latest-Linux-armv7l.sh chmod 755 Miniconda3-latest-Linux-armv7l.sh ./Miniconda3-latest-Linux-armv7l.sh Test Conda In Your PATH 1 which conda Should return: 1 /home/pi/miniconda3/bin/conda Add the Raspberry Pi Channel to Conda 1 2 conda config --add channels rpi conda install python = 3 .6 Test Python 1 python --version 1 Python 3.6.6 Create a DonkeyCar Conda Environment 1 conda create --name donkey python = 3 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 The following NEW packages will be INSTALLED : ca - certificates : 2018.8.24 - 0 rpi certifi : 2018.8.24 - py36_1 rpi ncurses : 6.1 - h4f752ac_1 rpi openssl : 1.0.2 r - hdff2a78_0 rpi pip : 18.0 - py36_1 rpi python : 3.6.6 - hd0568c0_1 rpi readline : 7.0 - hcb560eb_1 rpi setuptools : 40.2.0 - py36_0 rpi sqlite : 3.24.0 - hfcb1bcf_1 rpi tk : 8.6.8 - h849d6a0_0 rpi wheel : 0.31.1 - py36_1 rpi xz : 5.2.4 - hdff2a78_1 rpi zlib : 1.2.11 - hdff2a78_1003 rpi Proceed ( [ y ]/ n ) ? y Add the conda shell to the end of our .bashrc file 1 echo \". /home/pi/miniconda3/etc/profile.d/conda.sh\" >> ~/.bashrc 1 conda activate The shell prompt should now be \"base\" Activate Your Donkey Python Environment 1 source activate donkey You should see the prompt: 1 ( donkey ) pi @myhost : ~ $ Verify Git Is installed 1 git --version git version 2.20.1 Clone the DonkeyCar repository 1 2 3 git clone https://github.com/autorope/donkeycar cd donkeycar git checkout master 1 sudo apt-get install build-essential python3 python3-dev python3-pip python3-virtualenv python3-numpy python3-picamera python3-pandas python3-rpi.gpio i2c-tools avahi-utils joystick libopenjp2-7-dev libtiff5-dev gfortran libatlas-base-dev libopenblas-dev libhdf5-serial-dev libgeos-dev git ntp 1 sudo apt-get install libilmbase-dev libopenexr-dev libgstreamer1.0-dev libjasper-dev libwebp-dev libatlas-base-dev libavcodec-dev libavformat-dev libswscale-dev libqtgui4 libqt4-test Clone DonkeyCar Repo 1 pip freeze certifi==2018.8.24 1 2 3 git clone https://github.com/autorope/donkeycar cd donkeycar pip install -e . [ pi ]","title":"Setup Conda on Pi"},{"location":"setup/conda-pi-setup/#raspberry-pi-setup","text":"","title":"Raspberry Pi Setup"},{"location":"setup/conda-pi-setup/#install-conda-for-the-arm-processor","text":"When asked: Do you wish the installer to prepend the Miniconda3 install location to PATH in your /root/.bashrc? Answer: yes 1 2 3 4 cd /tmp wget http://repo.continuum.io/miniconda/Miniconda3-latest-Linux-armv7l.sh chmod 755 Miniconda3-latest-Linux-armv7l.sh ./Miniconda3-latest-Linux-armv7l.sh","title":"Install Conda for the ARM Processor"},{"location":"setup/conda-pi-setup/#test-conda-in-your-path","text":"1 which conda Should return: 1 /home/pi/miniconda3/bin/conda","title":"Test Conda In Your PATH"},{"location":"setup/conda-pi-setup/#add-the-raspberry-pi-channel-to-conda","text":"1 2 conda config --add channels rpi conda install python = 3 .6","title":"Add the Raspberry Pi Channel to Conda"},{"location":"setup/conda-pi-setup/#test-python","text":"1 python --version 1 Python 3.6.6","title":"Test Python"},{"location":"setup/conda-pi-setup/#create-a-donkeycar-conda-environment","text":"1 conda create --name donkey python = 3 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 The following NEW packages will be INSTALLED : ca - certificates : 2018.8.24 - 0 rpi certifi : 2018.8.24 - py36_1 rpi ncurses : 6.1 - h4f752ac_1 rpi openssl : 1.0.2 r - hdff2a78_0 rpi pip : 18.0 - py36_1 rpi python : 3.6.6 - hd0568c0_1 rpi readline : 7.0 - hcb560eb_1 rpi setuptools : 40.2.0 - py36_0 rpi sqlite : 3.24.0 - hfcb1bcf_1 rpi tk : 8.6.8 - h849d6a0_0 rpi wheel : 0.31.1 - py36_1 rpi xz : 5.2.4 - hdff2a78_1 rpi zlib : 1.2.11 - hdff2a78_1003 rpi Proceed ( [ y ]/ n ) ? y","title":"Create a DonkeyCar Conda Environment"},{"location":"setup/conda-pi-setup/#add-the-conda-shell-to-the-end-of-our-bashrc-file","text":"1 echo \". /home/pi/miniconda3/etc/profile.d/conda.sh\" >> ~/.bashrc 1 conda activate The shell prompt should now be \"base\"","title":"Add the conda shell to the end of our .bashrc file"},{"location":"setup/conda-pi-setup/#activate-your-donkey-python-environment","text":"1 source activate donkey You should see the prompt: 1 ( donkey ) pi @myhost : ~ $","title":"Activate Your Donkey Python Environment"},{"location":"setup/conda-pi-setup/#verify-git-is-installed","text":"1 git --version git version 2.20.1","title":"Verify Git Is installed"},{"location":"setup/conda-pi-setup/#clone-the-donkeycar-repository","text":"1 2 3 git clone https://github.com/autorope/donkeycar cd donkeycar git checkout master 1 sudo apt-get install build-essential python3 python3-dev python3-pip python3-virtualenv python3-numpy python3-picamera python3-pandas python3-rpi.gpio i2c-tools avahi-utils joystick libopenjp2-7-dev libtiff5-dev gfortran libatlas-base-dev libopenblas-dev libhdf5-serial-dev libgeos-dev git ntp 1 sudo apt-get install libilmbase-dev libopenexr-dev libgstreamer1.0-dev libjasper-dev libwebp-dev libatlas-base-dev libavcodec-dev libavformat-dev libswscale-dev libqtgui4 libqt4-test","title":"Clone the DonkeyCar repository"},{"location":"setup/conda-pi-setup/#clone-donkeycar-repo","text":"1 pip freeze certifi==2018.8.24 1 2 3 git clone https://github.com/autorope/donkeycar cd donkeycar pip install -e . [ pi ]","title":"Clone DonkeyCar Repo"},{"location":"setup/donkey-cmd/","text":"The Donkey Command In normal configuration, the donkey is a UNIX shell script that is located in the bin directory of your local environment. For example here is a sample path to the donkey command if you are logged into the standard AI Racing account called arl 1 /home/arl/env/bin 1 2 3 4 5 6 7 8 9 10 11 12 #!/home/dan/env/bin/python3 # EASY-INSTALL-ENTRY-SCRIPT: 'donkeycar','console_scripts','donkey' __requires__ = 'donkeycar' import re import sys from pkg_resources import load_entry_point if __name__ == '__main__' : sys.argv [ 0 ] = re.sub ( r '(-script\\.pyw?|\\.exe)?$' , '' , sys.argv [ 0 ]) sys.exit ( load_entry_point ( 'donkeycar' , 'console_scripts' , 'donkey' )() ) This file is automatically generated This UNIX script is automatically generated and serves as an entry point for the donkeycar package. The console_scripts and donkey parameters provide more information about the entry point, usually used by package management tools like setuptools . Imports 1 2 3 import re import sys from pkg_resources import load_entry_point re: imports Python's regular expression module. sys: imports Python's system module to interact with the Python runtime environment. load_entry_point: imports a function that is responsible for loading the entry point specified. On the installer for the 4.5.1, this file is not generated.","title":"The Donkey Command"},{"location":"setup/donkey-cmd/#the-donkey-command","text":"In normal configuration, the donkey is a UNIX shell script that is located in the bin directory of your local environment. For example here is a sample path to the donkey command if you are logged into the standard AI Racing account called arl 1 /home/arl/env/bin 1 2 3 4 5 6 7 8 9 10 11 12 #!/home/dan/env/bin/python3 # EASY-INSTALL-ENTRY-SCRIPT: 'donkeycar','console_scripts','donkey' __requires__ = 'donkeycar' import re import sys from pkg_resources import load_entry_point if __name__ == '__main__' : sys.argv [ 0 ] = re.sub ( r '(-script\\.pyw?|\\.exe)?$' , '' , sys.argv [ 0 ]) sys.exit ( load_entry_point ( 'donkeycar' , 'console_scripts' , 'donkey' )() ) This file is automatically generated This UNIX script is automatically generated and serves as an entry point for the donkeycar package. The console_scripts and donkey parameters provide more information about the entry point, usually used by package management tools like setuptools .","title":"The Donkey Command"},{"location":"setup/donkey-cmd/#imports","text":"1 2 3 import re import sys from pkg_resources import load_entry_point re: imports Python's regular expression module. sys: imports Python's system module to interact with the Python runtime environment. load_entry_point: imports a function that is responsible for loading the entry point specified. On the installer for the 4.5.1, this file is not generated.","title":"Imports"},{"location":"setup/gpu-options/","text":"GPU Options PCPartPicker Part List: https://pcpartpicker.com/list/mrFYPX CPU: AMD Ryzen 5 3600 3.6 GHz 6-Core Processor ($95.00 @ Amazon) Motherboard: MSI A520M-A PRO Micro ATX AM4 Motherboard ($101.11 @ Amazon) Memory: Silicon Power SP016GBLFU320X02 16 GB (1 x 16 GB) DDR4-3200 CL22 Memory ($23.99 @ Amazon) Storage: TEAMGROUP MP33 512 GB M.2-2280 PCIe 3.0 X4 NVME Solid State Drive ($22.49 @ Amazon) Video Card: Asus Dual GeForce RTX 3060 V2 OC Edition GeForce RTX 3060 12GB 12 GB Video Card ($299.99 @ Amazon) Case: Thermaltake Versa H18 MicroATX Mini Tower Case ($49.99 @ Amazon) Power Supply: be quiet! Pure Power 11 400 W 80+ Gold Certified ATX Power Supply ($89.69 @ Amazon) Monitor: *Acer V227Q Abmix 21.5\" 1920 x 1080 75 Hz Monitor ($87.29 @ Amazon) Total: $769.55","title":"GPU Options"},{"location":"setup/gpu-options/#gpu-options","text":"PCPartPicker Part List: https://pcpartpicker.com/list/mrFYPX CPU: AMD Ryzen 5 3600 3.6 GHz 6-Core Processor ($95.00 @ Amazon) Motherboard: MSI A520M-A PRO Micro ATX AM4 Motherboard ($101.11 @ Amazon) Memory: Silicon Power SP016GBLFU320X02 16 GB (1 x 16 GB) DDR4-3200 CL22 Memory ($23.99 @ Amazon) Storage: TEAMGROUP MP33 512 GB M.2-2280 PCIe 3.0 X4 NVME Solid State Drive ($22.49 @ Amazon) Video Card: Asus Dual GeForce RTX 3060 V2 OC Edition GeForce RTX 3060 12GB 12 GB Video Card ($299.99 @ Amazon) Case: Thermaltake Versa H18 MicroATX Mini Tower Case ($49.99 @ Amazon) Power Supply: be quiet! Pure Power 11 400 W 80+ Gold Certified ATX Power Supply ($89.69 @ Amazon) Monitor: *Acer V227Q Abmix 21.5\" 1920 x 1080 75 Hz Monitor ($87.29 @ Amazon) Total: $769.55","title":"GPU Options"},{"location":"setup/installation-tips/","text":"Installation Tips Installing the Donkey Car software is a complex process. It typically takes about six hours to add the required tools and Python libraries to a raw NVIDIA or Raspberry Pi OS image. Here are a few tips. Use the -H mode when installing software We should always use the HOME environment variables when using sudo: 1 sudo -H pip install package_name In this command, sudo -H is used to ensure that the home environment variable is set to the home directory of the target user (root in this case), which can sometimes avoid permissions issues that arise when sudo retains your normal user's home directory environment variable. Essentially, the -H option makes sure that the operation is as clean as possible from an environment perspective, which can be important when installing software that might write configuration files or data into the user's home directory. Remember to Use Python3 and Pip3 Sometimes older Python2 tools get mixed up with the current Python 3 tools. By adding the \"3\" suffix to your commands you can guarantee that that your path will pick up the right version of Python tools. When to Use python3 and pip3 Over python and pip in UNIX Shell Python Version Python 2 vs Python 3 : Python 2 and Python 3 are two different versions of the Python programming language. Python 2 is no longer maintained as of January 1, 2020, but it still exists on some systems for legacy reasons. python3 explicitly runs Python 3.x, whereas python might run either Python 2.x or Python 3.x depending on the system configuration. System Configuration Multiple Python Installations : On some systems, you may have both Python 2 and Python 3 installed. In such cases, python usually refers to Python 2 and python3 to Python 3. Similarly, pip might point to the package manager for Python 2, and pip3 will point to Python 3. Always use python3 and pip3 to ensure that you're working with Python 3.x. Aliases : Some systems alias python to python3 . This is common in more recent Linux distributions. On such systems, it may not matter if you use python or python3 . However, using python3 is more explicit and can avoid ambiguity. Script Compatibility Version-Specific Code : If you're running or writing code that is specific to Python 3, use python3 . Similarly, if you're installing packages that are intended for use with Python 3, use pip3 . Portability : If you're writing a script that you plan to share with others, it's safer to specify python3 if your code is not compatible with Python 2. Virtual Environments Virtualenv : If you're using a Python virtual environment, the python and pip commands will point to the versions associated with the active environment, regardless of whether it's Python 2 or 3. So, within a Python 3 virtual environment, python and pip will be equivalent to python3 and pip3 . Best Practices Explicit is Better : If you're in doubt, being explicit is usually better. Using python3 and pip3 makes it clear that you're using Python 3. Check Version : If ever in doubt, you can always check which version you're running by using python --version or python3 --version and pip --version or pip3 --version . In summary, if you want to make sure you are using Python 3 and its associated package manager, use python3 and pip3 .","title":"Installation Tips"},{"location":"setup/installation-tips/#installation-tips","text":"Installing the Donkey Car software is a complex process. It typically takes about six hours to add the required tools and Python libraries to a raw NVIDIA or Raspberry Pi OS image. Here are a few tips.","title":"Installation Tips"},{"location":"setup/installation-tips/#use-the-h-mode-when-installing-software","text":"We should always use the HOME environment variables when using sudo: 1 sudo -H pip install package_name In this command, sudo -H is used to ensure that the home environment variable is set to the home directory of the target user (root in this case), which can sometimes avoid permissions issues that arise when sudo retains your normal user's home directory environment variable. Essentially, the -H option makes sure that the operation is as clean as possible from an environment perspective, which can be important when installing software that might write configuration files or data into the user's home directory.","title":"Use the -H mode when installing software"},{"location":"setup/installation-tips/#remember-to-use-python3-and-pip3","text":"Sometimes older Python2 tools get mixed up with the current Python 3 tools. By adding the \"3\" suffix to your commands you can guarantee that that your path will pick up the right version of Python tools.","title":"Remember to Use Python3 and Pip3"},{"location":"setup/installation-tips/#when-to-use-python3-and-pip3-over-python-and-pip-in-unix-shell","text":"","title":"When to Use python3 and pip3 Over python and pip in UNIX Shell"},{"location":"setup/installation-tips/#python-version","text":"Python 2 vs Python 3 : Python 2 and Python 3 are two different versions of the Python programming language. Python 2 is no longer maintained as of January 1, 2020, but it still exists on some systems for legacy reasons. python3 explicitly runs Python 3.x, whereas python might run either Python 2.x or Python 3.x depending on the system configuration.","title":"Python Version"},{"location":"setup/installation-tips/#system-configuration","text":"Multiple Python Installations : On some systems, you may have both Python 2 and Python 3 installed. In such cases, python usually refers to Python 2 and python3 to Python 3. Similarly, pip might point to the package manager for Python 2, and pip3 will point to Python 3. Always use python3 and pip3 to ensure that you're working with Python 3.x. Aliases : Some systems alias python to python3 . This is common in more recent Linux distributions. On such systems, it may not matter if you use python or python3 . However, using python3 is more explicit and can avoid ambiguity.","title":"System Configuration"},{"location":"setup/installation-tips/#script-compatibility","text":"Version-Specific Code : If you're running or writing code that is specific to Python 3, use python3 . Similarly, if you're installing packages that are intended for use with Python 3, use pip3 . Portability : If you're writing a script that you plan to share with others, it's safer to specify python3 if your code is not compatible with Python 2.","title":"Script Compatibility"},{"location":"setup/installation-tips/#virtual-environments","text":"Virtualenv : If you're using a Python virtual environment, the python and pip commands will point to the versions associated with the active environment, regardless of whether it's Python 2 or 3. So, within a Python 3 virtual environment, python and pip will be equivalent to python3 and pip3 .","title":"Virtual Environments"},{"location":"setup/installation-tips/#best-practices","text":"Explicit is Better : If you're in doubt, being explicit is usually better. Using python3 and pip3 makes it clear that you're using Python 3. Check Version : If ever in doubt, you can always check which version you're running by using python --version or python3 --version and pip --version or pip3 --version . In summary, if you want to make sure you are using Python 3 and its associated package manager, use python3 and pip3 .","title":"Best Practices"},{"location":"setup/microSD-write-speed/","text":"Selecting the Right MicroSD Card Size Although you can boot the Raspberry Pi and NVIDIA Nano on a 16GB microSD card, it will quickly run out of room after you download the 36K Python libraries. Our suggestion is to use a minimum of a 64 GB card and ideally a 128 GB microSD card. In the item above, the \"Class 10\" indicates it will write above 10 MB/sec. Bandwidth Calculation for 224x224 Color Images What speed microSD card should you purchase? Low-cost cards write at 10 MB/sec and more expensive cards can run up to 100 MB/sec. What one should we purchase? The default image for the Donkey Car is 224x224 color images. Analysis Calculate the total microSD card write bandwidth needed for a camera that reads 224x224 color images and needs to write 20 images per second to the disk. Step-by-Step Calculation Step 1: Calculate the Size of a Single Image A 224x224 color image would have 224 * 224 = 50,176 pixels. Assuming the image uses a standard RGB color scheme, each pixel would have three color channels: Red, Green, and Blue. Each channel typically requires 1 byte (or 8 bits) to store its intensity value. So, each pixel would need 3 bytes. The size of a single image would be: ```linenums=\"0\" Size of a single image = Number of pixels * Bytes per pixel = 50,176 pixels * 3 bytes/pixel = 150,528 bytes/image = 150 KB approx 0.147 MB 1 2 3 4 5 6 7 8 9 ### Step 2: Calculate the Total Bandwidth Needed Per Second We are writing 20 images a second to the disk , so the total bandwidth needed per second would be : ``` linenums = \"0\" Total bandwidth per second = Size of a single image * Number of images per second = 0.147 MB * 20 = approx 2.94 megabytes / s Conclusion The microSD would need a total bandwidth of approximately 3 MB/s to write 20 224x224 color images to the disk each second. Since most microSD cards today write from 10 MB/s to 100 MB/s there is no worry about having enough write speed to the microSD card.","title":"MicroSD Options"},{"location":"setup/microSD-write-speed/#selecting-the-right-microsd-card","text":"","title":"Selecting the Right MicroSD Card"},{"location":"setup/microSD-write-speed/#size","text":"Although you can boot the Raspberry Pi and NVIDIA Nano on a 16GB microSD card, it will quickly run out of room after you download the 36K Python libraries. Our suggestion is to use a minimum of a 64 GB card and ideally a 128 GB microSD card. In the item above, the \"Class 10\" indicates it will write above 10 MB/sec.","title":"Size"},{"location":"setup/microSD-write-speed/#bandwidth-calculation-for-224x224-color-images","text":"What speed microSD card should you purchase? Low-cost cards write at 10 MB/sec and more expensive cards can run up to 100 MB/sec. What one should we purchase? The default image for the Donkey Car is 224x224 color images.","title":"Bandwidth Calculation for 224x224 Color Images"},{"location":"setup/microSD-write-speed/#analysis","text":"Calculate the total microSD card write bandwidth needed for a camera that reads 224x224 color images and needs to write 20 images per second to the disk.","title":"Analysis"},{"location":"setup/microSD-write-speed/#step-by-step-calculation","text":"","title":"Step-by-Step Calculation"},{"location":"setup/microSD-write-speed/#step-1-calculate-the-size-of-a-single-image","text":"A 224x224 color image would have 224 * 224 = 50,176 pixels. Assuming the image uses a standard RGB color scheme, each pixel would have three color channels: Red, Green, and Blue. Each channel typically requires 1 byte (or 8 bits) to store its intensity value. So, each pixel would need 3 bytes. The size of a single image would be: ```linenums=\"0\" Size of a single image = Number of pixels * Bytes per pixel = 50,176 pixels * 3 bytes/pixel = 150,528 bytes/image = 150 KB approx 0.147 MB 1 2 3 4 5 6 7 8 9 ### Step 2: Calculate the Total Bandwidth Needed Per Second We are writing 20 images a second to the disk , so the total bandwidth needed per second would be : ``` linenums = \"0\" Total bandwidth per second = Size of a single image * Number of images per second = 0.147 MB * 20 = approx 2.94 megabytes / s","title":"Step 1: Calculate the Size of a Single Image"},{"location":"setup/microSD-write-speed/#conclusion","text":"The microSD would need a total bandwidth of approximately 3 MB/s to write 20 224x224 color images to the disk each second. Since most microSD cards today write from 10 MB/s to 100 MB/s there is no worry about having enough write speed to the microSD card.","title":"Conclusion"},{"location":"setup/nano-docker/","text":"Docker on Nano !!! Note These instructions were copied from a message on the Discord Server that were posed by user naisy on at 08/31/2023 5:48 AM. They are not supported by the DonkeyCar project (yet). Due to the complexities of building the correct environments on the Nano, one alternative is to use the Docker container system. Although this adds some additional overhead, it means that you can download a virtual machine with all the correct software installed. You must have a minumum of a 64GB microSD card to run Docker on the NVIDIA Nano. Step 1: Download Docker GitHub Repo In this step we will clone a repository that has some very small UNIX shell scripts. 1 2 3 4 mkdir ~/projects cd ~/projects git clone https://github.com/naisy/Docker cd Docker Step 2: Run the Permissions Shell Script 1 2 3 4 5 6 mkdir ~/docker cp run-jetson-jp461-donkeycar??.sh ~/docker cp run-jetson-jp461-gpio-permission.sh ~/docker cd ~/docker ./run-jetson-jp461-gpio-permission.sh Step 3: Run the Main Shell Script Due to memory limitations, only one of either DonkeyCar 4.5 or DonkeyCar 5.0 should be activated. DonkeyCar 4.5 ./run-jetson-jp461-donkeycar45.sh or DonkeyCar 5.0 ./run-jetson-jp461-donkeycar50.sh Login into the Virtual Machine Log in to the jupyterlab terminal: PC Web browser for DonkeyCar 4.5 `` http://your_jetson_ip:8890/ 1 for DonkeyCar 5 . 0 ( dev3 ) http://your_jetson_ip:8891/ Password: jupyterlab Launch JupyterLab Terminal ``` DonkeyCar: From this point on, it is exactly the same as a official donkeycar. create mycar donkey createcar --path ~/data/mycar45 cd ~/data/mycar45 ls","title":"Nano Docker"},{"location":"setup/nano-docker/#docker-on-nano","text":"!!! Note These instructions were copied from a message on the Discord Server that were posed by user naisy on at 08/31/2023 5:48 AM. They are not supported by the DonkeyCar project (yet). Due to the complexities of building the correct environments on the Nano, one alternative is to use the Docker container system. Although this adds some additional overhead, it means that you can download a virtual machine with all the correct software installed. You must have a minumum of a 64GB microSD card to run Docker on the NVIDIA Nano.","title":"Docker on Nano"},{"location":"setup/nano-docker/#step-1-download-docker-github-repo","text":"In this step we will clone a repository that has some very small UNIX shell scripts. 1 2 3 4 mkdir ~/projects cd ~/projects git clone https://github.com/naisy/Docker cd Docker","title":"Step 1: Download Docker GitHub Repo"},{"location":"setup/nano-docker/#step-2-run-the-permissions-shell-script","text":"1 2 3 4 5 6 mkdir ~/docker cp run-jetson-jp461-donkeycar??.sh ~/docker cp run-jetson-jp461-gpio-permission.sh ~/docker cd ~/docker ./run-jetson-jp461-gpio-permission.sh","title":"Step 2: Run the Permissions Shell Script"},{"location":"setup/nano-docker/#step-3-run-the-main-shell-script","text":"Due to memory limitations, only one of either DonkeyCar 4.5 or DonkeyCar 5.0 should be activated.","title":"Step 3: Run the Main Shell Script"},{"location":"setup/nano-docker/#donkeycar-45","text":"./run-jetson-jp461-donkeycar45.sh","title":"DonkeyCar 4.5"},{"location":"setup/nano-docker/#or-donkeycar-50","text":"./run-jetson-jp461-donkeycar50.sh","title":"or DonkeyCar 5.0"},{"location":"setup/nano-docker/#login-into-the-virtual-machine","text":"Log in to the jupyterlab terminal: PC Web browser for DonkeyCar 4.5 `` http://your_jetson_ip:8890/ 1 for DonkeyCar 5 . 0 ( dev3 ) http://your_jetson_ip:8891/ Password: jupyterlab Launch JupyterLab Terminal ``` DonkeyCar: From this point on, it is exactly the same as a official donkeycar.","title":"Login into the Virtual Machine"},{"location":"setup/nano-docker/#create-mycar","text":"donkey createcar --path ~/data/mycar45 cd ~/data/mycar45 ls","title":"create mycar"},{"location":"setup/nano-python-upgrade/","text":"How to Upgrade Python on the NVIDIA Nano The current Jetson Nano image is version 4.6.1. The Donkey Car software only runs on 4.5.1 References Jetson Hacks","title":"How to Upgrade Python on the NVIDIA Nano"},{"location":"setup/nano-python-upgrade/#how-to-upgrade-python-on-the-nvidia-nano","text":"The current Jetson Nano image is version 4.6.1. The Donkey Car software only runs on 4.5.1","title":"How to Upgrade Python on the NVIDIA Nano"},{"location":"setup/nano-python-upgrade/#references","text":"Jetson Hacks","title":"References"},{"location":"setup/nano-sd-image-checklist/","text":"Nano SD Image Checklist This is a checklist that is genralized for all our events. We can't assume any network connectivity at these events. Required Image is based on the Nvidia Jetson image There is a user \"donkey\" with a password \"car\" The desktop has Chromium and Terminl locked at the top The DonkeyCar Bookmarks are in place The default WiFi is setup and working for the event (use a guest account if we are at company site?) A virtual envinroment is setup and the user is set to that automatically at the end of the .basrc script The Latest DonkeyCar software installed consistently and tested Optional Swap file setup (at least 6 gig) for compiling OpenCV The CSI Camera demo is installed from the Jetson Hacks site to test the camera and do face recognition demos The evtest program in installed to test the Logitech F710 joystick The default config.py and myconfig.py are setup and customized for the CSI camera The default image in the config.py file is 224X224 The \"desktop\" apps (word processing, spreadsheets, presentations) have been removed from the default dock The Chrome browser and the Terminal are on the dock The Chome bookmark bar is enabled (go to the Chrome Settings) The latest version of OpenCV (cv2) is installed, compiled and tested Decent python editor? Jupyter notebook support (arm version!) Sample Jupyter notebooks installed for viewing tub data and cleaning up the tub files (removing data with no speed) Unknowns Can we write a menu-driven UNIX script that will automatically copy tubs to a GPU server and get a model back? Can we assign static IP addresses and names to each car (dk1, dk2, dk3) Can we assume that ALL cars use the same default calibration? Can we write a short test script to verify that all the components are installed and working? What standards should we have for the GPU servers (Ubuntu, not RedHat) What other","title":"Nano SD Image Checklist"},{"location":"setup/nano-sd-image-checklist/#nano-sd-image-checklist","text":"This is a checklist that is genralized for all our events. We can't assume any network connectivity at these events.","title":"Nano SD Image Checklist"},{"location":"setup/nano-sd-image-checklist/#required","text":"Image is based on the Nvidia Jetson image There is a user \"donkey\" with a password \"car\" The desktop has Chromium and Terminl locked at the top The DonkeyCar Bookmarks are in place The default WiFi is setup and working for the event (use a guest account if we are at company site?) A virtual envinroment is setup and the user is set to that automatically at the end of the .basrc script The Latest DonkeyCar software installed consistently and tested","title":"Required"},{"location":"setup/nano-sd-image-checklist/#optional","text":"Swap file setup (at least 6 gig) for compiling OpenCV The CSI Camera demo is installed from the Jetson Hacks site to test the camera and do face recognition demos The evtest program in installed to test the Logitech F710 joystick The default config.py and myconfig.py are setup and customized for the CSI camera The default image in the config.py file is 224X224 The \"desktop\" apps (word processing, spreadsheets, presentations) have been removed from the default dock The Chrome browser and the Terminal are on the dock The Chome bookmark bar is enabled (go to the Chrome Settings) The latest version of OpenCV (cv2) is installed, compiled and tested Decent python editor? Jupyter notebook support (arm version!) Sample Jupyter notebooks installed for viewing tub data and cleaning up the tub files (removing data with no speed)","title":"Optional"},{"location":"setup/nano-sd-image-checklist/#unknowns","text":"Can we write a menu-driven UNIX script that will automatically copy tubs to a GPU server and get a model back? Can we assign static IP addresses and names to each car (dk1, dk2, dk3) Can we assume that ALL cars use the same default calibration? Can we write a short test script to verify that all the components are installed and working? What standards should we have for the GPU servers (Ubuntu, not RedHat) What other","title":"Unknowns"},{"location":"setup/pre-drive-checklist/","text":"DonkeyCar Pre-Drive Checklist Do you have all the software installed correctly? dependencies installed donkey command workings Do you have your PWM Board working? LED light on the PWM card i2cdetect working Battery checks Voltage in the motor battery is 7.2 volts Power level in SBC battery is 100% Configuration File Streering and Throttle calibrated","title":"DonkeyCar Pre-Drive Checklist"},{"location":"setup/pre-drive-checklist/#donkeycar-pre-drive-checklist","text":"Do you have all the software installed correctly? dependencies installed donkey command workings Do you have your PWM Board working? LED light on the PWM card i2cdetect working Battery checks Voltage in the motor battery is 7.2 volts Power level in SBC battery is 100% Configuration File Streering and Throttle calibrated","title":"DonkeyCar Pre-Drive Checklist"},{"location":"setup/pwm-board/","text":"PWM Board Deep Dive The DonkeyCar uses the low cost PCA9685 PWM board. PCA9685 PWM Board Pi 40 Pin Header Connections References Connections Handown (PowerPoint) Using a PCA9685 module with Raspberry Pi","title":"PWM Setup"},{"location":"setup/pwm-board/#pwm-board-deep-dive","text":"The DonkeyCar uses the low cost PCA9685 PWM board.","title":"PWM Board Deep Dive"},{"location":"setup/pwm-board/#pca9685-pwm-board","text":"","title":"PCA9685 PWM Board"},{"location":"setup/pwm-board/#pi-40-pin-header","text":"","title":"Pi 40 Pin Header"},{"location":"setup/pwm-board/#connections","text":"","title":"Connections"},{"location":"setup/pwm-board/#references","text":"Connections Handown (PowerPoint) Using a PCA9685 module with Raspberry Pi","title":"References"},{"location":"setup/software-install-notes/","text":"AI Racing League Software Installation Apt-get Apt-get is the software package installed on the Raspberry Pi OS that allows you to install application libraries. DonkeyCar Libraries (required) 1 sudo apt-get install build-essential python3 python3-dev python3-pip python3-virtualenv python3-numpy python3-picamera python3-pandas python3-rpi.gpio i2c-tools avahi-utils joystick libopenjp2-7-dev libtiff5-dev gfortran libatlas-base-dev libopenblas-dev libhdf5-serial-dev libgeos-dev git ntp build-essential is the library that tracks software library dependency lists. python3 is the library that runs Python 3. Note that the Raspberry Pi only has Python 2.7 as the default version. All our DonkeyCar software requires Python 3.7. python3-dev includes software development tools to manage python 3. python3-pip is the pip tool that allows you to install a specific version of a python library python-virtualenv is the tool that allows you to setup a virtual environment for the DonkeyCar specific libraries. We use this tool instead of the conda tools. python3-numpy are the numerical procssing python libraries. python3-picamera are the libaries to work with the camera on the Raspberry pi. python3-pandas are tools that allow data access for example reading CSV and JSON files. python2-rpi are python libraries that work with the Raspberry Pi. i2ctools are tools that allow you to work with the I2C communications bus. This is the bus that is used to control the PWM board and which controls the throttle and steering. The other libraries are mostly small support libraries used for supporting debugging. OpenCV (optional) 1 sudo apt-get install libilmbase-dev libopenexr-dev libgstreamer1.0-dev libjasper-dev libwebp-dev libatlas-base-dev libavcodec-dev libavformat-dev libswscale-dev libqtgui4 libqt4-test","title":"Software Installation Notes"},{"location":"setup/software-install-notes/#ai-racing-league-software-installation","text":"","title":"AI Racing League Software Installation"},{"location":"setup/software-install-notes/#apt-get","text":"Apt-get is the software package installed on the Raspberry Pi OS that allows you to install application libraries.","title":"Apt-get"},{"location":"setup/software-install-notes/#donkeycar-libraries-required","text":"1 sudo apt-get install build-essential python3 python3-dev python3-pip python3-virtualenv python3-numpy python3-picamera python3-pandas python3-rpi.gpio i2c-tools avahi-utils joystick libopenjp2-7-dev libtiff5-dev gfortran libatlas-base-dev libopenblas-dev libhdf5-serial-dev libgeos-dev git ntp build-essential is the library that tracks software library dependency lists. python3 is the library that runs Python 3. Note that the Raspberry Pi only has Python 2.7 as the default version. All our DonkeyCar software requires Python 3.7. python3-dev includes software development tools to manage python 3. python3-pip is the pip tool that allows you to install a specific version of a python library python-virtualenv is the tool that allows you to setup a virtual environment for the DonkeyCar specific libraries. We use this tool instead of the conda tools. python3-numpy are the numerical procssing python libraries. python3-picamera are the libaries to work with the camera on the Raspberry pi. python3-pandas are tools that allow data access for example reading CSV and JSON files. python2-rpi are python libraries that work with the Raspberry Pi. i2ctools are tools that allow you to work with the I2C communications bus. This is the bus that is used to control the PWM board and which controls the throttle and steering. The other libraries are mostly small support libraries used for supporting debugging.","title":"DonkeyCar Libraries (required)"},{"location":"setup/software-install-notes/#opencv-optional","text":"1 sudo apt-get install libilmbase-dev libopenexr-dev libgstreamer1.0-dev libjasper-dev libwebp-dev libatlas-base-dev libavcodec-dev libavformat-dev libswscale-dev libqtgui4 libqt4-test","title":"OpenCV (optional)"},{"location":"setup/track-options/","text":"Track Options Although you can just put tape down on a floor, that is time-consuming and is often a low-quality track. There are several other options and the prices vary from under $100 to $1,300. Vinyl Tarps In Minnesota, Billboard Tarps sells used vinyl sign material. For around $70 you can get a 16' X 25' used black billboard 9oz black vinyl sign that is ideal for creating your own track. Here is a full-size track that is 25' X 35' for about $170: This is what 1/2 of the 16'X'25 foot tarp looks like after delivery from Billboard Tarps and Vinyl . Billboard Tarps and Vinyl - We suggest you get a dark color (black or dark blue) and then tape down white edges and a yellow dashed line in the center. You can also buy smaller scraps and use HH-66 vinyl cement as an adhesive to glue smaller tarps together. You can also glue a black track on a white background to make the road. Picking the Right Size The typical dimensions of a full-event track is 22 x 34 feet. These dimensions are based on the DIYRobocars Standard Track , which is a popular track for donkey car racing. The smaller track is a good option for beginners, as it is easier to navigate and control. The larger track is a better option for experienced drivers, as it offers more challenges and opportunities for speed. Of course, the dimensions of a donkey car track can vary depending on the specific design. However, the dimensions listed above are a good starting point for anyone who is planning to build or race a donkey car. Keeping A Standard Width The standard width of all the \"road\" tracks is two feet or 24 inches. This is the distance to the centerline of the white edges. The roads are typically black with a white edge and a dashed yellow line down the middle of the track. The key is to have a high contrast between the black road and the white edges. Many people use 2\" (or 1 and 7/8\") inch wide duct tape or Gaffers tape. Gaffer's tape is often preferred for temporary events on carpet. Gaffer's tape doesn't harm the surface to which it adhered. Minnesota STEM Partners Tracks Below is a sample of a tarp purchased from Billboard Tarps. Note the actual track is twice this size since it is still folded in half in this photo. Track setup in the driver training room: Note that this track does not adhere to the 2-foot wide rule. This is sometimes done when you have many students doing practice driving on the same track. Optum Track Optum printed their own track on a local printer that specialized in printing large format signage. The custom printing cost was about $1,300.00 Best Buy Track Best Buy also printed its own track for their events. This photo only shows about 1/3 of the track. Dan McCreary's Basement Track This track is just a single piece of white electrical tape. Interlocking Foam Mats You can also purchase interlocking foam mats. These are typically two feet by two feet and cost about $30 for a package of 6. Since each package covers 24 square feet and a full track is about 24x36 feet (758 square feet) we can see the cost of 32 packages is around $960.00. Interlocking Foam Mats From WalMart Amazon Foam Mats References DIYRobocars Standard Track","title":"Track Options"},{"location":"setup/track-options/#track-options","text":"Although you can just put tape down on a floor, that is time-consuming and is often a low-quality track. There are several other options and the prices vary from under $100 to $1,300.","title":"Track Options"},{"location":"setup/track-options/#vinyl-tarps","text":"In Minnesota, Billboard Tarps sells used vinyl sign material. For around $70 you can get a 16' X 25' used black billboard 9oz black vinyl sign that is ideal for creating your own track. Here is a full-size track that is 25' X 35' for about $170: This is what 1/2 of the 16'X'25 foot tarp looks like after delivery from Billboard Tarps and Vinyl . Billboard Tarps and Vinyl - We suggest you get a dark color (black or dark blue) and then tape down white edges and a yellow dashed line in the center. You can also buy smaller scraps and use HH-66 vinyl cement as an adhesive to glue smaller tarps together. You can also glue a black track on a white background to make the road.","title":"Vinyl Tarps"},{"location":"setup/track-options/#picking-the-right-size","text":"The typical dimensions of a full-event track is 22 x 34 feet. These dimensions are based on the DIYRobocars Standard Track , which is a popular track for donkey car racing. The smaller track is a good option for beginners, as it is easier to navigate and control. The larger track is a better option for experienced drivers, as it offers more challenges and opportunities for speed. Of course, the dimensions of a donkey car track can vary depending on the specific design. However, the dimensions listed above are a good starting point for anyone who is planning to build or race a donkey car.","title":"Picking the Right Size"},{"location":"setup/track-options/#keeping-a-standard-width","text":"The standard width of all the \"road\" tracks is two feet or 24 inches. This is the distance to the centerline of the white edges. The roads are typically black with a white edge and a dashed yellow line down the middle of the track. The key is to have a high contrast between the black road and the white edges. Many people use 2\" (or 1 and 7/8\") inch wide duct tape or Gaffers tape. Gaffer's tape is often preferred for temporary events on carpet. Gaffer's tape doesn't harm the surface to which it adhered.","title":"Keeping A Standard Width"},{"location":"setup/track-options/#minnesota-stem-partners-tracks","text":"Below is a sample of a tarp purchased from Billboard Tarps. Note the actual track is twice this size since it is still folded in half in this photo. Track setup in the driver training room: Note that this track does not adhere to the 2-foot wide rule. This is sometimes done when you have many students doing practice driving on the same track.","title":"Minnesota STEM Partners Tracks"},{"location":"setup/track-options/#optum-track","text":"Optum printed their own track on a local printer that specialized in printing large format signage. The custom printing cost was about $1,300.00","title":"Optum Track"},{"location":"setup/track-options/#best-buy-track","text":"Best Buy also printed its own track for their events. This photo only shows about 1/3 of the track.","title":"Best Buy Track"},{"location":"setup/track-options/#dan-mccrearys-basement-track","text":"This track is just a single piece of white electrical tape.","title":"Dan McCreary's Basement Track"},{"location":"setup/track-options/#interlocking-foam-mats","text":"You can also purchase interlocking foam mats. These are typically two feet by two feet and cost about $30 for a package of 6. Since each package covers 24 square feet and a full track is about 24x36 feet (758 square feet) we can see the cost of 32 packages is around $960.00. Interlocking Foam Mats From WalMart Amazon Foam Mats","title":"Interlocking Foam Mats"},{"location":"setup/track-options/#references","text":"DIYRobocars Standard Track","title":"References"},{"location":"training-logs/dans-basement/","text":"Dans Basement Training Log I have a very small track in my basement. I put down a single white line about 3/4 inch wide using white electrical tape. The background was a marble blue expoy floor with a lot of color variation. The surface was very reflective and there were lights in the ceiling with lots of glare. I drove the car around 10 times in each direction and collected around 4,500 images. Catalogs I manually edited the catlog files and then edited the manifest.json file to modify the paths: 1 {\"paths\": [\"catalog_3.catalog\", \"catalog_4.catalog\", \"catalog_5.catalog\", \"catalog_6.catalog\", \"catalog_7.catalog\"] 1 wc -l data/dans-basement/*.catalog 1 2 3 4 5 6 781 data/dans-basement/catalog_3.catalog 1000 data/dans-basement/catalog_4.catalog 1000 data/dans-basement/catalog_5.catalog 1000 data/dans-basement/catalog_6.catalog 750 data/dans-basement/catalog_7.catalog 4531 total This matched the ls -1 ~/mycar/data/dans-basement/images | wc -l command that counted the number of images. I time the training time on the NIVID RTX 2080 and got the model trained in about 1.5 minutes. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 $ time donkey train -- tub =./ data / dans - basement -- model =./ models / dans - basement . h5 ________ ______ _________ ___ __ \\ _______________ / ___________ __ __ ____ / _____ ________ __ / / / __ \\ _ __ \\ _ // _ / _ \\ _ / / / _ / _ __ ` / _ ___ / _ / _ / // / _ / / / / / , < / __ / / _ / / / / ___ / / _ / / _ / / _____ / \\ ____ // _ / / _ // _ /| _ | \\ ___ / _ \\ __ , / \\ ____ / \\ __ , _ / / _ / / ____ / using donkey v4 . 2.1 ... loading config file : ./ config . py loading personal config over - rides from myconfig . py \"get_model_by_type\" model Type is : linear Created KerasLinear 2021 - 07 - 26 21 : 05 : 34.259364 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcuda . so . 1 2021 - 07 - 26 21 : 05 : 34.278301 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 05 : 34.278898 : I tensorflow / core / common_runtime / gpu / gpu_device . cc : 1561 ] Found device 0 with properties : pciBusID : 0000 : 09 : 00.0 name : NVIDIA GeForce RTX 2080 Ti computeCapability : 7.5 coreClock : 1.635 GHz coreCount : 68 deviceMemorySize : 10.76 GiB deviceMemoryBandwidth : 573.69 GiB / s 2021 - 07 - 26 21 : 05 : 34.279098 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcudart . so . 10.1 2021 - 07 - 26 21 : 05 : 34.280320 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcublas . so . 10 2021 - 07 - 26 21 : 05 : 34.281822 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcufft . so . 10 2021 - 07 - 26 21 : 05 : 34.282037 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcurand . so . 10 2021 - 07 - 26 21 : 05 : 34.283140 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcusolver . so . 10 2021 - 07 - 26 21 : 05 : 34.283726 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcusparse . so . 10 2021 - 07 - 26 21 : 05 : 34.285524 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcudnn . so . 7 2021 - 07 - 26 21 : 05 : 34.285676 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 05 : 34.286176 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 05 : 34.286568 : I tensorflow / core / common_runtime / gpu / gpu_device . cc : 1703 ] Adding visible gpu devices : 0 2021 - 07 - 26 21 : 05 : 34.286793 : I tensorflow / core / platform / cpu_feature_guard . cc : 143 ] Your CPU supports instructions that this TensorFlow binary was not compiled to use : SSE4 . 1 SSE4 . 2 AVX AVX2 FMA 2021 - 07 - 26 21 : 05 : 34.290920 : I tensorflow / core / platform / profile_utils / cpu_utils . cc : 102 ] CPU Frequency : 3592950000 Hz 2021 - 07 - 26 21 : 05 : 34.291228 : I tensorflow / compiler / xla / service / service . cc : 168 ] XLA service 0x557d8a05bbb0 initialized for platform Host ( this does not guarantee that XLA will be used ) . Devices : 2021 - 07 - 26 21 : 05 : 34.291241 : I tensorflow / compiler / xla / service / service . cc : 176 ] StreamExecutor device ( 0 ): Host , Default Version 2021 - 07 - 26 21 : 05 : 34.291374 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 05 : 34.291795 : I tensorflow / core / common_runtime / gpu / gpu_device . cc : 1561 ] Found device 0 with properties : pciBusID : 0000 : 09 : 00.0 name : NVIDIA GeForce RTX 2080 Ti computeCapability : 7.5 coreClock : 1.635 GHz coreCount : 68 deviceMemorySize : 10.76 GiB deviceMemoryBandwidth : 573.69 GiB / s 2021 - 07 - 26 21 : 05 : 34.291830 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcudart . so . 10.1 2021 - 07 - 26 21 : 05 : 34.291842 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcublas . so . 10 2021 - 07 - 26 21 : 05 : 34.291852 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcufft . so . 10 2021 - 07 - 26 21 : 05 : 34.291862 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcurand . so . 10 2021 - 07 - 26 21 : 05 : 34.291872 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcusolver . so . 10 2021 - 07 - 26 21 : 05 : 34.291881 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcusparse . so . 10 2021 - 07 - 26 21 : 05 : 34.291891 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcudnn . so . 7 2021 - 07 - 26 21 : 05 : 34.291955 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 05 : 34.292398 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 05 : 34.292782 : I tensorflow / core / common_runtime / gpu / gpu_device . cc : 1703 ] Adding visible gpu devices : 0 2021 - 07 - 26 21 : 05 : 34.292805 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcudart . so . 10.1 2021 - 07 - 26 21 : 05 : 34.366898 : I tensorflow / core / common_runtime / gpu / gpu_device . cc : 1102 ] Device interconnect StreamExecutor with strength 1 edge matrix : 2021 - 07 - 26 21 : 05 : 34.366930 : I tensorflow / core / common_runtime / gpu / gpu_device . cc : 1108 ] 0 2021 - 07 - 26 21 : 05 : 34.366937 : I tensorflow / core / common_runtime / gpu / gpu_device . cc : 1121 ] 0 : N 2021 - 07 - 26 21 : 05 : 34.367194 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 05 : 34.367855 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 05 : 34.368446 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 05 : 34.368971 : I tensorflow / core / common_runtime / gpu / gpu_device . cc : 1247 ] Created TensorFlow device ( / job : localhost / replica : 0 / task : 0 / device : GPU : 0 with 9911 MB memory ) -> physical GPU ( device : 0 , name : NVIDIA GeForce RTX 2080 Ti , pci bus id : 0000 : 09 : 00.0 , compute capability : 7.5 ) 2021 - 07 - 26 21 : 05 : 34.370680 : I tensorflow / compiler / xla / service / service . cc : 168 ] XLA service 0x557d8bec8fa0 initialized for platform CUDA ( this does not guarantee that XLA will be used ) . Devices : 2021 - 07 - 26 21 : 05 : 34.370693 : I tensorflow / compiler / xla / service / service . cc : 176 ] StreamExecutor device ( 0 ): NVIDIA GeForce RTX 2080 Ti , Compute Capability 7.5 Model : \"model\" __________________________________________________________________________________________________ Layer ( type ) Output Shape Param # Connected to ================================================================================================== img_in ( InputLayer ) [( None , 224 , 224 , 3 ) 0 __________________________________________________________________________________________________ conv2d_1 ( Conv2D ) ( None , 110 , 110 , 24 ) 1824 img_in [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout ( Dropout ) ( None , 110 , 110 , 24 ) 0 conv2d_1 [ 0 ][ 0 ] __________________________________________________________________________________________________ conv2d_2 ( Conv2D ) ( None , 53 , 53 , 32 ) 19232 dropout [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_1 ( Dropout ) ( None , 53 , 53 , 32 ) 0 conv2d_2 [ 0 ][ 0 ] __________________________________________________________________________________________________ conv2d_3 ( Conv2D ) ( None , 25 , 25 , 64 ) 51264 dropout_1 [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_2 ( Dropout ) ( None , 25 , 25 , 64 ) 0 conv2d_3 [ 0 ][ 0 ] __________________________________________________________________________________________________ conv2d_4 ( Conv2D ) ( None , 23 , 23 , 64 ) 36928 dropout_2 [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_3 ( Dropout ) ( None , 23 , 23 , 64 ) 0 conv2d_4 [ 0 ][ 0 ] __________________________________________________________________________________________________ conv2d_5 ( Conv2D ) ( None , 21 , 21 , 64 ) 36928 dropout_3 [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_4 ( Dropout ) ( None , 21 , 21 , 64 ) 0 conv2d_5 [ 0 ][ 0 ] __________________________________________________________________________________________________ flattened ( Flatten ) ( None , 28224 ) 0 dropout_4 [ 0 ][ 0 ] __________________________________________________________________________________________________ dense_1 ( Dense ) ( None , 100 ) 2822500 flattened [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_5 ( Dropout ) ( None , 100 ) 0 dense_1 [ 0 ][ 0 ] __________________________________________________________________________________________________ dense_2 ( Dense ) ( None , 50 ) 5050 dropout_5 [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_6 ( Dropout ) ( None , 50 ) 0 dense_2 [ 0 ][ 0 ] __________________________________________________________________________________________________ n_outputs0 ( Dense ) ( None , 1 ) 51 dropout_6 [ 0 ][ 0 ] __________________________________________________________________________________________________ n_outputs1 ( Dense ) ( None , 1 ) 51 dropout_6 [ 0 ][ 0 ] ================================================================================================== Total params : 2 , 973 , 828 Trainable params : 2 , 973 , 828 Non - trainable params : 0 __________________________________________________________________________________________________ None Using catalog / home / arl / mycar / data / dans - basement / catalog_7 . catalog Records # Training 3364 Records # Validation 842 Epoch 1 / 100 2021 - 07 - 26 21 : 05 : 35.291438 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcublas . so . 10 2021 - 07 - 26 21 : 05 : 35.613762 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcudnn . so . 7 2021 - 07 - 26 21 : 05 : 36.322576 : W tensorflow / stream_executor / gpu / asm_compiler . cc : 116 ] *** WARNING *** You are using ptxas 9.1 . 108 , which is older than 9.2 . 88. ptxas 9. x before 9.2 . 88 is known to miscompile XLA code , leading to incorrect results or invalid - address errors . You do not need to update to CUDA 9.2 . 88 ; cherry - picking the ptxas binary is sufficient . 2021 - 07 - 26 21 : 05 : 36.376195 : W tensorflow / stream_executor / gpu / redzone_allocator . cc : 314 ] Internal : ptxas exited with non - zero error code 65280 , output : ptxas fatal : Value 'sm_75' is not defined for option 'gpu-name' Relying on driver to perform ptx compilation . Modify $ PATH to customize ptxas location . This message will be only logged once . 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.2495 - n_outputs0_loss : 0.1717 - n_outputs1_loss : 0.0778 Epoch 00001 : val_loss improved from inf to 0.14744 , saving model to ./ models / dans - basement . h5 27 / 27 [ ============================== ] - 8 s 301 ms / step - loss : 0.2495 - n_outputs0_loss : 0.1717 - n_outputs1_loss : 0.0778 - val_loss : 0.1474 - val_n_outputs0_loss : 0.1291 - val_n_outputs1_loss : 0.0183 Epoch 2 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.1487 - n_outputs0_loss : 0.1265 - n_outputs1_loss : 0.0223 Epoch 00002 : val_loss improved from 0.14744 to 0.09815 , saving model to ./ models / dans - basement . h5 27 / 27 [ ============================== ] - 3 s 120 ms / step - loss : 0.1487 - n_outputs0_loss : 0.1265 - n_outputs1_loss : 0.0223 - val_loss : 0.0981 - val_n_outputs0_loss : 0.0777 - val_n_outputs1_loss : 0.0205 Epoch 3 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.1075 - n_outputs0_loss : 0.0893 - n_outputs1_loss : 0.0182 Epoch 00003 : val_loss improved from 0.09815 to 0.07897 , saving model to ./ models / dans - basement . h5 27 / 27 [ ============================== ] - 3 s 117 ms / step - loss : 0.1075 - n_outputs0_loss : 0.0893 - n_outputs1_loss : 0.0182 - val_loss : 0.0790 - val_n_outputs0_loss : 0.0687 - val_n_outputs1_loss : 0.0102 Epoch 4 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0917 - n_outputs0_loss : 0.0759 - n_outputs1_loss : 0.0158 Epoch 00004 : val_loss improved from 0.07897 to 0.07055 , saving model to ./ models / dans - basement . h5 27 / 27 [ ============================== ] - 3 s 110 ms / step - loss : 0.0917 - n_outputs0_loss : 0.0759 - n_outputs1_loss : 0.0158 - val_loss : 0.0705 - val_n_outputs0_loss : 0.0610 - val_n_outputs1_loss : 0.0096 Epoch 5 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0880 - n_outputs0_loss : 0.0734 - n_outputs1_loss : 0.0146 Epoch 00005 : val_loss did not improve from 0.07055 27 / 27 [ ============================== ] - 3 s 105 ms / step - loss : 0.0880 - n_outputs0_loss : 0.0734 - n_outputs1_loss : 0.0146 - val_loss : 0.0751 - val_n_outputs0_loss : 0.0553 - val_n_outputs1_loss : 0.0198 Epoch 6 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0757 - n_outputs0_loss : 0.0629 - n_outputs1_loss : 0.0127 Epoch 00006 : val_loss improved from 0.07055 to 0.05840 , saving model to ./ models / dans - basement . h5 27 / 27 [ ============================== ] - 3 s 111 ms / step - loss : 0.0757 - n_outputs0_loss : 0.0629 - n_outputs1_loss : 0.0127 - val_loss : 0.0584 - val_n_outputs0_loss : 0.0485 - val_n_outputs1_loss : 0.0099 Epoch 7 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0672 - n_outputs0_loss : 0.0551 - n_outputs1_loss : 0.0120 Epoch 00007 : val_loss improved from 0.05840 to 0.05028 , saving model to ./ models / dans - basement . h5 27 / 27 [ ============================== ] - 3 s 110 ms / step - loss : 0.0672 - n_outputs0_loss : 0.0551 - n_outputs1_loss : 0.0120 - val_loss : 0.0503 - val_n_outputs0_loss : 0.0450 - val_n_outputs1_loss : 0.0053 Epoch 8 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0621 - n_outputs0_loss : 0.0510 - n_outputs1_loss : 0.0111 Epoch 00008 : val_loss improved from 0.05028 to 0.04540 , saving model to ./ models / dans - basement . h5 27 / 27 [ ============================== ] - 3 s 110 ms / step - loss : 0.0621 - n_outputs0_loss : 0.0510 - n_outputs1_loss : 0.0111 - val_loss : 0.0454 - val_n_outputs0_loss : 0.0385 - val_n_outputs1_loss : 0.0069 Epoch 9 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0545 - n_outputs0_loss : 0.0441 - n_outputs1_loss : 0.0104 Epoch 00009 : val_loss improved from 0.04540 to 0.04351 , saving model to ./ models / dans - basement . h5 27 / 27 [ ============================== ] - 3 s 107 ms / step - loss : 0.0545 - n_outputs0_loss : 0.0441 - n_outputs1_loss : 0.0104 - val_loss : 0.0435 - val_n_outputs0_loss : 0.0358 - val_n_outputs1_loss : 0.0077 Epoch 10 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0558 - n_outputs0_loss : 0.0458 - n_outputs1_loss : 0.0099 Epoch 00010 : val_loss improved from 0.04351 to 0.04070 , saving model to ./ models / dans - basement . h5 27 / 27 [ ============================== ] - 3 s 110 ms / step - loss : 0.0558 - n_outputs0_loss : 0.0458 - n_outputs1_loss : 0.0099 - val_loss : 0.0407 - val_n_outputs0_loss : 0.0357 - val_n_outputs1_loss : 0.0050 Epoch 11 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0505 - n_outputs0_loss : 0.0415 - n_outputs1_loss : 0.0090 Epoch 00011 : val_loss improved from 0.04070 to 0.03935 , saving model to ./ models / dans - basement . h5 27 / 27 [ ============================== ] - 3 s 109 ms / step - loss : 0.0505 - n_outputs0_loss : 0.0415 - n_outputs1_loss : 0.0090 - val_loss : 0.0393 - val_n_outputs0_loss : 0.0340 - val_n_outputs1_loss : 0.0054 Epoch 12 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0476 - n_outputs0_loss : 0.0388 - n_outputs1_loss : 0.0088 Epoch 00012 : val_loss improved from 0.03935 to 0.03624 , saving model to ./ models / dans - basement . h5 27 / 27 [ ============================== ] - 3 s 110 ms / step - loss : 0.0476 - n_outputs0_loss : 0.0388 - n_outputs1_loss : 0.0088 - val_loss : 0.0362 - val_n_outputs0_loss : 0.0298 - val_n_outputs1_loss : 0.0065 Epoch 13 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0453 - n_outputs0_loss : 0.0373 - n_outputs1_loss : 0.0080 Epoch 00013 : val_loss improved from 0.03624 to 0.03507 , saving model to ./ models / dans - basement . h5 27 / 27 [ ============================== ] - 3 s 108 ms / step - loss : 0.0453 - n_outputs0_loss : 0.0373 - n_outputs1_loss : 0.0080 - val_loss : 0.0351 - val_n_outputs0_loss : 0.0294 - val_n_outputs1_loss : 0.0057 Epoch 14 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0430 - n_outputs0_loss : 0.0352 - n_outputs1_loss : 0.0079 Epoch 00014 : val_loss improved from 0.03507 to 0.03211 , saving model to ./ models / dans - basement . h5 27 / 27 [ ============================== ] - 3 s 111 ms / step - loss : 0.0430 - n_outputs0_loss : 0.0352 - n_outputs1_loss : 0.0079 - val_loss : 0.0321 - val_n_outputs0_loss : 0.0265 - val_n_outputs1_loss : 0.0056 Epoch 15 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0397 - n_outputs0_loss : 0.0327 - n_outputs1_loss : 0.0070 Epoch 00015 : val_loss improved from 0.03211 to 0.03208 , saving model to ./ models / dans - basement . h5 27 / 27 [ ============================== ] - 3 s 110 ms / step - loss : 0.0397 - n_outputs0_loss : 0.0327 - n_outputs1_loss : 0.0070 - val_loss : 0.0321 - val_n_outputs0_loss : 0.0279 - val_n_outputs1_loss : 0.0042 Epoch 16 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0382 - n_outputs0_loss : 0.0316 - n_outputs1_loss : 0.0065 Epoch 00016 : val_loss improved from 0.03208 to 0.02880 , saving model to ./ models / dans - basement . h5 27 / 27 [ ============================== ] - 3 s 108 ms / step - loss : 0.0382 - n_outputs0_loss : 0.0316 - n_outputs1_loss : 0.0065 - val_loss : 0.0288 - val_n_outputs0_loss : 0.0243 - val_n_outputs1_loss : 0.0046 Epoch 17 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0381 - n_outputs0_loss : 0.0313 - n_outputs1_loss : 0.0069 Epoch 00017 : val_loss did not improve from 0.02880 27 / 27 [ ============================== ] - 3 s 104 ms / step - loss : 0.0381 - n_outputs0_loss : 0.0313 - n_outputs1_loss : 0.0069 - val_loss : 0.0322 - val_n_outputs0_loss : 0.0281 - val_n_outputs1_loss : 0.0041 Epoch 18 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0375 - n_outputs0_loss : 0.0310 - n_outputs1_loss : 0.0065 Epoch 00018 : val_loss did not improve from 0.02880 27 / 27 [ ============================== ] - 3 s 107 ms / step - loss : 0.0375 - n_outputs0_loss : 0.0310 - n_outputs1_loss : 0.0065 - val_loss : 0.0293 - val_n_outputs0_loss : 0.0257 - val_n_outputs1_loss : 0.0036 Epoch 19 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0372 - n_outputs0_loss : 0.0308 - n_outputs1_loss : 0.0064 Epoch 00019 : val_loss did not improve from 0.02880 27 / 27 [ ============================== ] - 3 s 108 ms / step - loss : 0.0372 - n_outputs0_loss : 0.0308 - n_outputs1_loss : 0.0064 - val_loss : 0.0307 - val_n_outputs0_loss : 0.0275 - val_n_outputs1_loss : 0.0032 Epoch 20 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0347 - n_outputs0_loss : 0.0285 - n_outputs1_loss : 0.0062 Epoch 00020 : val_loss did not improve from 0.02880 27 / 27 [ ============================== ] - 3 s 104 ms / step - loss : 0.0347 - n_outputs0_loss : 0.0285 - n_outputs1_loss : 0.0062 - val_loss : 0.0325 - val_n_outputs0_loss : 0.0283 - val_n_outputs1_loss : 0.0042 Epoch 21 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0349 - n_outputs0_loss : 0.0290 - n_outputs1_loss : 0.0058 Epoch 00021 : val_loss did not improve from 0.02880 27 / 27 [ ============================== ] - 3 s 107 ms / step - loss : 0.0349 - n_outputs0_loss : 0.0290 - n_outputs1_loss : 0.0058 - val_loss : 0.0293 - val_n_outputs0_loss : 0.0258 - val_n_outputs1_loss : 0.0035 WARNING : CPU random generator seem to be failing , disable hardware random number generation WARNING : RDRND generated : 0xffffffff 0xffffffff 0xffffffff 0xffffffff real 1 m26 . 930 s user 1 m30 . 911 s sys 0 m42 . 818 s","title":"Dan's Basement"},{"location":"training-logs/dans-basement/#dans-basement-training-log","text":"I have a very small track in my basement. I put down a single white line about 3/4 inch wide using white electrical tape. The background was a marble blue expoy floor with a lot of color variation. The surface was very reflective and there were lights in the ceiling with lots of glare. I drove the car around 10 times in each direction and collected around 4,500 images.","title":"Dans Basement Training Log"},{"location":"training-logs/dans-basement/#catalogs","text":"I manually edited the catlog files and then edited the manifest.json file to modify the paths: 1 {\"paths\": [\"catalog_3.catalog\", \"catalog_4.catalog\", \"catalog_5.catalog\", \"catalog_6.catalog\", \"catalog_7.catalog\"] 1 wc -l data/dans-basement/*.catalog 1 2 3 4 5 6 781 data/dans-basement/catalog_3.catalog 1000 data/dans-basement/catalog_4.catalog 1000 data/dans-basement/catalog_5.catalog 1000 data/dans-basement/catalog_6.catalog 750 data/dans-basement/catalog_7.catalog 4531 total This matched the ls -1 ~/mycar/data/dans-basement/images | wc -l command that counted the number of images. I time the training time on the NIVID RTX 2080 and got the model trained in about 1.5 minutes. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 $ time donkey train -- tub =./ data / dans - basement -- model =./ models / dans - basement . h5 ________ ______ _________ ___ __ \\ _______________ / ___________ __ __ ____ / _____ ________ __ / / / __ \\ _ __ \\ _ // _ / _ \\ _ / / / _ / _ __ ` / _ ___ / _ / _ / // / _ / / / / / , < / __ / / _ / / / / ___ / / _ / / _ / / _____ / \\ ____ // _ / / _ // _ /| _ | \\ ___ / _ \\ __ , / \\ ____ / \\ __ , _ / / _ / / ____ / using donkey v4 . 2.1 ... loading config file : ./ config . py loading personal config over - rides from myconfig . py \"get_model_by_type\" model Type is : linear Created KerasLinear 2021 - 07 - 26 21 : 05 : 34.259364 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcuda . so . 1 2021 - 07 - 26 21 : 05 : 34.278301 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 05 : 34.278898 : I tensorflow / core / common_runtime / gpu / gpu_device . cc : 1561 ] Found device 0 with properties : pciBusID : 0000 : 09 : 00.0 name : NVIDIA GeForce RTX 2080 Ti computeCapability : 7.5 coreClock : 1.635 GHz coreCount : 68 deviceMemorySize : 10.76 GiB deviceMemoryBandwidth : 573.69 GiB / s 2021 - 07 - 26 21 : 05 : 34.279098 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcudart . so . 10.1 2021 - 07 - 26 21 : 05 : 34.280320 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcublas . so . 10 2021 - 07 - 26 21 : 05 : 34.281822 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcufft . so . 10 2021 - 07 - 26 21 : 05 : 34.282037 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcurand . so . 10 2021 - 07 - 26 21 : 05 : 34.283140 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcusolver . so . 10 2021 - 07 - 26 21 : 05 : 34.283726 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcusparse . so . 10 2021 - 07 - 26 21 : 05 : 34.285524 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcudnn . so . 7 2021 - 07 - 26 21 : 05 : 34.285676 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 05 : 34.286176 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 05 : 34.286568 : I tensorflow / core / common_runtime / gpu / gpu_device . cc : 1703 ] Adding visible gpu devices : 0 2021 - 07 - 26 21 : 05 : 34.286793 : I tensorflow / core / platform / cpu_feature_guard . cc : 143 ] Your CPU supports instructions that this TensorFlow binary was not compiled to use : SSE4 . 1 SSE4 . 2 AVX AVX2 FMA 2021 - 07 - 26 21 : 05 : 34.290920 : I tensorflow / core / platform / profile_utils / cpu_utils . cc : 102 ] CPU Frequency : 3592950000 Hz 2021 - 07 - 26 21 : 05 : 34.291228 : I tensorflow / compiler / xla / service / service . cc : 168 ] XLA service 0x557d8a05bbb0 initialized for platform Host ( this does not guarantee that XLA will be used ) . Devices : 2021 - 07 - 26 21 : 05 : 34.291241 : I tensorflow / compiler / xla / service / service . cc : 176 ] StreamExecutor device ( 0 ): Host , Default Version 2021 - 07 - 26 21 : 05 : 34.291374 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 05 : 34.291795 : I tensorflow / core / common_runtime / gpu / gpu_device . cc : 1561 ] Found device 0 with properties : pciBusID : 0000 : 09 : 00.0 name : NVIDIA GeForce RTX 2080 Ti computeCapability : 7.5 coreClock : 1.635 GHz coreCount : 68 deviceMemorySize : 10.76 GiB deviceMemoryBandwidth : 573.69 GiB / s 2021 - 07 - 26 21 : 05 : 34.291830 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcudart . so . 10.1 2021 - 07 - 26 21 : 05 : 34.291842 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcublas . so . 10 2021 - 07 - 26 21 : 05 : 34.291852 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcufft . so . 10 2021 - 07 - 26 21 : 05 : 34.291862 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcurand . so . 10 2021 - 07 - 26 21 : 05 : 34.291872 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcusolver . so . 10 2021 - 07 - 26 21 : 05 : 34.291881 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcusparse . so . 10 2021 - 07 - 26 21 : 05 : 34.291891 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcudnn . so . 7 2021 - 07 - 26 21 : 05 : 34.291955 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 05 : 34.292398 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 05 : 34.292782 : I tensorflow / core / common_runtime / gpu / gpu_device . cc : 1703 ] Adding visible gpu devices : 0 2021 - 07 - 26 21 : 05 : 34.292805 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcudart . so . 10.1 2021 - 07 - 26 21 : 05 : 34.366898 : I tensorflow / core / common_runtime / gpu / gpu_device . cc : 1102 ] Device interconnect StreamExecutor with strength 1 edge matrix : 2021 - 07 - 26 21 : 05 : 34.366930 : I tensorflow / core / common_runtime / gpu / gpu_device . cc : 1108 ] 0 2021 - 07 - 26 21 : 05 : 34.366937 : I tensorflow / core / common_runtime / gpu / gpu_device . cc : 1121 ] 0 : N 2021 - 07 - 26 21 : 05 : 34.367194 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 05 : 34.367855 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 05 : 34.368446 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 05 : 34.368971 : I tensorflow / core / common_runtime / gpu / gpu_device . cc : 1247 ] Created TensorFlow device ( / job : localhost / replica : 0 / task : 0 / device : GPU : 0 with 9911 MB memory ) -> physical GPU ( device : 0 , name : NVIDIA GeForce RTX 2080 Ti , pci bus id : 0000 : 09 : 00.0 , compute capability : 7.5 ) 2021 - 07 - 26 21 : 05 : 34.370680 : I tensorflow / compiler / xla / service / service . cc : 168 ] XLA service 0x557d8bec8fa0 initialized for platform CUDA ( this does not guarantee that XLA will be used ) . Devices : 2021 - 07 - 26 21 : 05 : 34.370693 : I tensorflow / compiler / xla / service / service . cc : 176 ] StreamExecutor device ( 0 ): NVIDIA GeForce RTX 2080 Ti , Compute Capability 7.5 Model : \"model\" __________________________________________________________________________________________________ Layer ( type ) Output Shape Param # Connected to ================================================================================================== img_in ( InputLayer ) [( None , 224 , 224 , 3 ) 0 __________________________________________________________________________________________________ conv2d_1 ( Conv2D ) ( None , 110 , 110 , 24 ) 1824 img_in [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout ( Dropout ) ( None , 110 , 110 , 24 ) 0 conv2d_1 [ 0 ][ 0 ] __________________________________________________________________________________________________ conv2d_2 ( Conv2D ) ( None , 53 , 53 , 32 ) 19232 dropout [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_1 ( Dropout ) ( None , 53 , 53 , 32 ) 0 conv2d_2 [ 0 ][ 0 ] __________________________________________________________________________________________________ conv2d_3 ( Conv2D ) ( None , 25 , 25 , 64 ) 51264 dropout_1 [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_2 ( Dropout ) ( None , 25 , 25 , 64 ) 0 conv2d_3 [ 0 ][ 0 ] __________________________________________________________________________________________________ conv2d_4 ( Conv2D ) ( None , 23 , 23 , 64 ) 36928 dropout_2 [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_3 ( Dropout ) ( None , 23 , 23 , 64 ) 0 conv2d_4 [ 0 ][ 0 ] __________________________________________________________________________________________________ conv2d_5 ( Conv2D ) ( None , 21 , 21 , 64 ) 36928 dropout_3 [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_4 ( Dropout ) ( None , 21 , 21 , 64 ) 0 conv2d_5 [ 0 ][ 0 ] __________________________________________________________________________________________________ flattened ( Flatten ) ( None , 28224 ) 0 dropout_4 [ 0 ][ 0 ] __________________________________________________________________________________________________ dense_1 ( Dense ) ( None , 100 ) 2822500 flattened [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_5 ( Dropout ) ( None , 100 ) 0 dense_1 [ 0 ][ 0 ] __________________________________________________________________________________________________ dense_2 ( Dense ) ( None , 50 ) 5050 dropout_5 [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_6 ( Dropout ) ( None , 50 ) 0 dense_2 [ 0 ][ 0 ] __________________________________________________________________________________________________ n_outputs0 ( Dense ) ( None , 1 ) 51 dropout_6 [ 0 ][ 0 ] __________________________________________________________________________________________________ n_outputs1 ( Dense ) ( None , 1 ) 51 dropout_6 [ 0 ][ 0 ] ================================================================================================== Total params : 2 , 973 , 828 Trainable params : 2 , 973 , 828 Non - trainable params : 0 __________________________________________________________________________________________________ None Using catalog / home / arl / mycar / data / dans - basement / catalog_7 . catalog Records # Training 3364 Records # Validation 842 Epoch 1 / 100 2021 - 07 - 26 21 : 05 : 35.291438 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcublas . so . 10 2021 - 07 - 26 21 : 05 : 35.613762 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcudnn . so . 7 2021 - 07 - 26 21 : 05 : 36.322576 : W tensorflow / stream_executor / gpu / asm_compiler . cc : 116 ] *** WARNING *** You are using ptxas 9.1 . 108 , which is older than 9.2 . 88. ptxas 9. x before 9.2 . 88 is known to miscompile XLA code , leading to incorrect results or invalid - address errors . You do not need to update to CUDA 9.2 . 88 ; cherry - picking the ptxas binary is sufficient . 2021 - 07 - 26 21 : 05 : 36.376195 : W tensorflow / stream_executor / gpu / redzone_allocator . cc : 314 ] Internal : ptxas exited with non - zero error code 65280 , output : ptxas fatal : Value 'sm_75' is not defined for option 'gpu-name' Relying on driver to perform ptx compilation . Modify $ PATH to customize ptxas location . This message will be only logged once . 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.2495 - n_outputs0_loss : 0.1717 - n_outputs1_loss : 0.0778 Epoch 00001 : val_loss improved from inf to 0.14744 , saving model to ./ models / dans - basement . h5 27 / 27 [ ============================== ] - 8 s 301 ms / step - loss : 0.2495 - n_outputs0_loss : 0.1717 - n_outputs1_loss : 0.0778 - val_loss : 0.1474 - val_n_outputs0_loss : 0.1291 - val_n_outputs1_loss : 0.0183 Epoch 2 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.1487 - n_outputs0_loss : 0.1265 - n_outputs1_loss : 0.0223 Epoch 00002 : val_loss improved from 0.14744 to 0.09815 , saving model to ./ models / dans - basement . h5 27 / 27 [ ============================== ] - 3 s 120 ms / step - loss : 0.1487 - n_outputs0_loss : 0.1265 - n_outputs1_loss : 0.0223 - val_loss : 0.0981 - val_n_outputs0_loss : 0.0777 - val_n_outputs1_loss : 0.0205 Epoch 3 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.1075 - n_outputs0_loss : 0.0893 - n_outputs1_loss : 0.0182 Epoch 00003 : val_loss improved from 0.09815 to 0.07897 , saving model to ./ models / dans - basement . h5 27 / 27 [ ============================== ] - 3 s 117 ms / step - loss : 0.1075 - n_outputs0_loss : 0.0893 - n_outputs1_loss : 0.0182 - val_loss : 0.0790 - val_n_outputs0_loss : 0.0687 - val_n_outputs1_loss : 0.0102 Epoch 4 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0917 - n_outputs0_loss : 0.0759 - n_outputs1_loss : 0.0158 Epoch 00004 : val_loss improved from 0.07897 to 0.07055 , saving model to ./ models / dans - basement . h5 27 / 27 [ ============================== ] - 3 s 110 ms / step - loss : 0.0917 - n_outputs0_loss : 0.0759 - n_outputs1_loss : 0.0158 - val_loss : 0.0705 - val_n_outputs0_loss : 0.0610 - val_n_outputs1_loss : 0.0096 Epoch 5 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0880 - n_outputs0_loss : 0.0734 - n_outputs1_loss : 0.0146 Epoch 00005 : val_loss did not improve from 0.07055 27 / 27 [ ============================== ] - 3 s 105 ms / step - loss : 0.0880 - n_outputs0_loss : 0.0734 - n_outputs1_loss : 0.0146 - val_loss : 0.0751 - val_n_outputs0_loss : 0.0553 - val_n_outputs1_loss : 0.0198 Epoch 6 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0757 - n_outputs0_loss : 0.0629 - n_outputs1_loss : 0.0127 Epoch 00006 : val_loss improved from 0.07055 to 0.05840 , saving model to ./ models / dans - basement . h5 27 / 27 [ ============================== ] - 3 s 111 ms / step - loss : 0.0757 - n_outputs0_loss : 0.0629 - n_outputs1_loss : 0.0127 - val_loss : 0.0584 - val_n_outputs0_loss : 0.0485 - val_n_outputs1_loss : 0.0099 Epoch 7 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0672 - n_outputs0_loss : 0.0551 - n_outputs1_loss : 0.0120 Epoch 00007 : val_loss improved from 0.05840 to 0.05028 , saving model to ./ models / dans - basement . h5 27 / 27 [ ============================== ] - 3 s 110 ms / step - loss : 0.0672 - n_outputs0_loss : 0.0551 - n_outputs1_loss : 0.0120 - val_loss : 0.0503 - val_n_outputs0_loss : 0.0450 - val_n_outputs1_loss : 0.0053 Epoch 8 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0621 - n_outputs0_loss : 0.0510 - n_outputs1_loss : 0.0111 Epoch 00008 : val_loss improved from 0.05028 to 0.04540 , saving model to ./ models / dans - basement . h5 27 / 27 [ ============================== ] - 3 s 110 ms / step - loss : 0.0621 - n_outputs0_loss : 0.0510 - n_outputs1_loss : 0.0111 - val_loss : 0.0454 - val_n_outputs0_loss : 0.0385 - val_n_outputs1_loss : 0.0069 Epoch 9 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0545 - n_outputs0_loss : 0.0441 - n_outputs1_loss : 0.0104 Epoch 00009 : val_loss improved from 0.04540 to 0.04351 , saving model to ./ models / dans - basement . h5 27 / 27 [ ============================== ] - 3 s 107 ms / step - loss : 0.0545 - n_outputs0_loss : 0.0441 - n_outputs1_loss : 0.0104 - val_loss : 0.0435 - val_n_outputs0_loss : 0.0358 - val_n_outputs1_loss : 0.0077 Epoch 10 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0558 - n_outputs0_loss : 0.0458 - n_outputs1_loss : 0.0099 Epoch 00010 : val_loss improved from 0.04351 to 0.04070 , saving model to ./ models / dans - basement . h5 27 / 27 [ ============================== ] - 3 s 110 ms / step - loss : 0.0558 - n_outputs0_loss : 0.0458 - n_outputs1_loss : 0.0099 - val_loss : 0.0407 - val_n_outputs0_loss : 0.0357 - val_n_outputs1_loss : 0.0050 Epoch 11 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0505 - n_outputs0_loss : 0.0415 - n_outputs1_loss : 0.0090 Epoch 00011 : val_loss improved from 0.04070 to 0.03935 , saving model to ./ models / dans - basement . h5 27 / 27 [ ============================== ] - 3 s 109 ms / step - loss : 0.0505 - n_outputs0_loss : 0.0415 - n_outputs1_loss : 0.0090 - val_loss : 0.0393 - val_n_outputs0_loss : 0.0340 - val_n_outputs1_loss : 0.0054 Epoch 12 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0476 - n_outputs0_loss : 0.0388 - n_outputs1_loss : 0.0088 Epoch 00012 : val_loss improved from 0.03935 to 0.03624 , saving model to ./ models / dans - basement . h5 27 / 27 [ ============================== ] - 3 s 110 ms / step - loss : 0.0476 - n_outputs0_loss : 0.0388 - n_outputs1_loss : 0.0088 - val_loss : 0.0362 - val_n_outputs0_loss : 0.0298 - val_n_outputs1_loss : 0.0065 Epoch 13 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0453 - n_outputs0_loss : 0.0373 - n_outputs1_loss : 0.0080 Epoch 00013 : val_loss improved from 0.03624 to 0.03507 , saving model to ./ models / dans - basement . h5 27 / 27 [ ============================== ] - 3 s 108 ms / step - loss : 0.0453 - n_outputs0_loss : 0.0373 - n_outputs1_loss : 0.0080 - val_loss : 0.0351 - val_n_outputs0_loss : 0.0294 - val_n_outputs1_loss : 0.0057 Epoch 14 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0430 - n_outputs0_loss : 0.0352 - n_outputs1_loss : 0.0079 Epoch 00014 : val_loss improved from 0.03507 to 0.03211 , saving model to ./ models / dans - basement . h5 27 / 27 [ ============================== ] - 3 s 111 ms / step - loss : 0.0430 - n_outputs0_loss : 0.0352 - n_outputs1_loss : 0.0079 - val_loss : 0.0321 - val_n_outputs0_loss : 0.0265 - val_n_outputs1_loss : 0.0056 Epoch 15 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0397 - n_outputs0_loss : 0.0327 - n_outputs1_loss : 0.0070 Epoch 00015 : val_loss improved from 0.03211 to 0.03208 , saving model to ./ models / dans - basement . h5 27 / 27 [ ============================== ] - 3 s 110 ms / step - loss : 0.0397 - n_outputs0_loss : 0.0327 - n_outputs1_loss : 0.0070 - val_loss : 0.0321 - val_n_outputs0_loss : 0.0279 - val_n_outputs1_loss : 0.0042 Epoch 16 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0382 - n_outputs0_loss : 0.0316 - n_outputs1_loss : 0.0065 Epoch 00016 : val_loss improved from 0.03208 to 0.02880 , saving model to ./ models / dans - basement . h5 27 / 27 [ ============================== ] - 3 s 108 ms / step - loss : 0.0382 - n_outputs0_loss : 0.0316 - n_outputs1_loss : 0.0065 - val_loss : 0.0288 - val_n_outputs0_loss : 0.0243 - val_n_outputs1_loss : 0.0046 Epoch 17 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0381 - n_outputs0_loss : 0.0313 - n_outputs1_loss : 0.0069 Epoch 00017 : val_loss did not improve from 0.02880 27 / 27 [ ============================== ] - 3 s 104 ms / step - loss : 0.0381 - n_outputs0_loss : 0.0313 - n_outputs1_loss : 0.0069 - val_loss : 0.0322 - val_n_outputs0_loss : 0.0281 - val_n_outputs1_loss : 0.0041 Epoch 18 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0375 - n_outputs0_loss : 0.0310 - n_outputs1_loss : 0.0065 Epoch 00018 : val_loss did not improve from 0.02880 27 / 27 [ ============================== ] - 3 s 107 ms / step - loss : 0.0375 - n_outputs0_loss : 0.0310 - n_outputs1_loss : 0.0065 - val_loss : 0.0293 - val_n_outputs0_loss : 0.0257 - val_n_outputs1_loss : 0.0036 Epoch 19 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0372 - n_outputs0_loss : 0.0308 - n_outputs1_loss : 0.0064 Epoch 00019 : val_loss did not improve from 0.02880 27 / 27 [ ============================== ] - 3 s 108 ms / step - loss : 0.0372 - n_outputs0_loss : 0.0308 - n_outputs1_loss : 0.0064 - val_loss : 0.0307 - val_n_outputs0_loss : 0.0275 - val_n_outputs1_loss : 0.0032 Epoch 20 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0347 - n_outputs0_loss : 0.0285 - n_outputs1_loss : 0.0062 Epoch 00020 : val_loss did not improve from 0.02880 27 / 27 [ ============================== ] - 3 s 104 ms / step - loss : 0.0347 - n_outputs0_loss : 0.0285 - n_outputs1_loss : 0.0062 - val_loss : 0.0325 - val_n_outputs0_loss : 0.0283 - val_n_outputs1_loss : 0.0042 Epoch 21 / 100 27 / 27 [ ============================== ] - ETA : 0 s - loss : 0.0349 - n_outputs0_loss : 0.0290 - n_outputs1_loss : 0.0058 Epoch 00021 : val_loss did not improve from 0.02880 27 / 27 [ ============================== ] - 3 s 107 ms / step - loss : 0.0349 - n_outputs0_loss : 0.0290 - n_outputs1_loss : 0.0058 - val_loss : 0.0293 - val_n_outputs0_loss : 0.0258 - val_n_outputs1_loss : 0.0035 WARNING : CPU random generator seem to be failing , disable hardware random number generation WARNING : RDRND generated : 0xffffffff 0xffffffff 0xffffffff 0xffffffff real 1 m26 . 930 s user 1 m30 . 911 s sys 0 m42 . 818 s","title":"Catalogs"},{"location":"training-logs/msp-1-cpu/","text":"Training Log for MSP Car #1 After cleanup we only got about 1,500 records. But here is a log of the training. It took about 1.5 minutes. 1 $ donkey train --tub = ./data/msp-car-1 --model = ./models/msp-car-1.f5 _ ______ _________ _ _ _ _ / _ _/ __ _ _ / / / _ _ // / _ _ / / / _ / _ `/ / _ / / // / / / / / / ,< / / / / / / / / / / / / / _ / _ // / / // /| | ___/ _ , / ____/ _ , / / / /____/ using donkey v4.2.1 ... loading config file: ./config.py loading personal config over-rides from myconfig.py \"get_model_by_type\" model Type is: linear Created KerasLinear 2021-07-26 19:50:45.562205: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA 2021-07-26 19:50:45.565106: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3592950000 Hz 2021-07-26 19:50:45.565470: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d85e9d19f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices: 2021-07-26 19:50:45.565492: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version 2021-07-26 19:50:45.565578: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. Model: \"model\" Layer (type) Output Shape Param # Connected to img_in (InputLayer) [(None, 224, 224, 3) 0 conv2d_1 (Conv2D) (None, 110, 110, 24) 1824 img_in[0][0] dropout (Dropout) (None, 110, 110, 24) 0 conv2d_1[0][0] conv2d_2 (Conv2D) (None, 53, 53, 32) 19232 dropout[0][0] dropout_1 (Dropout) (None, 53, 53, 32) 0 conv2d_2[0][0] conv2d_3 (Conv2D) (None, 25, 25, 64) 51264 dropout_1[0][0] dropout_2 (Dropout) (None, 25, 25, 64) 0 conv2d_3[0][0] conv2d_4 (Conv2D) (None, 23, 23, 64) 36928 dropout_2[0][0] dropout_3 (Dropout) (None, 23, 23, 64) 0 conv2d_4[0][0] conv2d_5 (Conv2D) (None, 21, 21, 64) 36928 dropout_3[0][0] dropout_4 (Dropout) (None, 21, 21, 64) 0 conv2d_5[0][0] flattened (Flatten) (None, 28224) 0 dropout_4[0][0] dense_1 (Dense) (None, 100) 2822500 flattened[0][0] dropout_5 (Dropout) (None, 100) 0 dense_1[0][0] dense_2 (Dense) (None, 50) 5050 dropout_5[0][0] dropout_6 (Dropout) (None, 50) 0 dense_2[0][0] n_outputs0 (Dense) (None, 1) 51 dropout_6[0][0] n_outputs1 (Dense) (None, 1) 51 dropout_6[0][0] Total params: 2,973,828 Trainable params: 2,973,828 Non-trainable params: 0 None Using catalog /home/arl/mycar/data/msp-car-1/catalog_17.catalog Records # Training 1265 Records # Validation 317 Epoch 1/100 10/10 [==============================] - ETA: 0s - loss: 1.0885 - n_outputs0_loss: 0.5975 - n_outputs1_loss: 0.4909 Epoch 00001: val_loss improved from inf to 0.54341, saving model to ./models/msp-car-1.f5 2021-07-26 19:50:57.881390: W tensorflow/python/util/util.cc:329] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them. WARNING:tensorflow:From /home/arl/miniconda3/envs/donkey/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable. init (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating: If using Keras pass *_constraint arguments to layers. 10/10 [==============================] - 11s 1s/step - loss: 1.0885 - n_outputs0_loss: 0.5975 - n_outputs1_loss: 0.4909 - val_loss: 0.5434 - val_n_outputs0_loss: 0.4668 - val_n_outputs1_loss: 0.0767 Epoch 2/100 10/10 [==============================] - ETA: 0s - loss: 0.5522 - n_outputs0_loss: 0.4640 - n_outputs1_loss: 0.0882 Epoch 00002: val_loss improved from 0.54341 to 0.53272, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 999ms/step - loss: 0.5522 - n_outputs0_loss: 0.4640 - n_outputs1_loss: 0.0882 - val_loss: 0.5327 - val_n_outputs0_loss: 0.4605 - val_n_outputs1_loss: 0.0722 Epoch 3/100 10/10 [==============================] - ETA: 0s - loss: 0.5392 - n_outputs0_loss: 0.4638 - n_outputs1_loss: 0.0754 Epoch 00003: val_loss improved from 0.53272 to 0.50775, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.5392 - n_outputs0_loss: 0.4638 - n_outputs1_loss: 0.0754 - val_loss: 0.5077 - val_n_outputs0_loss: 0.4551 - val_n_outputs1_loss: 0.0527 Epoch 4/100 10/10 [==============================] - ETA: 0s - loss: 0.5318 - n_outputs0_loss: 0.4605 - n_outputs1_loss: 0.0713 Epoch 00004: val_loss improved from 0.50775 to 0.49783, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 999ms/step - loss: 0.5318 - n_outputs0_loss: 0.4605 - n_outputs1_loss: 0.0713 - val_loss: 0.4978 - val_n_outputs0_loss: 0.4455 - val_n_outputs1_loss: 0.0523 Epoch 5/100 10/10 [==============================] - ETA: 0s - loss: 0.5333 - n_outputs0_loss: 0.4608 - n_outputs1_loss: 0.0725 Epoch 00005: val_loss improved from 0.49783 to 0.49721, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.5333 - n_outputs0_loss: 0.4608 - n_outputs1_loss: 0.0725 - val_loss: 0.4972 - val_n_outputs0_loss: 0.4451 - val_n_outputs1_loss: 0.0521 Epoch 6/100 10/10 [==============================] - ETA: 0s - loss: 0.5277 - n_outputs0_loss: 0.4619 - n_outputs1_loss: 0.0658 Epoch 00006: val_loss did not improve from 0.49721 10/10 [==============================] - 9s 934ms/step - loss: 0.5277 - n_outputs0_loss: 0.4619 - n_outputs1_loss: 0.0658 - val_loss: 0.4981 - val_n_outputs0_loss: 0.4461 - val_n_outputs1_loss: 0.0520 Epoch 7/100 10/10 [==============================] - ETA: 0s - loss: 0.5265 - n_outputs0_loss: 0.4577 - n_outputs1_loss: 0.0688 Epoch 00007: val_loss improved from 0.49721 to 0.49668, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.5265 - n_outputs0_loss: 0.4577 - n_outputs1_loss: 0.0688 - val_loss: 0.4967 - val_n_outputs0_loss: 0.4442 - val_n_outputs1_loss: 0.0525 Epoch 8/100 10/10 [==============================] - ETA: 0s - loss: 0.5138 - n_outputs0_loss: 0.4467 - n_outputs1_loss: 0.0671 Epoch 00008: val_loss improved from 0.49668 to 0.49536, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.5138 - n_outputs0_loss: 0.4467 - n_outputs1_loss: 0.0671 - val_loss: 0.4954 - val_n_outputs0_loss: 0.4408 - val_n_outputs1_loss: 0.0546 Epoch 9/100 10/10 [==============================] - ETA: 0s - loss: 0.5109 - n_outputs0_loss: 0.4468 - n_outputs1_loss: 0.0642 Epoch 00009: val_loss improved from 0.49536 to 0.48741, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.5109 - n_outputs0_loss: 0.4468 - n_outputs1_loss: 0.0642 - val_loss: 0.4874 - val_n_outputs0_loss: 0.4353 - val_n_outputs1_loss: 0.0521 Epoch 10/100 10/10 [==============================] - ETA: 0s - loss: 0.5030 - n_outputs0_loss: 0.4405 - n_outputs1_loss: 0.0625 Epoch 00010: val_loss did not improve from 0.48741 10/10 [==============================] - 9s 930ms/step - loss: 0.5030 - n_outputs0_loss: 0.4405 - n_outputs1_loss: 0.0625 - val_loss: 0.4936 - val_n_outputs0_loss: 0.4351 - val_n_outputs1_loss: 0.0585 Epoch 11/100 10/10 [==============================] - ETA: 0s - loss: 0.4974 - n_outputs0_loss: 0.4310 - n_outputs1_loss: 0.0664 Epoch 00011: val_loss improved from 0.48741 to 0.47748, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 11s 1s/step - loss: 0.4974 - n_outputs0_loss: 0.4310 - n_outputs1_loss: 0.0664 - val_loss: 0.4775 - val_n_outputs0_loss: 0.4238 - val_n_outputs1_loss: 0.0536 Epoch 12/100 10/10 [==============================] - ETA: 0s - loss: 0.4887 - n_outputs0_loss: 0.4208 - n_outputs1_loss: 0.0679 Epoch 00012: val_loss did not improve from 0.47748 10/10 [==============================] - 9s 925ms/step - loss: 0.4887 - n_outputs0_loss: 0.4208 - n_outputs1_loss: 0.0679 - val_loss: 0.4836 - val_n_outputs0_loss: 0.4148 - val_n_outputs1_loss: 0.0687 Epoch 13/100 10/10 [==============================] - ETA: 0s - loss: 0.4591 - n_outputs0_loss: 0.3927 - n_outputs1_loss: 0.0664 Epoch 00013: val_loss improved from 0.47748 to 0.40567, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.4591 - n_outputs0_loss: 0.3927 - n_outputs1_loss: 0.0664 - val_loss: 0.4057 - val_n_outputs0_loss: 0.3540 - val_n_outputs1_loss: 0.0516 Epoch 14/100 10/10 [==============================] - ETA: 0s - loss: 0.4323 - n_outputs0_loss: 0.3665 - n_outputs1_loss: 0.0658 Epoch 00014: val_loss improved from 0.40567 to 0.37099, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.4323 - n_outputs0_loss: 0.3665 - n_outputs1_loss: 0.0658 - val_loss: 0.3710 - val_n_outputs0_loss: 0.3153 - val_n_outputs1_loss: 0.0556 Epoch 15/100 10/10 [==============================] - ETA: 0s - loss: 0.3754 - n_outputs0_loss: 0.3063 - n_outputs1_loss: 0.0691 Epoch 00015: val_loss improved from 0.37099 to 0.33956, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.3754 - n_outputs0_loss: 0.3063 - n_outputs1_loss: 0.0691 - val_loss: 0.3396 - val_n_outputs0_loss: 0.2853 - val_n_outputs1_loss: 0.0542 Epoch 16/100 10/10 [==============================] - ETA: 0s - loss: 0.3314 - n_outputs0_loss: 0.2723 - n_outputs1_loss: 0.0591 Epoch 00016: val_loss improved from 0.33956 to 0.30289, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.3314 - n_outputs0_loss: 0.2723 - n_outputs1_loss: 0.0591 - val_loss: 0.3029 - val_n_outputs0_loss: 0.2524 - val_n_outputs1_loss: 0.0505 Epoch 17/100 10/10 [==============================] - ETA: 0s - loss: 0.3168 - n_outputs0_loss: 0.2591 - n_outputs1_loss: 0.0576 Epoch 00017: val_loss improved from 0.30289 to 0.28694, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.3168 - n_outputs0_loss: 0.2591 - n_outputs1_loss: 0.0576 - val_loss: 0.2869 - val_n_outputs0_loss: 0.2390 - val_n_outputs1_loss: 0.0479 Epoch 18/100 10/10 [==============================] - ETA: 0s - loss: 0.2990 - n_outputs0_loss: 0.2446 - n_outputs1_loss: 0.0544 Epoch 00018: val_loss improved from 0.28694 to 0.27270, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.2990 - n_outputs0_loss: 0.2446 - n_outputs1_loss: 0.0544 - val_loss: 0.2727 - val_n_outputs0_loss: 0.2257 - val_n_outputs1_loss: 0.0470 Epoch 19/100 10/10 [==============================] - ETA: 0s - loss: 0.2706 - n_outputs0_loss: 0.2185 - n_outputs1_loss: 0.0521 Epoch 00019: val_loss improved from 0.27270 to 0.25193, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.2706 - n_outputs0_loss: 0.2185 - n_outputs1_loss: 0.0521 - val_loss: 0.2519 - val_n_outputs0_loss: 0.2099 - val_n_outputs1_loss: 0.0421 Epoch 20/100 10/10 [==============================] - ETA: 0s - loss: 0.2602 - n_outputs0_loss: 0.2112 - n_outputs1_loss: 0.0490 Epoch 00020: val_loss improved from 0.25193 to 0.23899, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.2602 - n_outputs0_loss: 0.2112 - n_outputs1_loss: 0.0490 - val_loss: 0.2390 - val_n_outputs0_loss: 0.1974 - val_n_outputs1_loss: 0.0416 Epoch 21/100 10/10 [==============================] - ETA: 0s - loss: 0.2345 - n_outputs0_loss: 0.1866 - n_outputs1_loss: 0.0479 Epoch 00021: val_loss improved from 0.23899 to 0.23396, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.2345 - n_outputs0_loss: 0.1866 - n_outputs1_loss: 0.0479 - val_loss: 0.2340 - val_n_outputs0_loss: 0.1911 - val_n_outputs1_loss: 0.0428 Epoch 22/100 10/10 [==============================] - ETA: 0s - loss: 0.2229 - n_outputs0_loss: 0.1758 - n_outputs1_loss: 0.0471 Epoch 00022: val_loss improved from 0.23396 to 0.22651, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.2229 - n_outputs0_loss: 0.1758 - n_outputs1_loss: 0.0471 - val_loss: 0.2265 - val_n_outputs0_loss: 0.1858 - val_n_outputs1_loss: 0.0407 Epoch 23/100 10/10 [==============================] - ETA: 0s - loss: 0.2175 - n_outputs0_loss: 0.1730 - n_outputs1_loss: 0.0445 Epoch 00023: val_loss improved from 0.22651 to 0.22245, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.2175 - n_outputs0_loss: 0.1730 - n_outputs1_loss: 0.0445 - val_loss: 0.2225 - val_n_outputs0_loss: 0.1806 - val_n_outputs1_loss: 0.0419 Epoch 24/100 10/10 [==============================] - ETA: 0s - loss: 0.2084 - n_outputs0_loss: 0.1624 - n_outputs1_loss: 0.0460 Epoch 00024: val_loss improved from 0.22245 to 0.20674, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.2084 - n_outputs0_loss: 0.1624 - n_outputs1_loss: 0.0460 - val_loss: 0.2067 - val_n_outputs0_loss: 0.1694 - val_n_outputs1_loss: 0.0374 Epoch 25/100 10/10 [==============================] - ETA: 0s - loss: 0.1889 - n_outputs0_loss: 0.1457 - n_outputs1_loss: 0.0432 Epoch 00025: val_loss improved from 0.20674 to 0.20416, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.1889 - n_outputs0_loss: 0.1457 - n_outputs1_loss: 0.0432 - val_loss: 0.2042 - val_n_outputs0_loss: 0.1638 - val_n_outputs1_loss: 0.0403 Epoch 26/100 10/10 [==============================] - ETA: 0s - loss: 0.1882 - n_outputs0_loss: 0.1467 - n_outputs1_loss: 0.0414 Epoch 00026: val_loss improved from 0.20416 to 0.19422, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.1882 - n_outputs0_loss: 0.1467 - n_outputs1_loss: 0.0414 - val_loss: 0.1942 - val_n_outputs0_loss: 0.1557 - val_n_outputs1_loss: 0.0385 Epoch 27/100 10/10 [==============================] - ETA: 0s - loss: 0.1706 - n_outputs0_loss: 0.1328 - n_outputs1_loss: 0.0378 Epoch 00027: val_loss did not improve from 0.19422 10/10 [==============================] - 9s 930ms/step - loss: 0.1706 - n_outputs0_loss: 0.1328 - n_outputs1_loss: 0.0378 - val_loss: 0.2016 - val_n_outputs0_loss: 0.1615 - val_n_outputs1_loss: 0.0401 Epoch 28/100 10/10 [==============================] - ETA: 0s - loss: 0.1630 - n_outputs0_loss: 0.1248 - n_outputs1_loss: 0.0382 Epoch 00028: val_loss improved from 0.19422 to 0.18035, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.1630 - n_outputs0_loss: 0.1248 - n_outputs1_loss: 0.0382 - val_loss: 0.1803 - val_n_outputs0_loss: 0.1445 - val_n_outputs1_loss: 0.0358 Epoch 29/100 10/10 [==============================] - ETA: 0s - loss: 0.1601 - n_outputs0_loss: 0.1219 - n_outputs1_loss: 0.0382 Epoch 00029: val_loss improved from 0.18035 to 0.17528, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.1601 - n_outputs0_loss: 0.1219 - n_outputs1_loss: 0.0382 - val_loss: 0.1753 - val_n_outputs0_loss: 0.1410 - val_n_outputs1_loss: 0.0343 Epoch 30/100 10/10 [==============================] - ETA: 0s - loss: 0.1483 - n_outputs0_loss: 0.1117 - n_outputs1_loss: 0.0366 Epoch 00030: val_loss improved from 0.17528 to 0.17039, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.1483 - n_outputs0_loss: 0.1117 - n_outputs1_loss: 0.0366 - val_loss: 0.1704 - val_n_outputs0_loss: 0.1372 - val_n_outputs1_loss: 0.0332 Epoch 31/100 10/10 [==============================] - ETA: 0s - loss: 0.1481 - n_outputs0_loss: 0.1114 - n_outputs1_loss: 0.0368 Epoch 00031: val_loss did not improve from 0.17039 10/10 [==============================] - 9s 915ms/step - loss: 0.1481 - n_outputs0_loss: 0.1114 - n_outputs1_loss: 0.0368 - val_loss: 0.1783 - val_n_outputs0_loss: 0.1436 - val_n_outputs1_loss: 0.0347 Epoch 32/100 10/10 [==============================] - ETA: 0s - loss: 0.1470 - n_outputs0_loss: 0.1111 - n_outputs1_loss: 0.0358 Epoch 00032: val_loss improved from 0.17039 to 0.16278, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.1470 - n_outputs0_loss: 0.1111 - n_outputs1_loss: 0.0358 - val_loss: 0.1628 - val_n_outputs0_loss: 0.1301 - val_n_outputs1_loss: 0.0327 Epoch 33/100 10/10 [==============================] - ETA: 0s - loss: 0.1368 - n_outputs0_loss: 0.1027 - n_outputs1_loss: 0.0341 Epoch 00033: val_loss did not improve from 0.16278 10/10 [==============================] - 9s 928ms/step - loss: 0.1368 - n_outputs0_loss: 0.1027 - n_outputs1_loss: 0.0341 - val_loss: 0.1666 - val_n_outputs0_loss: 0.1345 - val_n_outputs1_loss: 0.0320 Epoch 34/100 10/10 [==============================] - ETA: 0s - loss: 0.1305 - n_outputs0_loss: 0.0971 - n_outputs1_loss: 0.0334 Epoch 00034: val_loss did not improve from 0.16278 10/10 [==============================] - 9s 929ms/step - loss: 0.1305 - n_outputs0_loss: 0.0971 - n_outputs1_loss: 0.0334 - val_loss: 0.1728 - val_n_outputs0_loss: 0.1413 - val_n_outputs1_loss: 0.0315 Epoch 35/100 10/10 [==============================] - ETA: 0s - loss: 0.1353 - n_outputs0_loss: 0.1027 - n_outputs1_loss: 0.0326 Epoch 00035: val_loss did not improve from 0.16278 10/10 [==============================] - 9s 933ms/step - loss: 0.1353 - n_outputs0_loss: 0.1027 - n_outputs1_loss: 0.0326 - val_loss: 0.1706 - val_n_outputs0_loss: 0.1391 - val_n_outputs1_loss: 0.0315 Epoch 36/100 10/10 [==============================] - ETA: 0s - loss: 0.1319 - n_outputs0_loss: 0.0989 - n_outputs1_loss: 0.0331 Epoch 00036: val_loss did not improve from 0.16278 10/10 [==============================] - 9s 936ms/step - loss: 0.1319 - n_outputs0_loss: 0.0989 - n_outputs1_loss: 0.0331 - val_loss: 0.1729 - val_n_outputs0_loss: 0.1401 - val_n_outputs1_loss: 0.0328 Epoch 37/100 10/10 [==============================] - ETA: 0s - loss: 0.1290 - n_outputs0_loss: 0.0952 - n_outputs1_loss: 0.0338 Epoch 00037: val_loss did not improve from 0.16278 10/10 [==============================] - 9s 929ms/step - loss: 0.1290 - n_outputs0_loss: 0.0952 - n_outputs1_loss: 0.0338 - val_loss: 0.1709 - val_n_outputs0_loss: 0.1381 - val_n_outputs1_loss: 0.0327 WARNING: CPU random generator seem to be failing, disable hardware random number generation WARNING: RDRND generated: 0xffffffff 0xffffffff 0xffffffff 0xffffffff (donkey) arl@arl1: ```","title":"Training Log for MSP Car #1"},{"location":"training-logs/msp-1-cpu/#training-log-for-msp-car-1","text":"After cleanup we only got about 1,500 records. But here is a log of the training. It took about 1.5 minutes. 1 $ donkey train --tub = ./data/msp-car-1 --model = ./models/msp-car-1.f5 _ ______ _________ _ _ _ _ / _ _/ __ _ _ / / / _ _ // / _ _ / / / _ / _ `/ / _ / / // / / / / / / ,< / / / / / / / / / / / / / _ / _ // / / // /| | ___/ _ , / ____/ _ , / / / /____/ using donkey v4.2.1 ... loading config file: ./config.py loading personal config over-rides from myconfig.py \"get_model_by_type\" model Type is: linear Created KerasLinear 2021-07-26 19:50:45.562205: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA 2021-07-26 19:50:45.565106: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3592950000 Hz 2021-07-26 19:50:45.565470: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d85e9d19f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices: 2021-07-26 19:50:45.565492: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version 2021-07-26 19:50:45.565578: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. Model: \"model\"","title":"Training Log for MSP Car #1"},{"location":"training-logs/msp-1-cpu/#layer-type-output-shape-param-connected-to","text":"img_in (InputLayer) [(None, 224, 224, 3) 0 conv2d_1 (Conv2D) (None, 110, 110, 24) 1824 img_in[0][0] dropout (Dropout) (None, 110, 110, 24) 0 conv2d_1[0][0] conv2d_2 (Conv2D) (None, 53, 53, 32) 19232 dropout[0][0] dropout_1 (Dropout) (None, 53, 53, 32) 0 conv2d_2[0][0] conv2d_3 (Conv2D) (None, 25, 25, 64) 51264 dropout_1[0][0] dropout_2 (Dropout) (None, 25, 25, 64) 0 conv2d_3[0][0] conv2d_4 (Conv2D) (None, 23, 23, 64) 36928 dropout_2[0][0] dropout_3 (Dropout) (None, 23, 23, 64) 0 conv2d_4[0][0] conv2d_5 (Conv2D) (None, 21, 21, 64) 36928 dropout_3[0][0] dropout_4 (Dropout) (None, 21, 21, 64) 0 conv2d_5[0][0] flattened (Flatten) (None, 28224) 0 dropout_4[0][0] dense_1 (Dense) (None, 100) 2822500 flattened[0][0] dropout_5 (Dropout) (None, 100) 0 dense_1[0][0] dense_2 (Dense) (None, 50) 5050 dropout_5[0][0] dropout_6 (Dropout) (None, 50) 0 dense_2[0][0] n_outputs0 (Dense) (None, 1) 51 dropout_6[0][0]","title":"Layer (type)                    Output Shape         Param #     Connected to"},{"location":"training-logs/msp-1-cpu/#n_outputs1-dense-none-1-51-dropout_600","text":"Total params: 2,973,828 Trainable params: 2,973,828 Non-trainable params: 0 None Using catalog /home/arl/mycar/data/msp-car-1/catalog_17.catalog Records # Training 1265 Records # Validation 317 Epoch 1/100 10/10 [==============================] - ETA: 0s - loss: 1.0885 - n_outputs0_loss: 0.5975 - n_outputs1_loss: 0.4909 Epoch 00001: val_loss improved from inf to 0.54341, saving model to ./models/msp-car-1.f5 2021-07-26 19:50:57.881390: W tensorflow/python/util/util.cc:329] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them. WARNING:tensorflow:From /home/arl/miniconda3/envs/donkey/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable. init (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating: If using Keras pass *_constraint arguments to layers. 10/10 [==============================] - 11s 1s/step - loss: 1.0885 - n_outputs0_loss: 0.5975 - n_outputs1_loss: 0.4909 - val_loss: 0.5434 - val_n_outputs0_loss: 0.4668 - val_n_outputs1_loss: 0.0767 Epoch 2/100 10/10 [==============================] - ETA: 0s - loss: 0.5522 - n_outputs0_loss: 0.4640 - n_outputs1_loss: 0.0882 Epoch 00002: val_loss improved from 0.54341 to 0.53272, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 999ms/step - loss: 0.5522 - n_outputs0_loss: 0.4640 - n_outputs1_loss: 0.0882 - val_loss: 0.5327 - val_n_outputs0_loss: 0.4605 - val_n_outputs1_loss: 0.0722 Epoch 3/100 10/10 [==============================] - ETA: 0s - loss: 0.5392 - n_outputs0_loss: 0.4638 - n_outputs1_loss: 0.0754 Epoch 00003: val_loss improved from 0.53272 to 0.50775, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.5392 - n_outputs0_loss: 0.4638 - n_outputs1_loss: 0.0754 - val_loss: 0.5077 - val_n_outputs0_loss: 0.4551 - val_n_outputs1_loss: 0.0527 Epoch 4/100 10/10 [==============================] - ETA: 0s - loss: 0.5318 - n_outputs0_loss: 0.4605 - n_outputs1_loss: 0.0713 Epoch 00004: val_loss improved from 0.50775 to 0.49783, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 999ms/step - loss: 0.5318 - n_outputs0_loss: 0.4605 - n_outputs1_loss: 0.0713 - val_loss: 0.4978 - val_n_outputs0_loss: 0.4455 - val_n_outputs1_loss: 0.0523 Epoch 5/100 10/10 [==============================] - ETA: 0s - loss: 0.5333 - n_outputs0_loss: 0.4608 - n_outputs1_loss: 0.0725 Epoch 00005: val_loss improved from 0.49783 to 0.49721, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.5333 - n_outputs0_loss: 0.4608 - n_outputs1_loss: 0.0725 - val_loss: 0.4972 - val_n_outputs0_loss: 0.4451 - val_n_outputs1_loss: 0.0521 Epoch 6/100 10/10 [==============================] - ETA: 0s - loss: 0.5277 - n_outputs0_loss: 0.4619 - n_outputs1_loss: 0.0658 Epoch 00006: val_loss did not improve from 0.49721 10/10 [==============================] - 9s 934ms/step - loss: 0.5277 - n_outputs0_loss: 0.4619 - n_outputs1_loss: 0.0658 - val_loss: 0.4981 - val_n_outputs0_loss: 0.4461 - val_n_outputs1_loss: 0.0520 Epoch 7/100 10/10 [==============================] - ETA: 0s - loss: 0.5265 - n_outputs0_loss: 0.4577 - n_outputs1_loss: 0.0688 Epoch 00007: val_loss improved from 0.49721 to 0.49668, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.5265 - n_outputs0_loss: 0.4577 - n_outputs1_loss: 0.0688 - val_loss: 0.4967 - val_n_outputs0_loss: 0.4442 - val_n_outputs1_loss: 0.0525 Epoch 8/100 10/10 [==============================] - ETA: 0s - loss: 0.5138 - n_outputs0_loss: 0.4467 - n_outputs1_loss: 0.0671 Epoch 00008: val_loss improved from 0.49668 to 0.49536, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.5138 - n_outputs0_loss: 0.4467 - n_outputs1_loss: 0.0671 - val_loss: 0.4954 - val_n_outputs0_loss: 0.4408 - val_n_outputs1_loss: 0.0546 Epoch 9/100 10/10 [==============================] - ETA: 0s - loss: 0.5109 - n_outputs0_loss: 0.4468 - n_outputs1_loss: 0.0642 Epoch 00009: val_loss improved from 0.49536 to 0.48741, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.5109 - n_outputs0_loss: 0.4468 - n_outputs1_loss: 0.0642 - val_loss: 0.4874 - val_n_outputs0_loss: 0.4353 - val_n_outputs1_loss: 0.0521 Epoch 10/100 10/10 [==============================] - ETA: 0s - loss: 0.5030 - n_outputs0_loss: 0.4405 - n_outputs1_loss: 0.0625 Epoch 00010: val_loss did not improve from 0.48741 10/10 [==============================] - 9s 930ms/step - loss: 0.5030 - n_outputs0_loss: 0.4405 - n_outputs1_loss: 0.0625 - val_loss: 0.4936 - val_n_outputs0_loss: 0.4351 - val_n_outputs1_loss: 0.0585 Epoch 11/100 10/10 [==============================] - ETA: 0s - loss: 0.4974 - n_outputs0_loss: 0.4310 - n_outputs1_loss: 0.0664 Epoch 00011: val_loss improved from 0.48741 to 0.47748, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 11s 1s/step - loss: 0.4974 - n_outputs0_loss: 0.4310 - n_outputs1_loss: 0.0664 - val_loss: 0.4775 - val_n_outputs0_loss: 0.4238 - val_n_outputs1_loss: 0.0536 Epoch 12/100 10/10 [==============================] - ETA: 0s - loss: 0.4887 - n_outputs0_loss: 0.4208 - n_outputs1_loss: 0.0679 Epoch 00012: val_loss did not improve from 0.47748 10/10 [==============================] - 9s 925ms/step - loss: 0.4887 - n_outputs0_loss: 0.4208 - n_outputs1_loss: 0.0679 - val_loss: 0.4836 - val_n_outputs0_loss: 0.4148 - val_n_outputs1_loss: 0.0687 Epoch 13/100 10/10 [==============================] - ETA: 0s - loss: 0.4591 - n_outputs0_loss: 0.3927 - n_outputs1_loss: 0.0664 Epoch 00013: val_loss improved from 0.47748 to 0.40567, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.4591 - n_outputs0_loss: 0.3927 - n_outputs1_loss: 0.0664 - val_loss: 0.4057 - val_n_outputs0_loss: 0.3540 - val_n_outputs1_loss: 0.0516 Epoch 14/100 10/10 [==============================] - ETA: 0s - loss: 0.4323 - n_outputs0_loss: 0.3665 - n_outputs1_loss: 0.0658 Epoch 00014: val_loss improved from 0.40567 to 0.37099, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.4323 - n_outputs0_loss: 0.3665 - n_outputs1_loss: 0.0658 - val_loss: 0.3710 - val_n_outputs0_loss: 0.3153 - val_n_outputs1_loss: 0.0556 Epoch 15/100 10/10 [==============================] - ETA: 0s - loss: 0.3754 - n_outputs0_loss: 0.3063 - n_outputs1_loss: 0.0691 Epoch 00015: val_loss improved from 0.37099 to 0.33956, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.3754 - n_outputs0_loss: 0.3063 - n_outputs1_loss: 0.0691 - val_loss: 0.3396 - val_n_outputs0_loss: 0.2853 - val_n_outputs1_loss: 0.0542 Epoch 16/100 10/10 [==============================] - ETA: 0s - loss: 0.3314 - n_outputs0_loss: 0.2723 - n_outputs1_loss: 0.0591 Epoch 00016: val_loss improved from 0.33956 to 0.30289, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.3314 - n_outputs0_loss: 0.2723 - n_outputs1_loss: 0.0591 - val_loss: 0.3029 - val_n_outputs0_loss: 0.2524 - val_n_outputs1_loss: 0.0505 Epoch 17/100 10/10 [==============================] - ETA: 0s - loss: 0.3168 - n_outputs0_loss: 0.2591 - n_outputs1_loss: 0.0576 Epoch 00017: val_loss improved from 0.30289 to 0.28694, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.3168 - n_outputs0_loss: 0.2591 - n_outputs1_loss: 0.0576 - val_loss: 0.2869 - val_n_outputs0_loss: 0.2390 - val_n_outputs1_loss: 0.0479 Epoch 18/100 10/10 [==============================] - ETA: 0s - loss: 0.2990 - n_outputs0_loss: 0.2446 - n_outputs1_loss: 0.0544 Epoch 00018: val_loss improved from 0.28694 to 0.27270, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.2990 - n_outputs0_loss: 0.2446 - n_outputs1_loss: 0.0544 - val_loss: 0.2727 - val_n_outputs0_loss: 0.2257 - val_n_outputs1_loss: 0.0470 Epoch 19/100 10/10 [==============================] - ETA: 0s - loss: 0.2706 - n_outputs0_loss: 0.2185 - n_outputs1_loss: 0.0521 Epoch 00019: val_loss improved from 0.27270 to 0.25193, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.2706 - n_outputs0_loss: 0.2185 - n_outputs1_loss: 0.0521 - val_loss: 0.2519 - val_n_outputs0_loss: 0.2099 - val_n_outputs1_loss: 0.0421 Epoch 20/100 10/10 [==============================] - ETA: 0s - loss: 0.2602 - n_outputs0_loss: 0.2112 - n_outputs1_loss: 0.0490 Epoch 00020: val_loss improved from 0.25193 to 0.23899, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.2602 - n_outputs0_loss: 0.2112 - n_outputs1_loss: 0.0490 - val_loss: 0.2390 - val_n_outputs0_loss: 0.1974 - val_n_outputs1_loss: 0.0416 Epoch 21/100 10/10 [==============================] - ETA: 0s - loss: 0.2345 - n_outputs0_loss: 0.1866 - n_outputs1_loss: 0.0479 Epoch 00021: val_loss improved from 0.23899 to 0.23396, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.2345 - n_outputs0_loss: 0.1866 - n_outputs1_loss: 0.0479 - val_loss: 0.2340 - val_n_outputs0_loss: 0.1911 - val_n_outputs1_loss: 0.0428 Epoch 22/100 10/10 [==============================] - ETA: 0s - loss: 0.2229 - n_outputs0_loss: 0.1758 - n_outputs1_loss: 0.0471 Epoch 00022: val_loss improved from 0.23396 to 0.22651, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.2229 - n_outputs0_loss: 0.1758 - n_outputs1_loss: 0.0471 - val_loss: 0.2265 - val_n_outputs0_loss: 0.1858 - val_n_outputs1_loss: 0.0407 Epoch 23/100 10/10 [==============================] - ETA: 0s - loss: 0.2175 - n_outputs0_loss: 0.1730 - n_outputs1_loss: 0.0445 Epoch 00023: val_loss improved from 0.22651 to 0.22245, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.2175 - n_outputs0_loss: 0.1730 - n_outputs1_loss: 0.0445 - val_loss: 0.2225 - val_n_outputs0_loss: 0.1806 - val_n_outputs1_loss: 0.0419 Epoch 24/100 10/10 [==============================] - ETA: 0s - loss: 0.2084 - n_outputs0_loss: 0.1624 - n_outputs1_loss: 0.0460 Epoch 00024: val_loss improved from 0.22245 to 0.20674, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.2084 - n_outputs0_loss: 0.1624 - n_outputs1_loss: 0.0460 - val_loss: 0.2067 - val_n_outputs0_loss: 0.1694 - val_n_outputs1_loss: 0.0374 Epoch 25/100 10/10 [==============================] - ETA: 0s - loss: 0.1889 - n_outputs0_loss: 0.1457 - n_outputs1_loss: 0.0432 Epoch 00025: val_loss improved from 0.20674 to 0.20416, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.1889 - n_outputs0_loss: 0.1457 - n_outputs1_loss: 0.0432 - val_loss: 0.2042 - val_n_outputs0_loss: 0.1638 - val_n_outputs1_loss: 0.0403 Epoch 26/100 10/10 [==============================] - ETA: 0s - loss: 0.1882 - n_outputs0_loss: 0.1467 - n_outputs1_loss: 0.0414 Epoch 00026: val_loss improved from 0.20416 to 0.19422, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.1882 - n_outputs0_loss: 0.1467 - n_outputs1_loss: 0.0414 - val_loss: 0.1942 - val_n_outputs0_loss: 0.1557 - val_n_outputs1_loss: 0.0385 Epoch 27/100 10/10 [==============================] - ETA: 0s - loss: 0.1706 - n_outputs0_loss: 0.1328 - n_outputs1_loss: 0.0378 Epoch 00027: val_loss did not improve from 0.19422 10/10 [==============================] - 9s 930ms/step - loss: 0.1706 - n_outputs0_loss: 0.1328 - n_outputs1_loss: 0.0378 - val_loss: 0.2016 - val_n_outputs0_loss: 0.1615 - val_n_outputs1_loss: 0.0401 Epoch 28/100 10/10 [==============================] - ETA: 0s - loss: 0.1630 - n_outputs0_loss: 0.1248 - n_outputs1_loss: 0.0382 Epoch 00028: val_loss improved from 0.19422 to 0.18035, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.1630 - n_outputs0_loss: 0.1248 - n_outputs1_loss: 0.0382 - val_loss: 0.1803 - val_n_outputs0_loss: 0.1445 - val_n_outputs1_loss: 0.0358 Epoch 29/100 10/10 [==============================] - ETA: 0s - loss: 0.1601 - n_outputs0_loss: 0.1219 - n_outputs1_loss: 0.0382 Epoch 00029: val_loss improved from 0.18035 to 0.17528, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.1601 - n_outputs0_loss: 0.1219 - n_outputs1_loss: 0.0382 - val_loss: 0.1753 - val_n_outputs0_loss: 0.1410 - val_n_outputs1_loss: 0.0343 Epoch 30/100 10/10 [==============================] - ETA: 0s - loss: 0.1483 - n_outputs0_loss: 0.1117 - n_outputs1_loss: 0.0366 Epoch 00030: val_loss improved from 0.17528 to 0.17039, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.1483 - n_outputs0_loss: 0.1117 - n_outputs1_loss: 0.0366 - val_loss: 0.1704 - val_n_outputs0_loss: 0.1372 - val_n_outputs1_loss: 0.0332 Epoch 31/100 10/10 [==============================] - ETA: 0s - loss: 0.1481 - n_outputs0_loss: 0.1114 - n_outputs1_loss: 0.0368 Epoch 00031: val_loss did not improve from 0.17039 10/10 [==============================] - 9s 915ms/step - loss: 0.1481 - n_outputs0_loss: 0.1114 - n_outputs1_loss: 0.0368 - val_loss: 0.1783 - val_n_outputs0_loss: 0.1436 - val_n_outputs1_loss: 0.0347 Epoch 32/100 10/10 [==============================] - ETA: 0s - loss: 0.1470 - n_outputs0_loss: 0.1111 - n_outputs1_loss: 0.0358 Epoch 00032: val_loss improved from 0.17039 to 0.16278, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.1470 - n_outputs0_loss: 0.1111 - n_outputs1_loss: 0.0358 - val_loss: 0.1628 - val_n_outputs0_loss: 0.1301 - val_n_outputs1_loss: 0.0327 Epoch 33/100 10/10 [==============================] - ETA: 0s - loss: 0.1368 - n_outputs0_loss: 0.1027 - n_outputs1_loss: 0.0341 Epoch 00033: val_loss did not improve from 0.16278 10/10 [==============================] - 9s 928ms/step - loss: 0.1368 - n_outputs0_loss: 0.1027 - n_outputs1_loss: 0.0341 - val_loss: 0.1666 - val_n_outputs0_loss: 0.1345 - val_n_outputs1_loss: 0.0320 Epoch 34/100 10/10 [==============================] - ETA: 0s - loss: 0.1305 - n_outputs0_loss: 0.0971 - n_outputs1_loss: 0.0334 Epoch 00034: val_loss did not improve from 0.16278 10/10 [==============================] - 9s 929ms/step - loss: 0.1305 - n_outputs0_loss: 0.0971 - n_outputs1_loss: 0.0334 - val_loss: 0.1728 - val_n_outputs0_loss: 0.1413 - val_n_outputs1_loss: 0.0315 Epoch 35/100 10/10 [==============================] - ETA: 0s - loss: 0.1353 - n_outputs0_loss: 0.1027 - n_outputs1_loss: 0.0326 Epoch 00035: val_loss did not improve from 0.16278 10/10 [==============================] - 9s 933ms/step - loss: 0.1353 - n_outputs0_loss: 0.1027 - n_outputs1_loss: 0.0326 - val_loss: 0.1706 - val_n_outputs0_loss: 0.1391 - val_n_outputs1_loss: 0.0315 Epoch 36/100 10/10 [==============================] - ETA: 0s - loss: 0.1319 - n_outputs0_loss: 0.0989 - n_outputs1_loss: 0.0331 Epoch 00036: val_loss did not improve from 0.16278 10/10 [==============================] - 9s 936ms/step - loss: 0.1319 - n_outputs0_loss: 0.0989 - n_outputs1_loss: 0.0331 - val_loss: 0.1729 - val_n_outputs0_loss: 0.1401 - val_n_outputs1_loss: 0.0328 Epoch 37/100 10/10 [==============================] - ETA: 0s - loss: 0.1290 - n_outputs0_loss: 0.0952 - n_outputs1_loss: 0.0338 Epoch 00037: val_loss did not improve from 0.16278 10/10 [==============================] - 9s 929ms/step - loss: 0.1290 - n_outputs0_loss: 0.0952 - n_outputs1_loss: 0.0338 - val_loss: 0.1709 - val_n_outputs0_loss: 0.1381 - val_n_outputs1_loss: 0.0327 WARNING: CPU random generator seem to be failing, disable hardware random number generation WARNING: RDRND generated: 0xffffffff 0xffffffff 0xffffffff 0xffffffff (donkey) arl@arl1: ```","title":"n_outputs1 (Dense)              (None, 1)            51          dropout_6[0][0]"},{"location":"training-logs/msp-car-1/","text":"Minnesota STEM Partners Car 1 Training Log 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 time donkey train -- tub =./ data / msp - car - 1 -- model =./ models / msp - car - 1. h5 ________ ______ _________ ___ __ \\ _______________ / ___________ __ __ ____ / _____ ________ __ / / / __ \\ _ __ \\ _ // _ / _ \\ _ / / / _ / _ __ ` / _ ___ / _ / _ / // / _ / / / / / , < / __ / / _ / / / / ___ / / _ / / _ / / _____ / \\ ____ // _ / / _ // _ /| _ | \\ ___ / _ \\ __ , / \\ ____ / \\ __ , _ / / _ / / ____ / using donkey v4 . 2.1 ... loading config file : ./ config . py loading personal config over - rides from myconfig . py \"get_model_by_type\" model Type is : linear Created KerasLinear 2021 - 07 - 26 21 : 18 : 57.390998 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcuda . so . 1 2021 - 07 - 26 21 : 18 : 57.409838 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 18 : 57.410285 : I tensorflow / core / common_runtime / gpu / gpu_device . cc : 1561 ] Found device 0 with properties : pciBusID : 0000 : 09 : 00.0 name : NVIDIA GeForce RTX 2080 Ti computeCapability : 7.5 coreClock : 1.635 GHz coreCount : 68 deviceMemorySize : 10.76 GiB deviceMemoryBandwidth : 573.69 GiB / s 2021 - 07 - 26 21 : 18 : 57.410424 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcudart . so . 10.1 2021 - 07 - 26 21 : 18 : 57.411314 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcublas . so . 10 2021 - 07 - 26 21 : 18 : 57.412358 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcufft . so . 10 2021 - 07 - 26 21 : 18 : 57.412506 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcurand . so . 10 2021 - 07 - 26 21 : 18 : 57.413323 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcusolver . so . 10 2021 - 07 - 26 21 : 18 : 57.413712 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcusparse . so . 10 2021 - 07 - 26 21 : 18 : 57.415437 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcudnn . so . 7 2021 - 07 - 26 21 : 18 : 57.415619 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 18 : 57.416133 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 18 : 57.416523 : I tensorflow / core / common_runtime / gpu / gpu_device . cc : 1703 ] Adding visible gpu devices : 0 2021 - 07 - 26 21 : 18 : 57.416750 : I tensorflow / core / platform / cpu_feature_guard . cc : 143 ] Your CPU supports instructions that this TensorFlow binary was not compiled to use : SSE4 . 1 SSE4 . 2 AVX AVX2 FMA 2021 - 07 - 26 21 : 18 : 57.420820 : I tensorflow / core / platform / profile_utils / cpu_utils . cc : 102 ] CPU Frequency : 3592950000 Hz 2021 - 07 - 26 21 : 18 : 57.421125 : I tensorflow / compiler / xla / service / service . cc : 168 ] XLA service 0x5629cbee7970 initialized for platform Host ( this does not guarantee that XLA will be used ) . Devices : 2021 - 07 - 26 21 : 18 : 57.421136 : I tensorflow / compiler / xla / service / service . cc : 176 ] StreamExecutor device ( 0 ): Host , Default Version 2021 - 07 - 26 21 : 18 : 57.421270 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 18 : 57.421679 : I tensorflow / core / common_runtime / gpu / gpu_device . cc : 1561 ] Found device 0 with properties : pciBusID : 0000 : 09 : 00.0 name : NVIDIA GeForce RTX 2080 Ti computeCapability : 7.5 coreClock : 1.635 GHz coreCount : 68 deviceMemorySize : 10.76 GiB deviceMemoryBandwidth : 573.69 GiB / s 2021 - 07 - 26 21 : 18 : 57.421712 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcudart . so . 10.1 2021 - 07 - 26 21 : 18 : 57.421724 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcublas . so . 10 2021 - 07 - 26 21 : 18 : 57.421735 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcufft . so . 10 2021 - 07 - 26 21 : 18 : 57.421746 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcurand . so . 10 2021 - 07 - 26 21 : 18 : 57.421756 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcusolver . so . 10 2021 - 07 - 26 21 : 18 : 57.421766 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcusparse . so . 10 2021 - 07 - 26 21 : 18 : 57.421776 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcudnn . so . 7 2021 - 07 - 26 21 : 18 : 57.421840 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 18 : 57.422285 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 18 : 57.422675 : I tensorflow / core / common_runtime / gpu / gpu_device . cc : 1703 ] Adding visible gpu devices : 0 2021 - 07 - 26 21 : 18 : 57.422700 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcudart . so . 10.1 2021 - 07 - 26 21 : 18 : 57.504507 : I tensorflow / core / common_runtime / gpu / gpu_device . cc : 1102 ] Device interconnect StreamExecutor with strength 1 edge matrix : 2021 - 07 - 26 21 : 18 : 57.504534 : I tensorflow / core / common_runtime / gpu / gpu_device . cc : 1108 ] 0 2021 - 07 - 26 21 : 18 : 57.504541 : I tensorflow / core / common_runtime / gpu / gpu_device . cc : 1121 ] 0 : N 2021 - 07 - 26 21 : 18 : 57.504754 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 18 : 57.505207 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 18 : 57.505632 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 18 : 57.506019 : I tensorflow / core / common_runtime / gpu / gpu_device . cc : 1247 ] Created TensorFlow device ( / job : localhost / replica : 0 / task : 0 / device : GPU : 0 with 9892 MB memory ) -> physical GPU ( device : 0 , name : NVIDIA GeForce RTX 2080 Ti , pci bus id : 0000 : 09 : 00.0 , compute capability : 7.5 ) 2021 - 07 - 26 21 : 18 : 57.507379 : I tensorflow / compiler / xla / service / service . cc : 168 ] XLA service 0x5629cdd66f30 initialized for platform CUDA ( this does not guarantee that XLA will be used ) . Devices : 2021 - 07 - 26 21 : 18 : 57.507389 : I tensorflow / compiler / xla / service / service . cc : 176 ] StreamExecutor device ( 0 ): NVIDIA GeForce RTX 2080 Ti , Compute Capability 7.5 Model : \"model\" __________________________________________________________________________________________________ Layer ( type ) Output Shape Param # Connected to ================================================================================================== img_in ( InputLayer ) [( None , 224 , 224 , 3 ) 0 __________________________________________________________________________________________________ conv2d_1 ( Conv2D ) ( None , 110 , 110 , 24 ) 1824 img_in [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout ( Dropout ) ( None , 110 , 110 , 24 ) 0 conv2d_1 [ 0 ][ 0 ] __________________________________________________________________________________________________ conv2d_2 ( Conv2D ) ( None , 53 , 53 , 32 ) 19232 dropout [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_1 ( Dropout ) ( None , 53 , 53 , 32 ) 0 conv2d_2 [ 0 ][ 0 ] __________________________________________________________________________________________________ conv2d_3 ( Conv2D ) ( None , 25 , 25 , 64 ) 51264 dropout_1 [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_2 ( Dropout ) ( None , 25 , 25 , 64 ) 0 conv2d_3 [ 0 ][ 0 ] __________________________________________________________________________________________________ conv2d_4 ( Conv2D ) ( None , 23 , 23 , 64 ) 36928 dropout_2 [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_3 ( Dropout ) ( None , 23 , 23 , 64 ) 0 conv2d_4 [ 0 ][ 0 ] __________________________________________________________________________________________________ conv2d_5 ( Conv2D ) ( None , 21 , 21 , 64 ) 36928 dropout_3 [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_4 ( Dropout ) ( None , 21 , 21 , 64 ) 0 conv2d_5 [ 0 ][ 0 ] __________________________________________________________________________________________________ flattened ( Flatten ) ( None , 28224 ) 0 dropout_4 [ 0 ][ 0 ] __________________________________________________________________________________________________ dense_1 ( Dense ) ( None , 100 ) 2822500 flattened [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_5 ( Dropout ) ( None , 100 ) 0 dense_1 [ 0 ][ 0 ] __________________________________________________________________________________________________ dense_2 ( Dense ) ( None , 50 ) 5050 dropout_5 [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_6 ( Dropout ) ( None , 50 ) 0 dense_2 [ 0 ][ 0 ] __________________________________________________________________________________________________ n_outputs0 ( Dense ) ( None , 1 ) 51 dropout_6 [ 0 ][ 0 ] __________________________________________________________________________________________________ n_outputs1 ( Dense ) ( None , 1 ) 51 dropout_6 [ 0 ][ 0 ] ================================================================================================== Total params : 2 , 973 , 828 Trainable params : 2 , 973 , 828 Non - trainable params : 0 __________________________________________________________________________________________________ None Using catalog / home / arl / mycar / data / msp - car - 1 / catalog_17 . catalog Records # Training 1265 Records # Validation 317 Epoch 1 / 100 2021 - 07 - 26 21 : 18 : 58.397797 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcublas . so . 10 2021 - 07 - 26 21 : 18 : 58.705078 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcudnn . so . 7 2021 - 07 - 26 21 : 18 : 59.429125 : W tensorflow / stream_executor / gpu / asm_compiler . cc : 116 ] *** WARNING *** You are using ptxas 9.1 . 108 , which is older than 9.2 . 88. ptxas 9. x before 9.2 . 88 is known to miscompile XLA code , leading to incorrect results or invalid - address errors . You do not need to update to CUDA 9.2 . 88 ; cherry - picking the ptxas binary is sufficient . 2021 - 07 - 26 21 : 18 : 59.481809 : W tensorflow / stream_executor / gpu / redzone_allocator . cc : 314 ] Internal : ptxas exited with non - zero error code 65280 , output : ptxas fatal : Value 'sm_75' is not defined for option 'gpu-name' Relying on driver to perform ptx compilation . Modify $ PATH to customize ptxas location . This message will be only logged once . 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.6674 - n_outputs0_loss : 0.5162 - n_outputs1_loss : 0.1512 Epoch 00001 : val_loss improved from inf to 0.60297 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 3 s 288 ms / step - loss : 0.6674 - n_outputs0_loss : 0.5162 - n_outputs1_loss : 0.1512 - val_loss : 0.6030 - val_n_outputs0_loss : 0.4514 - val_n_outputs1_loss : 0.1516 Epoch 2 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.6050 - n_outputs0_loss : 0.5074 - n_outputs1_loss : 0.0976 Epoch 00002 : val_loss improved from 0.60297 to 0.51595 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 124 ms / step - loss : 0.6050 - n_outputs0_loss : 0.5074 - n_outputs1_loss : 0.0976 - val_loss : 0.5160 - val_n_outputs0_loss : 0.4188 - val_n_outputs1_loss : 0.0972 Epoch 3 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.5707 - n_outputs0_loss : 0.4923 - n_outputs1_loss : 0.0784 Epoch 00003 : val_loss improved from 0.51595 to 0.50280 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 110 ms / step - loss : 0.5707 - n_outputs0_loss : 0.4923 - n_outputs1_loss : 0.0784 - val_loss : 0.5028 - val_n_outputs0_loss : 0.4224 - val_n_outputs1_loss : 0.0804 Epoch 4 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.5615 - n_outputs0_loss : 0.4917 - n_outputs1_loss : 0.0698 Epoch 00004 : val_loss improved from 0.50280 to 0.49159 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 110 ms / step - loss : 0.5615 - n_outputs0_loss : 0.4917 - n_outputs1_loss : 0.0698 - val_loss : 0.4916 - val_n_outputs0_loss : 0.4203 - val_n_outputs1_loss : 0.0713 Epoch 5 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.5541 - n_outputs0_loss : 0.4854 - n_outputs1_loss : 0.0687 Epoch 00005 : val_loss improved from 0.49159 to 0.48784 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 104 ms / step - loss : 0.5541 - n_outputs0_loss : 0.4854 - n_outputs1_loss : 0.0687 - val_loss : 0.4878 - val_n_outputs0_loss : 0.4107 - val_n_outputs1_loss : 0.0772 Epoch 6 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.5527 - n_outputs0_loss : 0.4827 - n_outputs1_loss : 0.0701 Epoch 00006 : val_loss improved from 0.48784 to 0.48521 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 109 ms / step - loss : 0.5527 - n_outputs0_loss : 0.4827 - n_outputs1_loss : 0.0701 - val_loss : 0.4852 - val_n_outputs0_loss : 0.4127 - val_n_outputs1_loss : 0.0725 Epoch 7 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.5405 - n_outputs0_loss : 0.4764 - n_outputs1_loss : 0.0641 Epoch 00007 : val_loss improved from 0.48521 to 0.48270 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 110 ms / step - loss : 0.5405 - n_outputs0_loss : 0.4764 - n_outputs1_loss : 0.0641 - val_loss : 0.4827 - val_n_outputs0_loss : 0.4097 - val_n_outputs1_loss : 0.0730 Epoch 8 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.5383 - n_outputs0_loss : 0.4724 - n_outputs1_loss : 0.0659 Epoch 00008 : val_loss improved from 0.48270 to 0.47415 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 109 ms / step - loss : 0.5383 - n_outputs0_loss : 0.4724 - n_outputs1_loss : 0.0659 - val_loss : 0.4741 - val_n_outputs0_loss : 0.4026 - val_n_outputs1_loss : 0.0715 Epoch 9 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.5288 - n_outputs0_loss : 0.4640 - n_outputs1_loss : 0.0648 Epoch 00009 : val_loss did not improve from 0.47415 10 / 10 [ ============================== ] - 1 s 101 ms / step - loss : 0.5288 - n_outputs0_loss : 0.4640 - n_outputs1_loss : 0.0648 - val_loss : 0.4780 - val_n_outputs0_loss : 0.4069 - val_n_outputs1_loss : 0.0711 Epoch 10 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.5344 - n_outputs0_loss : 0.4677 - n_outputs1_loss : 0.0667 Epoch 00010 : val_loss improved from 0.47415 to 0.45939 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 111 ms / step - loss : 0.5344 - n_outputs0_loss : 0.4677 - n_outputs1_loss : 0.0667 - val_loss : 0.4594 - val_n_outputs0_loss : 0.3903 - val_n_outputs1_loss : 0.0691 Epoch 11 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.5014 - n_outputs0_loss : 0.4349 - n_outputs1_loss : 0.0666 Epoch 00011 : val_loss improved from 0.45939 to 0.44304 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 112 ms / step - loss : 0.5014 - n_outputs0_loss : 0.4349 - n_outputs1_loss : 0.0666 - val_loss : 0.4430 - val_n_outputs0_loss : 0.3672 - val_n_outputs1_loss : 0.0758 Epoch 12 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.4585 - n_outputs0_loss : 0.3847 - n_outputs1_loss : 0.0738 Epoch 00012 : val_loss improved from 0.44304 to 0.36563 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 109 ms / step - loss : 0.4585 - n_outputs0_loss : 0.3847 - n_outputs1_loss : 0.0738 - val_loss : 0.3656 - val_n_outputs0_loss : 0.2934 - val_n_outputs1_loss : 0.0723 Epoch 13 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.3922 - n_outputs0_loss : 0.3257 - n_outputs1_loss : 0.0664 Epoch 00013 : val_loss improved from 0.36563 to 0.30773 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 111 ms / step - loss : 0.3922 - n_outputs0_loss : 0.3257 - n_outputs1_loss : 0.0664 - val_loss : 0.3077 - val_n_outputs0_loss : 0.2463 - val_n_outputs1_loss : 0.0614 Epoch 14 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.3662 - n_outputs0_loss : 0.3052 - n_outputs1_loss : 0.0610 Epoch 00014 : val_loss improved from 0.30773 to 0.27574 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 107 ms / step - loss : 0.3662 - n_outputs0_loss : 0.3052 - n_outputs1_loss : 0.0610 - val_loss : 0.2757 - val_n_outputs0_loss : 0.2294 - val_n_outputs1_loss : 0.0463 Epoch 15 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.3233 - n_outputs0_loss : 0.2626 - n_outputs1_loss : 0.0607 Epoch 00015 : val_loss improved from 0.27574 to 0.24205 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 110 ms / step - loss : 0.3233 - n_outputs0_loss : 0.2626 - n_outputs1_loss : 0.0607 - val_loss : 0.2421 - val_n_outputs0_loss : 0.1966 - val_n_outputs1_loss : 0.0454 Epoch 16 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.3078 - n_outputs0_loss : 0.2500 - n_outputs1_loss : 0.0577 Epoch 00016 : val_loss did not improve from 0.24205 10 / 10 [ ============================== ] - 1 s 100 ms / step - loss : 0.3078 - n_outputs0_loss : 0.2500 - n_outputs1_loss : 0.0577 - val_loss : 0.2473 - val_n_outputs0_loss : 0.2023 - val_n_outputs1_loss : 0.0450 Epoch 17 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.2959 - n_outputs0_loss : 0.2404 - n_outputs1_loss : 0.0555 Epoch 00017 : val_loss improved from 0.24205 to 0.22809 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 113 ms / step - loss : 0.2959 - n_outputs0_loss : 0.2404 - n_outputs1_loss : 0.0555 - val_loss : 0.2281 - val_n_outputs0_loss : 0.1842 - val_n_outputs1_loss : 0.0438 Epoch 18 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.2820 - n_outputs0_loss : 0.2280 - n_outputs1_loss : 0.0540 Epoch 00018 : val_loss improved from 0.22809 to 0.21671 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 107 ms / step - loss : 0.2820 - n_outputs0_loss : 0.2280 - n_outputs1_loss : 0.0540 - val_loss : 0.2167 - val_n_outputs0_loss : 0.1768 - val_n_outputs1_loss : 0.0400 Epoch 19 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.2568 - n_outputs0_loss : 0.2044 - n_outputs1_loss : 0.0524 Epoch 00019 : val_loss did not improve from 0.21671 10 / 10 [ ============================== ] - 1 s 99 ms / step - loss : 0.2568 - n_outputs0_loss : 0.2044 - n_outputs1_loss : 0.0524 - val_loss : 0.2190 - val_n_outputs0_loss : 0.1788 - val_n_outputs1_loss : 0.0402 Epoch 20 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.2621 - n_outputs0_loss : 0.2123 - n_outputs1_loss : 0.0499 Epoch 00020 : val_loss improved from 0.21671 to 0.21046 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 113 ms / step - loss : 0.2621 - n_outputs0_loss : 0.2123 - n_outputs1_loss : 0.0499 - val_loss : 0.2105 - val_n_outputs0_loss : 0.1718 - val_n_outputs1_loss : 0.0386 Epoch 21 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.2521 - n_outputs0_loss : 0.2052 - n_outputs1_loss : 0.0469 Epoch 00021 : val_loss improved from 0.21046 to 0.20605 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 111 ms / step - loss : 0.2521 - n_outputs0_loss : 0.2052 - n_outputs1_loss : 0.0469 - val_loss : 0.2060 - val_n_outputs0_loss : 0.1675 - val_n_outputs1_loss : 0.0385 Epoch 22 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.2261 - n_outputs0_loss : 0.1781 - n_outputs1_loss : 0.0480 Epoch 00022 : val_loss improved from 0.20605 to 0.20553 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 106 ms / step - loss : 0.2261 - n_outputs0_loss : 0.1781 - n_outputs1_loss : 0.0480 - val_loss : 0.2055 - val_n_outputs0_loss : 0.1711 - val_n_outputs1_loss : 0.0344 Epoch 23 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.2222 - n_outputs0_loss : 0.1794 - n_outputs1_loss : 0.0429 Epoch 00023 : val_loss improved from 0.20553 to 0.20273 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 109 ms / step - loss : 0.2222 - n_outputs0_loss : 0.1794 - n_outputs1_loss : 0.0429 - val_loss : 0.2027 - val_n_outputs0_loss : 0.1697 - val_n_outputs1_loss : 0.0331 Epoch 24 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.2126 - n_outputs0_loss : 0.1698 - n_outputs1_loss : 0.0428 Epoch 00024 : val_loss improved from 0.20273 to 0.19049 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 105 ms / step - loss : 0.2126 - n_outputs0_loss : 0.1698 - n_outputs1_loss : 0.0428 - val_loss : 0.1905 - val_n_outputs0_loss : 0.1562 - val_n_outputs1_loss : 0.0343 Epoch 25 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.2062 - n_outputs0_loss : 0.1658 - n_outputs1_loss : 0.0404 Epoch 00025 : val_loss improved from 0.19049 to 0.18404 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 109 ms / step - loss : 0.2062 - n_outputs0_loss : 0.1658 - n_outputs1_loss : 0.0404 - val_loss : 0.1840 - val_n_outputs0_loss : 0.1488 - val_n_outputs1_loss : 0.0352 Epoch 26 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.1928 - n_outputs0_loss : 0.1555 - n_outputs1_loss : 0.0372 Epoch 00026 : val_loss did not improve from 0.18404 10 / 10 [ ============================== ] - 1 s 102 ms / step - loss : 0.1928 - n_outputs0_loss : 0.1555 - n_outputs1_loss : 0.0372 - val_loss : 0.1907 - val_n_outputs0_loss : 0.1563 - val_n_outputs1_loss : 0.0344 Epoch 27 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.1834 - n_outputs0_loss : 0.1428 - n_outputs1_loss : 0.0406 Epoch 00027 : val_loss did not improve from 0.18404 10 / 10 [ ============================== ] - 1 s 103 ms / step - loss : 0.1834 - n_outputs0_loss : 0.1428 - n_outputs1_loss : 0.0406 - val_loss : 0.1922 - val_n_outputs0_loss : 0.1527 - val_n_outputs1_loss : 0.0396 Epoch 28 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.1668 - n_outputs0_loss : 0.1282 - n_outputs1_loss : 0.0386 Epoch 00028 : val_loss improved from 0.18404 to 0.17462 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 113 ms / step - loss : 0.1668 - n_outputs0_loss : 0.1282 - n_outputs1_loss : 0.0386 - val_loss : 0.1746 - val_n_outputs0_loss : 0.1436 - val_n_outputs1_loss : 0.0311 Epoch 29 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.1654 - n_outputs0_loss : 0.1282 - n_outputs1_loss : 0.0372 Epoch 00029 : val_loss improved from 0.17462 to 0.17365 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 107 ms / step - loss : 0.1654 - n_outputs0_loss : 0.1282 - n_outputs1_loss : 0.0372 - val_loss : 0.1736 - val_n_outputs0_loss : 0.1432 - val_n_outputs1_loss : 0.0305 Epoch 30 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.1615 - n_outputs0_loss : 0.1250 - n_outputs1_loss : 0.0364 Epoch 00030 : val_loss did not improve from 0.17365 10 / 10 [ ============================== ] - 1 s 96 ms / step - loss : 0.1615 - n_outputs0_loss : 0.1250 - n_outputs1_loss : 0.0364 - val_loss : 0.1799 - val_n_outputs0_loss : 0.1493 - val_n_outputs1_loss : 0.0306 Epoch 31 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.1495 - n_outputs0_loss : 0.1162 - n_outputs1_loss : 0.0332 Epoch 00031 : val_loss improved from 0.17365 to 0.17255 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 112 ms / step - loss : 0.1495 - n_outputs0_loss : 0.1162 - n_outputs1_loss : 0.0332 - val_loss : 0.1726 - val_n_outputs0_loss : 0.1383 - val_n_outputs1_loss : 0.0342 Epoch 32 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.1453 - n_outputs0_loss : 0.1121 - n_outputs1_loss : 0.0333 Epoch 00032 : val_loss did not improve from 0.17255 10 / 10 [ ============================== ] - 1 s 104 ms / step - loss : 0.1453 - n_outputs0_loss : 0.1121 - n_outputs1_loss : 0.0333 - val_loss : 0.1764 - val_n_outputs0_loss : 0.1456 - val_n_outputs1_loss : 0.0308 Epoch 33 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.1346 - n_outputs0_loss : 0.1043 - n_outputs1_loss : 0.0303 Epoch 00033 : val_loss improved from 0.17255 to 0.17092 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 115 ms / step - loss : 0.1346 - n_outputs0_loss : 0.1043 - n_outputs1_loss : 0.0303 - val_loss : 0.1709 - val_n_outputs0_loss : 0.1395 - val_n_outputs1_loss : 0.0315 Epoch 34 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.1293 - n_outputs0_loss : 0.0991 - n_outputs1_loss : 0.0302 Epoch 00034 : val_loss improved from 0.17092 to 0.16704 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 109 ms / step - loss : 0.1293 - n_outputs0_loss : 0.0991 - n_outputs1_loss : 0.0302 - val_loss : 0.1670 - val_n_outputs0_loss : 0.1342 - val_n_outputs1_loss : 0.0329 Epoch 35 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.1196 - n_outputs0_loss : 0.0890 - n_outputs1_loss : 0.0306 Epoch 00035 : val_loss improved from 0.16704 to 0.15917 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 110 ms / step - loss : 0.1196 - n_outputs0_loss : 0.0890 - n_outputs1_loss : 0.0306 - val_loss : 0.1592 - val_n_outputs0_loss : 0.1280 - val_n_outputs1_loss : 0.0311 Epoch 36 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.1086 - n_outputs0_loss : 0.0805 - n_outputs1_loss : 0.0281 Epoch 00036 : val_loss improved from 0.15917 to 0.15774 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 114 ms / step - loss : 0.1086 - n_outputs0_loss : 0.0805 - n_outputs1_loss : 0.0281 - val_loss : 0.1577 - val_n_outputs0_loss : 0.1264 - val_n_outputs1_loss : 0.0313 Epoch 37 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.1032 - n_outputs0_loss : 0.0753 - n_outputs1_loss : 0.0279 Epoch 00037 : val_loss did not improve from 0.15774 10 / 10 [ ============================== ] - 1 s 99 ms / step - loss : 0.1032 - n_outputs0_loss : 0.0753 - n_outputs1_loss : 0.0279 - val_loss : 0.1598 - val_n_outputs0_loss : 0.1281 - val_n_outputs1_loss : 0.0317 Epoch 38 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.1050 - n_outputs0_loss : 0.0783 - n_outputs1_loss : 0.0266 Epoch 00038 : val_loss did not improve from 0.15774 10 / 10 [ ============================== ] - 1 s 105 ms / step - loss : 0.1050 - n_outputs0_loss : 0.0783 - n_outputs1_loss : 0.0266 - val_loss : 0.1586 - val_n_outputs0_loss : 0.1269 - val_n_outputs1_loss : 0.0317 Epoch 39 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.0983 - n_outputs0_loss : 0.0722 - n_outputs1_loss : 0.0261 Epoch 00039 : val_loss improved from 0.15774 to 0.15441 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 111 ms / step - loss : 0.0983 - n_outputs0_loss : 0.0722 - n_outputs1_loss : 0.0261 - val_loss : 0.1544 - val_n_outputs0_loss : 0.1243 - val_n_outputs1_loss : 0.0301 Epoch 40 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.0967 - n_outputs0_loss : 0.0703 - n_outputs1_loss : 0.0265 Epoch 00040 : val_loss did not improve from 0.15441 10 / 10 [ ============================== ] - 1 s 103 ms / step - loss : 0.0967 - n_outputs0_loss : 0.0703 - n_outputs1_loss : 0.0265 - val_loss : 0.1588 - val_n_outputs0_loss : 0.1275 - val_n_outputs1_loss : 0.0313 Epoch 41 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.0989 - n_outputs0_loss : 0.0736 - n_outputs1_loss : 0.0253 Epoch 00041 : val_loss did not improve from 0.15441 10 / 10 [ ============================== ] - 1 s 104 ms / step - loss : 0.0989 - n_outputs0_loss : 0.0736 - n_outputs1_loss : 0.0253 - val_loss : 0.1580 - val_n_outputs0_loss : 0.1271 - val_n_outputs1_loss : 0.0308 Epoch 42 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.1010 - n_outputs0_loss : 0.0758 - n_outputs1_loss : 0.0253 Epoch 00042 : val_loss did not improve from 0.15441 10 / 10 [ ============================== ] - 1 s 107 ms / step - loss : 0.1010 - n_outputs0_loss : 0.0758 - n_outputs1_loss : 0.0253 - val_loss : 0.1614 - val_n_outputs0_loss : 0.1315 - val_n_outputs1_loss : 0.0299 Epoch 43 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.0923 - n_outputs0_loss : 0.0680 - n_outputs1_loss : 0.0243 Epoch 00043 : val_loss did not improve from 0.15441 10 / 10 [ ============================== ] - 1 s 101 ms / step - loss : 0.0923 - n_outputs0_loss : 0.0680 - n_outputs1_loss : 0.0243 - val_loss : 0.1587 - val_n_outputs0_loss : 0.1298 - val_n_outputs1_loss : 0.0288 Epoch 44 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.0870 - n_outputs0_loss : 0.0629 - n_outputs1_loss : 0.0242 Epoch 00044 : val_loss did not improve from 0.15441 10 / 10 [ ============================== ] - 1 s 105 ms / step - loss : 0.0870 - n_outputs0_loss : 0.0629 - n_outputs1_loss : 0.0242 - val_loss : 0.1601 - val_n_outputs0_loss : 0.1304 - val_n_outputs1_loss : 0.0296 WARNING : CPU random generator seem to be failing , disable hardware random number generation WARNING : RDRND generated : 0xffffffff 0xffffffff 0xffffffff 0xffffffff real 1 m10 . 563 s user 1 m11 . 485 s sys 0 m39 . 110 s","title":"MSP Car 1"},{"location":"training-logs/msp-car-1/#minnesota-stem-partners-car-1-training-log","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 time donkey train -- tub =./ data / msp - car - 1 -- model =./ models / msp - car - 1. h5 ________ ______ _________ ___ __ \\ _______________ / ___________ __ __ ____ / _____ ________ __ / / / __ \\ _ __ \\ _ // _ / _ \\ _ / / / _ / _ __ ` / _ ___ / _ / _ / // / _ / / / / / , < / __ / / _ / / / / ___ / / _ / / _ / / _____ / \\ ____ // _ / / _ // _ /| _ | \\ ___ / _ \\ __ , / \\ ____ / \\ __ , _ / / _ / / ____ / using donkey v4 . 2.1 ... loading config file : ./ config . py loading personal config over - rides from myconfig . py \"get_model_by_type\" model Type is : linear Created KerasLinear 2021 - 07 - 26 21 : 18 : 57.390998 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcuda . so . 1 2021 - 07 - 26 21 : 18 : 57.409838 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 18 : 57.410285 : I tensorflow / core / common_runtime / gpu / gpu_device . cc : 1561 ] Found device 0 with properties : pciBusID : 0000 : 09 : 00.0 name : NVIDIA GeForce RTX 2080 Ti computeCapability : 7.5 coreClock : 1.635 GHz coreCount : 68 deviceMemorySize : 10.76 GiB deviceMemoryBandwidth : 573.69 GiB / s 2021 - 07 - 26 21 : 18 : 57.410424 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcudart . so . 10.1 2021 - 07 - 26 21 : 18 : 57.411314 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcublas . so . 10 2021 - 07 - 26 21 : 18 : 57.412358 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcufft . so . 10 2021 - 07 - 26 21 : 18 : 57.412506 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcurand . so . 10 2021 - 07 - 26 21 : 18 : 57.413323 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcusolver . so . 10 2021 - 07 - 26 21 : 18 : 57.413712 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcusparse . so . 10 2021 - 07 - 26 21 : 18 : 57.415437 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcudnn . so . 7 2021 - 07 - 26 21 : 18 : 57.415619 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 18 : 57.416133 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 18 : 57.416523 : I tensorflow / core / common_runtime / gpu / gpu_device . cc : 1703 ] Adding visible gpu devices : 0 2021 - 07 - 26 21 : 18 : 57.416750 : I tensorflow / core / platform / cpu_feature_guard . cc : 143 ] Your CPU supports instructions that this TensorFlow binary was not compiled to use : SSE4 . 1 SSE4 . 2 AVX AVX2 FMA 2021 - 07 - 26 21 : 18 : 57.420820 : I tensorflow / core / platform / profile_utils / cpu_utils . cc : 102 ] CPU Frequency : 3592950000 Hz 2021 - 07 - 26 21 : 18 : 57.421125 : I tensorflow / compiler / xla / service / service . cc : 168 ] XLA service 0x5629cbee7970 initialized for platform Host ( this does not guarantee that XLA will be used ) . Devices : 2021 - 07 - 26 21 : 18 : 57.421136 : I tensorflow / compiler / xla / service / service . cc : 176 ] StreamExecutor device ( 0 ): Host , Default Version 2021 - 07 - 26 21 : 18 : 57.421270 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 18 : 57.421679 : I tensorflow / core / common_runtime / gpu / gpu_device . cc : 1561 ] Found device 0 with properties : pciBusID : 0000 : 09 : 00.0 name : NVIDIA GeForce RTX 2080 Ti computeCapability : 7.5 coreClock : 1.635 GHz coreCount : 68 deviceMemorySize : 10.76 GiB deviceMemoryBandwidth : 573.69 GiB / s 2021 - 07 - 26 21 : 18 : 57.421712 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcudart . so . 10.1 2021 - 07 - 26 21 : 18 : 57.421724 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcublas . so . 10 2021 - 07 - 26 21 : 18 : 57.421735 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcufft . so . 10 2021 - 07 - 26 21 : 18 : 57.421746 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcurand . so . 10 2021 - 07 - 26 21 : 18 : 57.421756 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcusolver . so . 10 2021 - 07 - 26 21 : 18 : 57.421766 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcusparse . so . 10 2021 - 07 - 26 21 : 18 : 57.421776 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcudnn . so . 7 2021 - 07 - 26 21 : 18 : 57.421840 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 18 : 57.422285 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 18 : 57.422675 : I tensorflow / core / common_runtime / gpu / gpu_device . cc : 1703 ] Adding visible gpu devices : 0 2021 - 07 - 26 21 : 18 : 57.422700 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcudart . so . 10.1 2021 - 07 - 26 21 : 18 : 57.504507 : I tensorflow / core / common_runtime / gpu / gpu_device . cc : 1102 ] Device interconnect StreamExecutor with strength 1 edge matrix : 2021 - 07 - 26 21 : 18 : 57.504534 : I tensorflow / core / common_runtime / gpu / gpu_device . cc : 1108 ] 0 2021 - 07 - 26 21 : 18 : 57.504541 : I tensorflow / core / common_runtime / gpu / gpu_device . cc : 1121 ] 0 : N 2021 - 07 - 26 21 : 18 : 57.504754 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 18 : 57.505207 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 18 : 57.505632 : I tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : 981 ] successful NUMA node read from SysFS had negative value ( - 1 ), but there must be at least one NUMA node , so returning NUMA node zero 2021 - 07 - 26 21 : 18 : 57.506019 : I tensorflow / core / common_runtime / gpu / gpu_device . cc : 1247 ] Created TensorFlow device ( / job : localhost / replica : 0 / task : 0 / device : GPU : 0 with 9892 MB memory ) -> physical GPU ( device : 0 , name : NVIDIA GeForce RTX 2080 Ti , pci bus id : 0000 : 09 : 00.0 , compute capability : 7.5 ) 2021 - 07 - 26 21 : 18 : 57.507379 : I tensorflow / compiler / xla / service / service . cc : 168 ] XLA service 0x5629cdd66f30 initialized for platform CUDA ( this does not guarantee that XLA will be used ) . Devices : 2021 - 07 - 26 21 : 18 : 57.507389 : I tensorflow / compiler / xla / service / service . cc : 176 ] StreamExecutor device ( 0 ): NVIDIA GeForce RTX 2080 Ti , Compute Capability 7.5 Model : \"model\" __________________________________________________________________________________________________ Layer ( type ) Output Shape Param # Connected to ================================================================================================== img_in ( InputLayer ) [( None , 224 , 224 , 3 ) 0 __________________________________________________________________________________________________ conv2d_1 ( Conv2D ) ( None , 110 , 110 , 24 ) 1824 img_in [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout ( Dropout ) ( None , 110 , 110 , 24 ) 0 conv2d_1 [ 0 ][ 0 ] __________________________________________________________________________________________________ conv2d_2 ( Conv2D ) ( None , 53 , 53 , 32 ) 19232 dropout [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_1 ( Dropout ) ( None , 53 , 53 , 32 ) 0 conv2d_2 [ 0 ][ 0 ] __________________________________________________________________________________________________ conv2d_3 ( Conv2D ) ( None , 25 , 25 , 64 ) 51264 dropout_1 [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_2 ( Dropout ) ( None , 25 , 25 , 64 ) 0 conv2d_3 [ 0 ][ 0 ] __________________________________________________________________________________________________ conv2d_4 ( Conv2D ) ( None , 23 , 23 , 64 ) 36928 dropout_2 [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_3 ( Dropout ) ( None , 23 , 23 , 64 ) 0 conv2d_4 [ 0 ][ 0 ] __________________________________________________________________________________________________ conv2d_5 ( Conv2D ) ( None , 21 , 21 , 64 ) 36928 dropout_3 [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_4 ( Dropout ) ( None , 21 , 21 , 64 ) 0 conv2d_5 [ 0 ][ 0 ] __________________________________________________________________________________________________ flattened ( Flatten ) ( None , 28224 ) 0 dropout_4 [ 0 ][ 0 ] __________________________________________________________________________________________________ dense_1 ( Dense ) ( None , 100 ) 2822500 flattened [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_5 ( Dropout ) ( None , 100 ) 0 dense_1 [ 0 ][ 0 ] __________________________________________________________________________________________________ dense_2 ( Dense ) ( None , 50 ) 5050 dropout_5 [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_6 ( Dropout ) ( None , 50 ) 0 dense_2 [ 0 ][ 0 ] __________________________________________________________________________________________________ n_outputs0 ( Dense ) ( None , 1 ) 51 dropout_6 [ 0 ][ 0 ] __________________________________________________________________________________________________ n_outputs1 ( Dense ) ( None , 1 ) 51 dropout_6 [ 0 ][ 0 ] ================================================================================================== Total params : 2 , 973 , 828 Trainable params : 2 , 973 , 828 Non - trainable params : 0 __________________________________________________________________________________________________ None Using catalog / home / arl / mycar / data / msp - car - 1 / catalog_17 . catalog Records # Training 1265 Records # Validation 317 Epoch 1 / 100 2021 - 07 - 26 21 : 18 : 58.397797 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcublas . so . 10 2021 - 07 - 26 21 : 18 : 58.705078 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 44 ] Successfully opened dynamic library libcudnn . so . 7 2021 - 07 - 26 21 : 18 : 59.429125 : W tensorflow / stream_executor / gpu / asm_compiler . cc : 116 ] *** WARNING *** You are using ptxas 9.1 . 108 , which is older than 9.2 . 88. ptxas 9. x before 9.2 . 88 is known to miscompile XLA code , leading to incorrect results or invalid - address errors . You do not need to update to CUDA 9.2 . 88 ; cherry - picking the ptxas binary is sufficient . 2021 - 07 - 26 21 : 18 : 59.481809 : W tensorflow / stream_executor / gpu / redzone_allocator . cc : 314 ] Internal : ptxas exited with non - zero error code 65280 , output : ptxas fatal : Value 'sm_75' is not defined for option 'gpu-name' Relying on driver to perform ptx compilation . Modify $ PATH to customize ptxas location . This message will be only logged once . 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.6674 - n_outputs0_loss : 0.5162 - n_outputs1_loss : 0.1512 Epoch 00001 : val_loss improved from inf to 0.60297 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 3 s 288 ms / step - loss : 0.6674 - n_outputs0_loss : 0.5162 - n_outputs1_loss : 0.1512 - val_loss : 0.6030 - val_n_outputs0_loss : 0.4514 - val_n_outputs1_loss : 0.1516 Epoch 2 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.6050 - n_outputs0_loss : 0.5074 - n_outputs1_loss : 0.0976 Epoch 00002 : val_loss improved from 0.60297 to 0.51595 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 124 ms / step - loss : 0.6050 - n_outputs0_loss : 0.5074 - n_outputs1_loss : 0.0976 - val_loss : 0.5160 - val_n_outputs0_loss : 0.4188 - val_n_outputs1_loss : 0.0972 Epoch 3 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.5707 - n_outputs0_loss : 0.4923 - n_outputs1_loss : 0.0784 Epoch 00003 : val_loss improved from 0.51595 to 0.50280 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 110 ms / step - loss : 0.5707 - n_outputs0_loss : 0.4923 - n_outputs1_loss : 0.0784 - val_loss : 0.5028 - val_n_outputs0_loss : 0.4224 - val_n_outputs1_loss : 0.0804 Epoch 4 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.5615 - n_outputs0_loss : 0.4917 - n_outputs1_loss : 0.0698 Epoch 00004 : val_loss improved from 0.50280 to 0.49159 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 110 ms / step - loss : 0.5615 - n_outputs0_loss : 0.4917 - n_outputs1_loss : 0.0698 - val_loss : 0.4916 - val_n_outputs0_loss : 0.4203 - val_n_outputs1_loss : 0.0713 Epoch 5 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.5541 - n_outputs0_loss : 0.4854 - n_outputs1_loss : 0.0687 Epoch 00005 : val_loss improved from 0.49159 to 0.48784 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 104 ms / step - loss : 0.5541 - n_outputs0_loss : 0.4854 - n_outputs1_loss : 0.0687 - val_loss : 0.4878 - val_n_outputs0_loss : 0.4107 - val_n_outputs1_loss : 0.0772 Epoch 6 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.5527 - n_outputs0_loss : 0.4827 - n_outputs1_loss : 0.0701 Epoch 00006 : val_loss improved from 0.48784 to 0.48521 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 109 ms / step - loss : 0.5527 - n_outputs0_loss : 0.4827 - n_outputs1_loss : 0.0701 - val_loss : 0.4852 - val_n_outputs0_loss : 0.4127 - val_n_outputs1_loss : 0.0725 Epoch 7 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.5405 - n_outputs0_loss : 0.4764 - n_outputs1_loss : 0.0641 Epoch 00007 : val_loss improved from 0.48521 to 0.48270 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 110 ms / step - loss : 0.5405 - n_outputs0_loss : 0.4764 - n_outputs1_loss : 0.0641 - val_loss : 0.4827 - val_n_outputs0_loss : 0.4097 - val_n_outputs1_loss : 0.0730 Epoch 8 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.5383 - n_outputs0_loss : 0.4724 - n_outputs1_loss : 0.0659 Epoch 00008 : val_loss improved from 0.48270 to 0.47415 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 109 ms / step - loss : 0.5383 - n_outputs0_loss : 0.4724 - n_outputs1_loss : 0.0659 - val_loss : 0.4741 - val_n_outputs0_loss : 0.4026 - val_n_outputs1_loss : 0.0715 Epoch 9 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.5288 - n_outputs0_loss : 0.4640 - n_outputs1_loss : 0.0648 Epoch 00009 : val_loss did not improve from 0.47415 10 / 10 [ ============================== ] - 1 s 101 ms / step - loss : 0.5288 - n_outputs0_loss : 0.4640 - n_outputs1_loss : 0.0648 - val_loss : 0.4780 - val_n_outputs0_loss : 0.4069 - val_n_outputs1_loss : 0.0711 Epoch 10 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.5344 - n_outputs0_loss : 0.4677 - n_outputs1_loss : 0.0667 Epoch 00010 : val_loss improved from 0.47415 to 0.45939 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 111 ms / step - loss : 0.5344 - n_outputs0_loss : 0.4677 - n_outputs1_loss : 0.0667 - val_loss : 0.4594 - val_n_outputs0_loss : 0.3903 - val_n_outputs1_loss : 0.0691 Epoch 11 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.5014 - n_outputs0_loss : 0.4349 - n_outputs1_loss : 0.0666 Epoch 00011 : val_loss improved from 0.45939 to 0.44304 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 112 ms / step - loss : 0.5014 - n_outputs0_loss : 0.4349 - n_outputs1_loss : 0.0666 - val_loss : 0.4430 - val_n_outputs0_loss : 0.3672 - val_n_outputs1_loss : 0.0758 Epoch 12 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.4585 - n_outputs0_loss : 0.3847 - n_outputs1_loss : 0.0738 Epoch 00012 : val_loss improved from 0.44304 to 0.36563 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 109 ms / step - loss : 0.4585 - n_outputs0_loss : 0.3847 - n_outputs1_loss : 0.0738 - val_loss : 0.3656 - val_n_outputs0_loss : 0.2934 - val_n_outputs1_loss : 0.0723 Epoch 13 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.3922 - n_outputs0_loss : 0.3257 - n_outputs1_loss : 0.0664 Epoch 00013 : val_loss improved from 0.36563 to 0.30773 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 111 ms / step - loss : 0.3922 - n_outputs0_loss : 0.3257 - n_outputs1_loss : 0.0664 - val_loss : 0.3077 - val_n_outputs0_loss : 0.2463 - val_n_outputs1_loss : 0.0614 Epoch 14 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.3662 - n_outputs0_loss : 0.3052 - n_outputs1_loss : 0.0610 Epoch 00014 : val_loss improved from 0.30773 to 0.27574 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 107 ms / step - loss : 0.3662 - n_outputs0_loss : 0.3052 - n_outputs1_loss : 0.0610 - val_loss : 0.2757 - val_n_outputs0_loss : 0.2294 - val_n_outputs1_loss : 0.0463 Epoch 15 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.3233 - n_outputs0_loss : 0.2626 - n_outputs1_loss : 0.0607 Epoch 00015 : val_loss improved from 0.27574 to 0.24205 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 110 ms / step - loss : 0.3233 - n_outputs0_loss : 0.2626 - n_outputs1_loss : 0.0607 - val_loss : 0.2421 - val_n_outputs0_loss : 0.1966 - val_n_outputs1_loss : 0.0454 Epoch 16 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.3078 - n_outputs0_loss : 0.2500 - n_outputs1_loss : 0.0577 Epoch 00016 : val_loss did not improve from 0.24205 10 / 10 [ ============================== ] - 1 s 100 ms / step - loss : 0.3078 - n_outputs0_loss : 0.2500 - n_outputs1_loss : 0.0577 - val_loss : 0.2473 - val_n_outputs0_loss : 0.2023 - val_n_outputs1_loss : 0.0450 Epoch 17 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.2959 - n_outputs0_loss : 0.2404 - n_outputs1_loss : 0.0555 Epoch 00017 : val_loss improved from 0.24205 to 0.22809 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 113 ms / step - loss : 0.2959 - n_outputs0_loss : 0.2404 - n_outputs1_loss : 0.0555 - val_loss : 0.2281 - val_n_outputs0_loss : 0.1842 - val_n_outputs1_loss : 0.0438 Epoch 18 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.2820 - n_outputs0_loss : 0.2280 - n_outputs1_loss : 0.0540 Epoch 00018 : val_loss improved from 0.22809 to 0.21671 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 107 ms / step - loss : 0.2820 - n_outputs0_loss : 0.2280 - n_outputs1_loss : 0.0540 - val_loss : 0.2167 - val_n_outputs0_loss : 0.1768 - val_n_outputs1_loss : 0.0400 Epoch 19 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.2568 - n_outputs0_loss : 0.2044 - n_outputs1_loss : 0.0524 Epoch 00019 : val_loss did not improve from 0.21671 10 / 10 [ ============================== ] - 1 s 99 ms / step - loss : 0.2568 - n_outputs0_loss : 0.2044 - n_outputs1_loss : 0.0524 - val_loss : 0.2190 - val_n_outputs0_loss : 0.1788 - val_n_outputs1_loss : 0.0402 Epoch 20 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.2621 - n_outputs0_loss : 0.2123 - n_outputs1_loss : 0.0499 Epoch 00020 : val_loss improved from 0.21671 to 0.21046 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 113 ms / step - loss : 0.2621 - n_outputs0_loss : 0.2123 - n_outputs1_loss : 0.0499 - val_loss : 0.2105 - val_n_outputs0_loss : 0.1718 - val_n_outputs1_loss : 0.0386 Epoch 21 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.2521 - n_outputs0_loss : 0.2052 - n_outputs1_loss : 0.0469 Epoch 00021 : val_loss improved from 0.21046 to 0.20605 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 111 ms / step - loss : 0.2521 - n_outputs0_loss : 0.2052 - n_outputs1_loss : 0.0469 - val_loss : 0.2060 - val_n_outputs0_loss : 0.1675 - val_n_outputs1_loss : 0.0385 Epoch 22 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.2261 - n_outputs0_loss : 0.1781 - n_outputs1_loss : 0.0480 Epoch 00022 : val_loss improved from 0.20605 to 0.20553 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 106 ms / step - loss : 0.2261 - n_outputs0_loss : 0.1781 - n_outputs1_loss : 0.0480 - val_loss : 0.2055 - val_n_outputs0_loss : 0.1711 - val_n_outputs1_loss : 0.0344 Epoch 23 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.2222 - n_outputs0_loss : 0.1794 - n_outputs1_loss : 0.0429 Epoch 00023 : val_loss improved from 0.20553 to 0.20273 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 109 ms / step - loss : 0.2222 - n_outputs0_loss : 0.1794 - n_outputs1_loss : 0.0429 - val_loss : 0.2027 - val_n_outputs0_loss : 0.1697 - val_n_outputs1_loss : 0.0331 Epoch 24 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.2126 - n_outputs0_loss : 0.1698 - n_outputs1_loss : 0.0428 Epoch 00024 : val_loss improved from 0.20273 to 0.19049 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 105 ms / step - loss : 0.2126 - n_outputs0_loss : 0.1698 - n_outputs1_loss : 0.0428 - val_loss : 0.1905 - val_n_outputs0_loss : 0.1562 - val_n_outputs1_loss : 0.0343 Epoch 25 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.2062 - n_outputs0_loss : 0.1658 - n_outputs1_loss : 0.0404 Epoch 00025 : val_loss improved from 0.19049 to 0.18404 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 109 ms / step - loss : 0.2062 - n_outputs0_loss : 0.1658 - n_outputs1_loss : 0.0404 - val_loss : 0.1840 - val_n_outputs0_loss : 0.1488 - val_n_outputs1_loss : 0.0352 Epoch 26 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.1928 - n_outputs0_loss : 0.1555 - n_outputs1_loss : 0.0372 Epoch 00026 : val_loss did not improve from 0.18404 10 / 10 [ ============================== ] - 1 s 102 ms / step - loss : 0.1928 - n_outputs0_loss : 0.1555 - n_outputs1_loss : 0.0372 - val_loss : 0.1907 - val_n_outputs0_loss : 0.1563 - val_n_outputs1_loss : 0.0344 Epoch 27 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.1834 - n_outputs0_loss : 0.1428 - n_outputs1_loss : 0.0406 Epoch 00027 : val_loss did not improve from 0.18404 10 / 10 [ ============================== ] - 1 s 103 ms / step - loss : 0.1834 - n_outputs0_loss : 0.1428 - n_outputs1_loss : 0.0406 - val_loss : 0.1922 - val_n_outputs0_loss : 0.1527 - val_n_outputs1_loss : 0.0396 Epoch 28 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.1668 - n_outputs0_loss : 0.1282 - n_outputs1_loss : 0.0386 Epoch 00028 : val_loss improved from 0.18404 to 0.17462 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 113 ms / step - loss : 0.1668 - n_outputs0_loss : 0.1282 - n_outputs1_loss : 0.0386 - val_loss : 0.1746 - val_n_outputs0_loss : 0.1436 - val_n_outputs1_loss : 0.0311 Epoch 29 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.1654 - n_outputs0_loss : 0.1282 - n_outputs1_loss : 0.0372 Epoch 00029 : val_loss improved from 0.17462 to 0.17365 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 107 ms / step - loss : 0.1654 - n_outputs0_loss : 0.1282 - n_outputs1_loss : 0.0372 - val_loss : 0.1736 - val_n_outputs0_loss : 0.1432 - val_n_outputs1_loss : 0.0305 Epoch 30 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.1615 - n_outputs0_loss : 0.1250 - n_outputs1_loss : 0.0364 Epoch 00030 : val_loss did not improve from 0.17365 10 / 10 [ ============================== ] - 1 s 96 ms / step - loss : 0.1615 - n_outputs0_loss : 0.1250 - n_outputs1_loss : 0.0364 - val_loss : 0.1799 - val_n_outputs0_loss : 0.1493 - val_n_outputs1_loss : 0.0306 Epoch 31 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.1495 - n_outputs0_loss : 0.1162 - n_outputs1_loss : 0.0332 Epoch 00031 : val_loss improved from 0.17365 to 0.17255 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 112 ms / step - loss : 0.1495 - n_outputs0_loss : 0.1162 - n_outputs1_loss : 0.0332 - val_loss : 0.1726 - val_n_outputs0_loss : 0.1383 - val_n_outputs1_loss : 0.0342 Epoch 32 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.1453 - n_outputs0_loss : 0.1121 - n_outputs1_loss : 0.0333 Epoch 00032 : val_loss did not improve from 0.17255 10 / 10 [ ============================== ] - 1 s 104 ms / step - loss : 0.1453 - n_outputs0_loss : 0.1121 - n_outputs1_loss : 0.0333 - val_loss : 0.1764 - val_n_outputs0_loss : 0.1456 - val_n_outputs1_loss : 0.0308 Epoch 33 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.1346 - n_outputs0_loss : 0.1043 - n_outputs1_loss : 0.0303 Epoch 00033 : val_loss improved from 0.17255 to 0.17092 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 115 ms / step - loss : 0.1346 - n_outputs0_loss : 0.1043 - n_outputs1_loss : 0.0303 - val_loss : 0.1709 - val_n_outputs0_loss : 0.1395 - val_n_outputs1_loss : 0.0315 Epoch 34 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.1293 - n_outputs0_loss : 0.0991 - n_outputs1_loss : 0.0302 Epoch 00034 : val_loss improved from 0.17092 to 0.16704 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 109 ms / step - loss : 0.1293 - n_outputs0_loss : 0.0991 - n_outputs1_loss : 0.0302 - val_loss : 0.1670 - val_n_outputs0_loss : 0.1342 - val_n_outputs1_loss : 0.0329 Epoch 35 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.1196 - n_outputs0_loss : 0.0890 - n_outputs1_loss : 0.0306 Epoch 00035 : val_loss improved from 0.16704 to 0.15917 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 110 ms / step - loss : 0.1196 - n_outputs0_loss : 0.0890 - n_outputs1_loss : 0.0306 - val_loss : 0.1592 - val_n_outputs0_loss : 0.1280 - val_n_outputs1_loss : 0.0311 Epoch 36 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.1086 - n_outputs0_loss : 0.0805 - n_outputs1_loss : 0.0281 Epoch 00036 : val_loss improved from 0.15917 to 0.15774 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 114 ms / step - loss : 0.1086 - n_outputs0_loss : 0.0805 - n_outputs1_loss : 0.0281 - val_loss : 0.1577 - val_n_outputs0_loss : 0.1264 - val_n_outputs1_loss : 0.0313 Epoch 37 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.1032 - n_outputs0_loss : 0.0753 - n_outputs1_loss : 0.0279 Epoch 00037 : val_loss did not improve from 0.15774 10 / 10 [ ============================== ] - 1 s 99 ms / step - loss : 0.1032 - n_outputs0_loss : 0.0753 - n_outputs1_loss : 0.0279 - val_loss : 0.1598 - val_n_outputs0_loss : 0.1281 - val_n_outputs1_loss : 0.0317 Epoch 38 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.1050 - n_outputs0_loss : 0.0783 - n_outputs1_loss : 0.0266 Epoch 00038 : val_loss did not improve from 0.15774 10 / 10 [ ============================== ] - 1 s 105 ms / step - loss : 0.1050 - n_outputs0_loss : 0.0783 - n_outputs1_loss : 0.0266 - val_loss : 0.1586 - val_n_outputs0_loss : 0.1269 - val_n_outputs1_loss : 0.0317 Epoch 39 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.0983 - n_outputs0_loss : 0.0722 - n_outputs1_loss : 0.0261 Epoch 00039 : val_loss improved from 0.15774 to 0.15441 , saving model to ./ models / msp - car - 1. h5 10 / 10 [ ============================== ] - 1 s 111 ms / step - loss : 0.0983 - n_outputs0_loss : 0.0722 - n_outputs1_loss : 0.0261 - val_loss : 0.1544 - val_n_outputs0_loss : 0.1243 - val_n_outputs1_loss : 0.0301 Epoch 40 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.0967 - n_outputs0_loss : 0.0703 - n_outputs1_loss : 0.0265 Epoch 00040 : val_loss did not improve from 0.15441 10 / 10 [ ============================== ] - 1 s 103 ms / step - loss : 0.0967 - n_outputs0_loss : 0.0703 - n_outputs1_loss : 0.0265 - val_loss : 0.1588 - val_n_outputs0_loss : 0.1275 - val_n_outputs1_loss : 0.0313 Epoch 41 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.0989 - n_outputs0_loss : 0.0736 - n_outputs1_loss : 0.0253 Epoch 00041 : val_loss did not improve from 0.15441 10 / 10 [ ============================== ] - 1 s 104 ms / step - loss : 0.0989 - n_outputs0_loss : 0.0736 - n_outputs1_loss : 0.0253 - val_loss : 0.1580 - val_n_outputs0_loss : 0.1271 - val_n_outputs1_loss : 0.0308 Epoch 42 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.1010 - n_outputs0_loss : 0.0758 - n_outputs1_loss : 0.0253 Epoch 00042 : val_loss did not improve from 0.15441 10 / 10 [ ============================== ] - 1 s 107 ms / step - loss : 0.1010 - n_outputs0_loss : 0.0758 - n_outputs1_loss : 0.0253 - val_loss : 0.1614 - val_n_outputs0_loss : 0.1315 - val_n_outputs1_loss : 0.0299 Epoch 43 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.0923 - n_outputs0_loss : 0.0680 - n_outputs1_loss : 0.0243 Epoch 00043 : val_loss did not improve from 0.15441 10 / 10 [ ============================== ] - 1 s 101 ms / step - loss : 0.0923 - n_outputs0_loss : 0.0680 - n_outputs1_loss : 0.0243 - val_loss : 0.1587 - val_n_outputs0_loss : 0.1298 - val_n_outputs1_loss : 0.0288 Epoch 44 / 100 10 / 10 [ ============================== ] - ETA : 0 s - loss : 0.0870 - n_outputs0_loss : 0.0629 - n_outputs1_loss : 0.0242 Epoch 00044 : val_loss did not improve from 0.15441 10 / 10 [ ============================== ] - 1 s 105 ms / step - loss : 0.0870 - n_outputs0_loss : 0.0629 - n_outputs1_loss : 0.0242 - val_loss : 0.1601 - val_n_outputs0_loss : 0.1304 - val_n_outputs1_loss : 0.0296 WARNING : CPU random generator seem to be failing , disable hardware random number generation WARNING : RDRND generated : 0xffffffff 0xffffffff 0xffffffff 0xffffffff real 1 m10 . 563 s user 1 m11 . 485 s sys 0 m39 . 110 s","title":"Minnesota STEM Partners Car 1 Training Log"},{"location":"training-logs/msp-car-2/","text":"Training run for Minneapolis STEM Partners Car #2 had 15045 images wc -l ~/mycar/data/msp-car-2/*.catalog 15045 ls -1 ~/mycar/data/msp-car-2/images | wc -l 15045 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 ( donkey ) arl@arl1:~/mycar$ donkey train --tub = ./data/msp-car-2 --model = ./models/msp-car-2.h5 ________ ______ _________ ___ __ \\_ ______________ /___________ __ __ ____/_____ ________ __ / / / __ \\_ __ \\_ //_/ _ \\_ / / / _ / _ __ ` /_ ___/ _ /_/ // /_/ / / / / ,< / __/ /_/ / / /___ / /_/ /_ / /_____/ \\_ ___//_/ /_//_/ | _ | \\_ __/_ \\_ _, / \\_ ___/ \\_ _,_/ /_/ /____/ using donkey v4.2.1 ... loading config file: ./config.py loading personal config over-rides from myconfig.py \"get_model_by_type\" model Type is: linear Created KerasLinear 2021 -07-26 20 :22:54.320076: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcuda.so.1 2021 -07-26 20 :22:54.338339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981 ] successful NUMA node read from SysFS had negative value ( -1 ) , but there must be at least one NUMA node, so returning NUMA node zero 2021 -07-26 20 :22:54.338783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561 ] Found device 0 with properties: pciBusID: 0000 :09:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7 .5 coreClock: 1 .635GHz coreCount: 68 deviceMemorySize: 10 .76GiB deviceMemoryBandwidth: 573 .69GiB/s 2021 -07-26 20 :22:54.338925: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcudart.so.10.1 2021 -07-26 20 :22:54.339823: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcublas.so.10 2021 -07-26 20 :22:54.340834: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcufft.so.10 2021 -07-26 20 :22:54.340981: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcurand.so.10 2021 -07-26 20 :22:54.341775: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcusolver.so.10 2021 -07-26 20 :22:54.342170: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcusparse.so.10 2021 -07-26 20 :22:54.343898: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcudnn.so.7 2021 -07-26 20 :22:54.344043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981 ] successful NUMA node read from SysFS had negative value ( -1 ) , but there must be at least one NUMA node, so returning NUMA node zero 2021 -07-26 20 :22:54.344546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981 ] successful NUMA node read from SysFS had negative value ( -1 ) , but there must be at least one NUMA node, so returning NUMA node zero 2021 -07-26 20 :22:54.344933: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703 ] Adding visible gpu devices: 0 2021 -07-26 20 :22:54.345163: I tensorflow/core/platform/cpu_feature_guard.cc:143 ] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA 2021 -07-26 20 :22:54.349277: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102 ] CPU Frequency: 3592950000 Hz 2021 -07-26 20 :22:54.349572: I tensorflow/compiler/xla/service/service.cc:168 ] XLA service 0x5575e341a9f0 initialized for platform Host ( this does not guarantee that XLA will be used ) . Devices: 2021 -07-26 20 :22:54.349585: I tensorflow/compiler/xla/service/service.cc:176 ] StreamExecutor device ( 0 ) : Host, Default Version 2021 -07-26 20 :22:54.349717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981 ] successful NUMA node read from SysFS had negative value ( -1 ) , but there must be at least one NUMA node, so returning NUMA node zero 2021 -07-26 20 :22:54.350124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561 ] Found device 0 with properties: pciBusID: 0000 :09:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7 .5 coreClock: 1 .635GHz coreCount: 68 deviceMemorySize: 10 .76GiB deviceMemoryBandwidth: 573 .69GiB/s 2021 -07-26 20 :22:54.350160: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcudart.so.10.1 2021 -07-26 20 :22:54.350171: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcublas.so.10 2021 -07-26 20 :22:54.350180: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcufft.so.10 2021 -07-26 20 :22:54.350191: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcurand.so.10 2021 -07-26 20 :22:54.350200: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcusolver.so.10 2021 -07-26 20 :22:54.350210: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcusparse.so.10 2021 -07-26 20 :22:54.350220: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcudnn.so.7 2021 -07-26 20 :22:54.350282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981 ] successful NUMA node read from SysFS had negative value ( -1 ) , but there must be at least one NUMA node, so returning NUMA node zero 2021 -07-26 20 :22:54.350723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981 ] successful NUMA node read from SysFS had negative value ( -1 ) , but there must be at least one NUMA node, so returning NUMA node zero 2021 -07-26 20 :22:54.351106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703 ] Adding visible gpu devices: 0 2021 -07-26 20 :22:54.351127: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcudart.so.10.1 2021 -07-26 20 :22:54.423106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102 ] Device interconnect StreamExecutor with strength 1 edge matrix: 2021 -07-26 20 :22:54.423133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108 ] 0 2021 -07-26 20 :22:54.423138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121 ] 0 : N 2021 -07-26 20 :22:54.423354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981 ] successful NUMA node read from SysFS had negative value ( -1 ) , but there must be at least one NUMA node, so returning NUMA node zero 2021 -07-26 20 :22:54.423819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981 ] successful NUMA node read from SysFS had negative value ( -1 ) , but there must be at least one NUMA node, so returning NUMA node zero 2021 -07-26 20 :22:54.424248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981 ] successful NUMA node read from SysFS had negative value ( -1 ) , but there must be at least one NUMA node, so returning NUMA node zero 2021 -07-26 20 :22:54.424632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247 ] Created TensorFlow device ( /job:localhost/replica:0/task:0/device:GPU:0 with 9890 MB memory ) -> physical GPU ( device: 0 , name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000 :09:00.0, compute capability: 7 .5 ) 2021 -07-26 20 :22:54.425999: I tensorflow/compiler/xla/service/service.cc:168 ] XLA service 0x5575e52b18b0 initialized for platform CUDA ( this does not guarantee that XLA will be used ) . Devices: 2021 -07-26 20 :22:54.426009: I tensorflow/compiler/xla/service/service.cc:176 ] StreamExecutor device ( 0 ) : NVIDIA GeForce RTX 2080 Ti, Compute Capability 7 .5 Model: \"model\" __________________________________________________________________________________________________ Layer ( type ) Output Shape Param # Connected to ================================================================================================== img_in ( InputLayer ) [( None, 224 , 224 , 3 ) 0 __________________________________________________________________________________________________ conv2d_1 ( Conv2D ) ( None, 110 , 110 , 24 ) 1824 img_in [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout ( Dropout ) ( None, 110 , 110 , 24 ) 0 conv2d_1 [ 0 ][ 0 ] __________________________________________________________________________________________________ conv2d_2 ( Conv2D ) ( None, 53 , 53 , 32 ) 19232 dropout [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_1 ( Dropout ) ( None, 53 , 53 , 32 ) 0 conv2d_2 [ 0 ][ 0 ] __________________________________________________________________________________________________ conv2d_3 ( Conv2D ) ( None, 25 , 25 , 64 ) 51264 dropout_1 [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_2 ( Dropout ) ( None, 25 , 25 , 64 ) 0 conv2d_3 [ 0 ][ 0 ] __________________________________________________________________________________________________ conv2d_4 ( Conv2D ) ( None, 23 , 23 , 64 ) 36928 dropout_2 [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_3 ( Dropout ) ( None, 23 , 23 , 64 ) 0 conv2d_4 [ 0 ][ 0 ] __________________________________________________________________________________________________ conv2d_5 ( Conv2D ) ( None, 21 , 21 , 64 ) 36928 dropout_3 [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_4 ( Dropout ) ( None, 21 , 21 , 64 ) 0 conv2d_5 [ 0 ][ 0 ] __________________________________________________________________________________________________ flattened ( Flatten ) ( None, 28224 ) 0 dropout_4 [ 0 ][ 0 ] __________________________________________________________________________________________________ dense_1 ( Dense ) ( None, 100 ) 2822500 flattened [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_5 ( Dropout ) ( None, 100 ) 0 dense_1 [ 0 ][ 0 ] __________________________________________________________________________________________________ dense_2 ( Dense ) ( None, 50 ) 5050 dropout_5 [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_6 ( Dropout ) ( None, 50 ) 0 dense_2 [ 0 ][ 0 ] __________________________________________________________________________________________________ n_outputs0 ( Dense ) ( None, 1 ) 51 dropout_6 [ 0 ][ 0 ] __________________________________________________________________________________________________ n_outputs1 ( Dense ) ( None, 1 ) 51 dropout_6 [ 0 ][ 0 ] ================================================================================================== Total params: 2 ,973,828 Trainable params: 2 ,973,828 Non-trainable params: 0 __________________________________________________________________________________________________ None Using catalog /home/arl/mycar/data/msp-car-2/catalog_22.catalog Records # Training 11696 Records # Validation 2924 Epoch 1 /100 2021 -07-26 20 :22:55.471623: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcublas.so.10 2021 -07-26 20 :22:55.802565: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcudnn.so.7 2021 -07-26 20 :22:56.515413: W tensorflow/stream_executor/gpu/asm_compiler.cc:116 ] *** WARNING *** You are using ptxas 9 .1.108, which is older than 9 .2.88. ptxas 9 .x before 9 .2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors. You do not need to update to CUDA 9 .2.88 ; cherry-picking the ptxas binary is sufficient. 2021 -07-26 20 :22:56.559204: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314 ] Internal: ptxas exited with non-zero error code 65280 , output: ptxas fatal : Value 'sm_75' is not defined for option 'gpu-name' Relying on driver to perform ptx compilation. Modify $PATH to customize ptxas location. This message will be only logged once. 92 /92 [==============================] - ETA: 0s - loss: 0 .6304 - n_outputs0_loss: 0 .3238 - n_outputs1_loss: 0 .3066 Epoch 00001 : val_loss improved from inf to 0 .59133, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 29s 310ms/step - loss: 0 .6304 - n_outputs0_loss: 0 .3238 - n_outputs1_loss: 0 .3066 - val_loss: 0 .5913 - val_n_outputs0_loss: 0 .3121 - val_n_outputs1_loss: 0 .2793 Epoch 2 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .5150 - n_outputs0_loss: 0 .2730 - n_outputs1_loss: 0 .2419 Epoch 00002 : val_loss improved from 0 .59133 to 0 .39368, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 11s 117ms/step - loss: 0 .5150 - n_outputs0_loss: 0 .2730 - n_outputs1_loss: 0 .2419 - val_loss: 0 .3937 - val_n_outputs0_loss: 0 .2108 - val_n_outputs1_loss: 0 .1828 Epoch 3 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .3885 - n_outputs0_loss: 0 .2088 - n_outputs1_loss: 0 .1797 Epoch 00003 : val_loss improved from 0 .39368 to 0 .34087, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 11s 117ms/step - loss: 0 .3885 - n_outputs0_loss: 0 .2088 - n_outputs1_loss: 0 .1797 - val_loss: 0 .3409 - val_n_outputs0_loss: 0 .1923 - val_n_outputs1_loss: 0 .1486 Epoch 4 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .3449 - n_outputs0_loss: 0 .1870 - n_outputs1_loss: 0 .1579 Epoch 00004 : val_loss improved from 0 .34087 to 0 .30588, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 11s 119ms/step - loss: 0 .3449 - n_outputs0_loss: 0 .1870 - n_outputs1_loss: 0 .1579 - val_loss: 0 .3059 - val_n_outputs0_loss: 0 .1771 - val_n_outputs1_loss: 0 .1288 Epoch 5 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .3161 - n_outputs0_loss: 0 .1763 - n_outputs1_loss: 0 .1397 Epoch 00005 : val_loss improved from 0 .30588 to 0 .28650, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 10s 113ms/step - loss: 0 .3161 - n_outputs0_loss: 0 .1763 - n_outputs1_loss: 0 .1397 - val_loss: 0 .2865 - val_n_outputs0_loss: 0 .1722 - val_n_outputs1_loss: 0 .1143 Epoch 6 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .2876 - n_outputs0_loss: 0 .1633 - n_outputs1_loss: 0 .1243 Epoch 00006 : val_loss improved from 0 .28650 to 0 .26754, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 10s 110ms/step - loss: 0 .2876 - n_outputs0_loss: 0 .1633 - n_outputs1_loss: 0 .1243 - val_loss: 0 .2675 - val_n_outputs0_loss: 0 .1623 - val_n_outputs1_loss: 0 .1053 Epoch 7 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .2612 - n_outputs0_loss: 0 .1508 - n_outputs1_loss: 0 .1103 Epoch 00007 : val_loss improved from 0 .26754 to 0 .25034, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 10s 111ms/step - loss: 0 .2612 - n_outputs0_loss: 0 .1508 - n_outputs1_loss: 0 .1103 - val_loss: 0 .2503 - val_n_outputs0_loss: 0 .1551 - val_n_outputs1_loss: 0 .0952 Epoch 8 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .2476 - n_outputs0_loss: 0 .1435 - n_outputs1_loss: 0 .1041 Epoch 00008 : val_loss improved from 0 .25034 to 0 .24291, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 10s 109ms/step - loss: 0 .2476 - n_outputs0_loss: 0 .1435 - n_outputs1_loss: 0 .1041 - val_loss: 0 .2429 - val_n_outputs0_loss: 0 .1524 - val_n_outputs1_loss: 0 .0905 Epoch 9 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .2283 - n_outputs0_loss: 0 .1323 - n_outputs1_loss: 0 .0960 Epoch 00009 : val_loss improved from 0 .24291 to 0 .22718, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 10s 110ms/step - loss: 0 .2283 - n_outputs0_loss: 0 .1323 - n_outputs1_loss: 0 .0960 - val_loss: 0 .2272 - val_n_outputs0_loss: 0 .1450 - val_n_outputs1_loss: 0 .0821 Epoch 10 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .2183 - n_outputs0_loss: 0 .1267 - n_outputs1_loss: 0 .0916 Epoch 00010 : val_loss did not improve from 0 .22718 92 /92 [==============================] - 10s 109ms/step - loss: 0 .2183 - n_outputs0_loss: 0 .1267 - n_outputs1_loss: 0 .0916 - val_loss: 0 .2305 - val_n_outputs0_loss: 0 .1471 - val_n_outputs1_loss: 0 .0834 Epoch 11 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .2022 - n_outputs0_loss: 0 .1187 - n_outputs1_loss: 0 .0835 Epoch 00011 : val_loss improved from 0 .22718 to 0 .21581, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 10s 110ms/step - loss: 0 .2022 - n_outputs0_loss: 0 .1187 - n_outputs1_loss: 0 .0835 - val_loss: 0 .2158 - val_n_outputs0_loss: 0 .1375 - val_n_outputs1_loss: 0 .0783 Epoch 12 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1921 - n_outputs0_loss: 0 .1085 - n_outputs1_loss: 0 .0836 Epoch 00012 : val_loss did not improve from 0 .21581 92 /92 [==============================] - 10s 110ms/step - loss: 0 .1921 - n_outputs0_loss: 0 .1085 - n_outputs1_loss: 0 .0836 - val_loss: 0 .2185 - val_n_outputs0_loss: 0 .1382 - val_n_outputs1_loss: 0 .0802 Epoch 13 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1826 - n_outputs0_loss: 0 .1056 - n_outputs1_loss: 0 .0770 Epoch 00013 : val_loss did not improve from 0 .21581 92 /92 [==============================] - 10s 110ms/step - loss: 0 .1826 - n_outputs0_loss: 0 .1056 - n_outputs1_loss: 0 .0770 - val_loss: 0 .2198 - val_n_outputs0_loss: 0 .1394 - val_n_outputs1_loss: 0 .0804 Epoch 14 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1771 - n_outputs0_loss: 0 .1009 - n_outputs1_loss: 0 .0762 Epoch 00014 : val_loss did not improve from 0 .21581 92 /92 [==============================] - 10s 111ms/step - loss: 0 .1771 - n_outputs0_loss: 0 .1009 - n_outputs1_loss: 0 .0762 - val_loss: 0 .2167 - val_n_outputs0_loss: 0 .1389 - val_n_outputs1_loss: 0 .0778 Epoch 15 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1676 - n_outputs0_loss: 0 .0959 - n_outputs1_loss: 0 .0718 Epoch 00015 : val_loss improved from 0 .21581 to 0 .20899, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 10s 112ms/step - loss: 0 .1676 - n_outputs0_loss: 0 .0959 - n_outputs1_loss: 0 .0718 - val_loss: 0 .2090 - val_n_outputs0_loss: 0 .1345 - val_n_outputs1_loss: 0 .0745 Epoch 16 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1608 - n_outputs0_loss: 0 .0910 - n_outputs1_loss: 0 .0698 Epoch 00016 : val_loss did not improve from 0 .20899 92 /92 [==============================] - 10s 110ms/step - loss: 0 .1608 - n_outputs0_loss: 0 .0910 - n_outputs1_loss: 0 .0698 - val_loss: 0 .2097 - val_n_outputs0_loss: 0 .1348 - val_n_outputs1_loss: 0 .0748 Epoch 17 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1534 - n_outputs0_loss: 0 .0870 - n_outputs1_loss: 0 .0664 Epoch 00017 : val_loss improved from 0 .20899 to 0 .20324, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 10s 112ms/step - loss: 0 .1534 - n_outputs0_loss: 0 .0870 - n_outputs1_loss: 0 .0664 - val_loss: 0 .2032 - val_n_outputs0_loss: 0 .1329 - val_n_outputs1_loss: 0 .0703 Epoch 18 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1490 - n_outputs0_loss: 0 .0846 - n_outputs1_loss: 0 .0644 Epoch 00018 : val_loss improved from 0 .20324 to 0 .19965, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 10s 111ms/step - loss: 0 .1490 - n_outputs0_loss: 0 .0846 - n_outputs1_loss: 0 .0644 - val_loss: 0 .1997 - val_n_outputs0_loss: 0 .1309 - val_n_outputs1_loss: 0 .0688 Epoch 19 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1452 - n_outputs0_loss: 0 .0828 - n_outputs1_loss: 0 .0624 Epoch 00019 : val_loss improved from 0 .19965 to 0 .19877, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 10s 111ms/step - loss: 0 .1452 - n_outputs0_loss: 0 .0828 - n_outputs1_loss: 0 .0624 - val_loss: 0 .1988 - val_n_outputs0_loss: 0 .1294 - val_n_outputs1_loss: 0 .0694 Epoch 20 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1353 - n_outputs0_loss: 0 .0747 - n_outputs1_loss: 0 .0606 Epoch 00020 : val_loss did not improve from 0 .19877 92 /92 [==============================] - 10s 110ms/step - loss: 0 .1353 - n_outputs0_loss: 0 .0747 - n_outputs1_loss: 0 .0606 - val_loss: 0 .2004 - val_n_outputs0_loss: 0 .1312 - val_n_outputs1_loss: 0 .0692 Epoch 21 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1319 - n_outputs0_loss: 0 .0731 - n_outputs1_loss: 0 .0588 Epoch 00021 : val_loss improved from 0 .19877 to 0 .19564, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 10s 111ms/step - loss: 0 .1319 - n_outputs0_loss: 0 .0731 - n_outputs1_loss: 0 .0588 - val_loss: 0 .1956 - val_n_outputs0_loss: 0 .1252 - val_n_outputs1_loss: 0 .0704 Epoch 22 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1299 - n_outputs0_loss: 0 .0713 - n_outputs1_loss: 0 .0585 Epoch 00022 : val_loss improved from 0 .19564 to 0 .19422, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 10s 111ms/step - loss: 0 .1299 - n_outputs0_loss: 0 .0713 - n_outputs1_loss: 0 .0585 - val_loss: 0 .1942 - val_n_outputs0_loss: 0 .1259 - val_n_outputs1_loss: 0 .0683 Epoch 23 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1231 - n_outputs0_loss: 0 .0684 - n_outputs1_loss: 0 .0548 Epoch 00023 : val_loss improved from 0 .19422 to 0 .19270, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 10s 111ms/step - loss: 0 .1231 - n_outputs0_loss: 0 .0684 - n_outputs1_loss: 0 .0548 - val_loss: 0 .1927 - val_n_outputs0_loss: 0 .1245 - val_n_outputs1_loss: 0 .0682 Epoch 24 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1239 - n_outputs0_loss: 0 .0673 - n_outputs1_loss: 0 .0566 Epoch 00024 : val_loss did not improve from 0 .19270 92 /92 [==============================] - 10s 110ms/step - loss: 0 .1239 - n_outputs0_loss: 0 .0673 - n_outputs1_loss: 0 .0566 - val_loss: 0 .1969 - val_n_outputs0_loss: 0 .1283 - val_n_outputs1_loss: 0 .0686 Epoch 25 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1200 - n_outputs0_loss: 0 .0650 - n_outputs1_loss: 0 .0550 Epoch 00025 : val_loss did not improve from 0 .19270 92 /92 [==============================] - 10s 111ms/step - loss: 0 .1200 - n_outputs0_loss: 0 .0650 - n_outputs1_loss: 0 .0550 - val_loss: 0 .1990 - val_n_outputs0_loss: 0 .1284 - val_n_outputs1_loss: 0 .0706 Epoch 26 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1171 - n_outputs0_loss: 0 .0636 - n_outputs1_loss: 0 .0535 Epoch 00026 : val_loss did not improve from 0 .19270 92 /92 [==============================] - 10s 111ms/step - loss: 0 .1171 - n_outputs0_loss: 0 .0636 - n_outputs1_loss: 0 .0535 - val_loss: 0 .1929 - val_n_outputs0_loss: 0 .1250 - val_n_outputs1_loss: 0 .0678 Epoch 27 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1167 - n_outputs0_loss: 0 .0638 - n_outputs1_loss: 0 .0529 Epoch 00027 : val_loss did not improve from 0 .19270 92 /92 [==============================] - 10s 112ms/step - loss: 0 .1167 - n_outputs0_loss: 0 .0638 - n_outputs1_loss: 0 .0529 - val_loss: 0 .1937 - val_n_outputs0_loss: 0 .1269 - val_n_outputs1_loss: 0 .0668 Epoch 28 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1123 - n_outputs0_loss: 0 .0610 - n_outputs1_loss: 0 .0513 Epoch 00028 : val_loss improved from 0 .19270 to 0 .19161, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 10s 112ms/step - loss: 0 .1123 - n_outputs0_loss: 0 .0610 - n_outputs1_loss: 0 .0513 - val_loss: 0 .1916 - val_n_outputs0_loss: 0 .1230 - val_n_outputs1_loss: 0 .0686 Epoch 29 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1086 - n_outputs0_loss: 0 .0584 - n_outputs1_loss: 0 .0501 Epoch 00029 : val_loss improved from 0 .19161 to 0 .18655, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 10s 110ms/step - loss: 0 .1086 - n_outputs0_loss: 0 .0584 - n_outputs1_loss: 0 .0501 - val_loss: 0 .1865 - val_n_outputs0_loss: 0 .1216 - val_n_outputs1_loss: 0 .0650 Epoch 30 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1093 - n_outputs0_loss: 0 .0593 - n_outputs1_loss: 0 .0500 Epoch 00030 : val_loss did not improve from 0 .18655 92 /92 [==============================] - 10s 109ms/step - loss: 0 .1093 - n_outputs0_loss: 0 .0593 - n_outputs1_loss: 0 .0500 - val_loss: 0 .1936 - val_n_outputs0_loss: 0 .1240 - val_n_outputs1_loss: 0 .0696 Epoch 31 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1077 - n_outputs0_loss: 0 .0578 - n_outputs1_loss: 0 .0499 Epoch 00031 : val_loss did not improve from 0 .18655 92 /92 [==============================] - 10s 110ms/step - loss: 0 .1077 - n_outputs0_loss: 0 .0578 - n_outputs1_loss: 0 .0499 - val_loss: 0 .1889 - val_n_outputs0_loss: 0 .1222 - val_n_outputs1_loss: 0 .0667 Epoch 32 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1026 - n_outputs0_loss: 0 .0551 - n_outputs1_loss: 0 .0475 Epoch 00032 : val_loss improved from 0 .18655 to 0 .18343, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 10s 111ms/step - loss: 0 .1026 - n_outputs0_loss: 0 .0551 - n_outputs1_loss: 0 .0475 - val_loss: 0 .1834 - val_n_outputs0_loss: 0 .1206 - val_n_outputs1_loss: 0 .0629 Epoch 33 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1022 - n_outputs0_loss: 0 .0545 - n_outputs1_loss: 0 .0477 Epoch 00033 : val_loss did not improve from 0 .18343 92 /92 [==============================] - 10s 110ms/step - loss: 0 .1022 - n_outputs0_loss: 0 .0545 - n_outputs1_loss: 0 .0477 - val_loss: 0 .1843 - val_n_outputs0_loss: 0 .1191 - val_n_outputs1_loss: 0 .0652 Epoch 34 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .0995 - n_outputs0_loss: 0 .0529 - n_outputs1_loss: 0 .0466 Epoch 00034 : val_loss improved from 0 .18343 to 0 .18117, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 10s 110ms/step - loss: 0 .0995 - n_outputs0_loss: 0 .0529 - n_outputs1_loss: 0 .0466 - val_loss: 0 .1812 - val_n_outputs0_loss: 0 .1166 - val_n_outputs1_loss: 0 .0646 Epoch 35 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .0989 - n_outputs0_loss: 0 .0526 - n_outputs1_loss: 0 .0463 Epoch 00035 : val_loss did not improve from 0 .18117 92 /92 [==============================] - 10s 110ms/step - loss: 0 .0989 - n_outputs0_loss: 0 .0526 - n_outputs1_loss: 0 .0463 - val_loss: 0 .1835 - val_n_outputs0_loss: 0 .1177 - val_n_outputs1_loss: 0 .0657 Epoch 36 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .0972 - n_outputs0_loss: 0 .0514 - n_outputs1_loss: 0 .0458 Epoch 00036 : val_loss did not improve from 0 .18117 92 /92 [==============================] - 10s 111ms/step - loss: 0 .0972 - n_outputs0_loss: 0 .0514 - n_outputs1_loss: 0 .0458 - val_loss: 0 .1838 - val_n_outputs0_loss: 0 .1198 - val_n_outputs1_loss: 0 .0641 Epoch 37 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .0959 - n_outputs0_loss: 0 .0509 - n_outputs1_loss: 0 .0450 Epoch 00037 : val_loss did not improve from 0 .18117 92 /92 [==============================] - 10s 109ms/step - loss: 0 .0959 - n_outputs0_loss: 0 .0509 - n_outputs1_loss: 0 .0450 - val_loss: 0 .1830 - val_n_outputs0_loss: 0 .1191 - val_n_outputs1_loss: 0 .0639 Epoch 38 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .0934 - n_outputs0_loss: 0 .0496 - n_outputs1_loss: 0 .0438 Epoch 00038 : val_loss did not improve from 0 .18117 92 /92 [==============================] - 10s 110ms/step - loss: 0 .0934 - n_outputs0_loss: 0 .0496 - n_outputs1_loss: 0 .0438 - val_loss: 0 .1845 - val_n_outputs0_loss: 0 .1185 - val_n_outputs1_loss: 0 .0660 Epoch 39 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .0923 - n_outputs0_loss: 0 .0477 - n_outputs1_loss: 0 .0446 Epoch 00039 : val_loss did not improve from 0 .18117 92 /92 [==============================] - 10s 110ms/step - loss: 0 .0923 - n_outputs0_loss: 0 .0477 - n_outputs1_loss: 0 .0446 - val_loss: 0 .1818 - val_n_outputs0_loss: 0 .1186 - val_n_outputs1_loss: 0 .0632 WARNING: CPU random generator seem to be failing, disable hardware random number generation WARNING: RDRND generated: 0xffffffff 0xffffffff 0xffffffff 0xffffffff ( donkey ) arl@arl1:~/mycar$ Checking the models 1 ls -l models/* returns 1 2 3 4 5 6 7 8 9 10 ls -l models/* -rw-r--r-- 1 arl arl 32317 Jul 26 20:30 models/database.json -rw-r--r-- 1 arl arl 35773936 Jul 26 20:17 models/msp-car-1-gpu.h5 -rw-r--r-- 1 arl arl 27506 Jul 26 20:17 models/msp-car-1-gpu.png -rw-r--r-- 1 arl arl 23659 Jul 26 19:57 models/msp-car-1.png -rw-r--r-- 1 arl arl 35773936 Jul 26 20:29 models/msp-car-2.h5 -rw-r--r-- 1 arl arl 25670 Jul 26 20:30 models/msp-car-2.png -rw-r--r-- 1 arl arl 22616 Feb 2 2020 models/mypilot.h5_loss_acc_0.040245.png -rw-r--r-- 1 arl arl 26687 Feb 2 2020 models/mypilot.h5_loss_acc_0.042222.png -rw-r--r-- 1 arl arl 11939744 Feb 2 2020 models/ref-model.h5","title":"MSP Car 2"},{"location":"training-logs/msp-car-2/#training-run-for-minneapolis-stem-partners","text":"Car #2 had 15045 images wc -l ~/mycar/data/msp-car-2/*.catalog 15045","title":"Training run for Minneapolis STEM Partners"},{"location":"training-logs/msp-car-2/#ls-1-mycardatamsp-car-2images-wc-l","text":"15045 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 ( donkey ) arl@arl1:~/mycar$ donkey train --tub = ./data/msp-car-2 --model = ./models/msp-car-2.h5 ________ ______ _________ ___ __ \\_ ______________ /___________ __ __ ____/_____ ________ __ / / / __ \\_ __ \\_ //_/ _ \\_ / / / _ / _ __ ` /_ ___/ _ /_/ // /_/ / / / / ,< / __/ /_/ / / /___ / /_/ /_ / /_____/ \\_ ___//_/ /_//_/ | _ | \\_ __/_ \\_ _, / \\_ ___/ \\_ _,_/ /_/ /____/ using donkey v4.2.1 ... loading config file: ./config.py loading personal config over-rides from myconfig.py \"get_model_by_type\" model Type is: linear Created KerasLinear 2021 -07-26 20 :22:54.320076: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcuda.so.1 2021 -07-26 20 :22:54.338339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981 ] successful NUMA node read from SysFS had negative value ( -1 ) , but there must be at least one NUMA node, so returning NUMA node zero 2021 -07-26 20 :22:54.338783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561 ] Found device 0 with properties: pciBusID: 0000 :09:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7 .5 coreClock: 1 .635GHz coreCount: 68 deviceMemorySize: 10 .76GiB deviceMemoryBandwidth: 573 .69GiB/s 2021 -07-26 20 :22:54.338925: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcudart.so.10.1 2021 -07-26 20 :22:54.339823: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcublas.so.10 2021 -07-26 20 :22:54.340834: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcufft.so.10 2021 -07-26 20 :22:54.340981: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcurand.so.10 2021 -07-26 20 :22:54.341775: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcusolver.so.10 2021 -07-26 20 :22:54.342170: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcusparse.so.10 2021 -07-26 20 :22:54.343898: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcudnn.so.7 2021 -07-26 20 :22:54.344043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981 ] successful NUMA node read from SysFS had negative value ( -1 ) , but there must be at least one NUMA node, so returning NUMA node zero 2021 -07-26 20 :22:54.344546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981 ] successful NUMA node read from SysFS had negative value ( -1 ) , but there must be at least one NUMA node, so returning NUMA node zero 2021 -07-26 20 :22:54.344933: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703 ] Adding visible gpu devices: 0 2021 -07-26 20 :22:54.345163: I tensorflow/core/platform/cpu_feature_guard.cc:143 ] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA 2021 -07-26 20 :22:54.349277: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102 ] CPU Frequency: 3592950000 Hz 2021 -07-26 20 :22:54.349572: I tensorflow/compiler/xla/service/service.cc:168 ] XLA service 0x5575e341a9f0 initialized for platform Host ( this does not guarantee that XLA will be used ) . Devices: 2021 -07-26 20 :22:54.349585: I tensorflow/compiler/xla/service/service.cc:176 ] StreamExecutor device ( 0 ) : Host, Default Version 2021 -07-26 20 :22:54.349717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981 ] successful NUMA node read from SysFS had negative value ( -1 ) , but there must be at least one NUMA node, so returning NUMA node zero 2021 -07-26 20 :22:54.350124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561 ] Found device 0 with properties: pciBusID: 0000 :09:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7 .5 coreClock: 1 .635GHz coreCount: 68 deviceMemorySize: 10 .76GiB deviceMemoryBandwidth: 573 .69GiB/s 2021 -07-26 20 :22:54.350160: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcudart.so.10.1 2021 -07-26 20 :22:54.350171: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcublas.so.10 2021 -07-26 20 :22:54.350180: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcufft.so.10 2021 -07-26 20 :22:54.350191: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcurand.so.10 2021 -07-26 20 :22:54.350200: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcusolver.so.10 2021 -07-26 20 :22:54.350210: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcusparse.so.10 2021 -07-26 20 :22:54.350220: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcudnn.so.7 2021 -07-26 20 :22:54.350282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981 ] successful NUMA node read from SysFS had negative value ( -1 ) , but there must be at least one NUMA node, so returning NUMA node zero 2021 -07-26 20 :22:54.350723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981 ] successful NUMA node read from SysFS had negative value ( -1 ) , but there must be at least one NUMA node, so returning NUMA node zero 2021 -07-26 20 :22:54.351106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703 ] Adding visible gpu devices: 0 2021 -07-26 20 :22:54.351127: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcudart.so.10.1 2021 -07-26 20 :22:54.423106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102 ] Device interconnect StreamExecutor with strength 1 edge matrix: 2021 -07-26 20 :22:54.423133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108 ] 0 2021 -07-26 20 :22:54.423138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121 ] 0 : N 2021 -07-26 20 :22:54.423354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981 ] successful NUMA node read from SysFS had negative value ( -1 ) , but there must be at least one NUMA node, so returning NUMA node zero 2021 -07-26 20 :22:54.423819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981 ] successful NUMA node read from SysFS had negative value ( -1 ) , but there must be at least one NUMA node, so returning NUMA node zero 2021 -07-26 20 :22:54.424248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981 ] successful NUMA node read from SysFS had negative value ( -1 ) , but there must be at least one NUMA node, so returning NUMA node zero 2021 -07-26 20 :22:54.424632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247 ] Created TensorFlow device ( /job:localhost/replica:0/task:0/device:GPU:0 with 9890 MB memory ) -> physical GPU ( device: 0 , name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000 :09:00.0, compute capability: 7 .5 ) 2021 -07-26 20 :22:54.425999: I tensorflow/compiler/xla/service/service.cc:168 ] XLA service 0x5575e52b18b0 initialized for platform CUDA ( this does not guarantee that XLA will be used ) . Devices: 2021 -07-26 20 :22:54.426009: I tensorflow/compiler/xla/service/service.cc:176 ] StreamExecutor device ( 0 ) : NVIDIA GeForce RTX 2080 Ti, Compute Capability 7 .5 Model: \"model\" __________________________________________________________________________________________________ Layer ( type ) Output Shape Param # Connected to ================================================================================================== img_in ( InputLayer ) [( None, 224 , 224 , 3 ) 0 __________________________________________________________________________________________________ conv2d_1 ( Conv2D ) ( None, 110 , 110 , 24 ) 1824 img_in [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout ( Dropout ) ( None, 110 , 110 , 24 ) 0 conv2d_1 [ 0 ][ 0 ] __________________________________________________________________________________________________ conv2d_2 ( Conv2D ) ( None, 53 , 53 , 32 ) 19232 dropout [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_1 ( Dropout ) ( None, 53 , 53 , 32 ) 0 conv2d_2 [ 0 ][ 0 ] __________________________________________________________________________________________________ conv2d_3 ( Conv2D ) ( None, 25 , 25 , 64 ) 51264 dropout_1 [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_2 ( Dropout ) ( None, 25 , 25 , 64 ) 0 conv2d_3 [ 0 ][ 0 ] __________________________________________________________________________________________________ conv2d_4 ( Conv2D ) ( None, 23 , 23 , 64 ) 36928 dropout_2 [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_3 ( Dropout ) ( None, 23 , 23 , 64 ) 0 conv2d_4 [ 0 ][ 0 ] __________________________________________________________________________________________________ conv2d_5 ( Conv2D ) ( None, 21 , 21 , 64 ) 36928 dropout_3 [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_4 ( Dropout ) ( None, 21 , 21 , 64 ) 0 conv2d_5 [ 0 ][ 0 ] __________________________________________________________________________________________________ flattened ( Flatten ) ( None, 28224 ) 0 dropout_4 [ 0 ][ 0 ] __________________________________________________________________________________________________ dense_1 ( Dense ) ( None, 100 ) 2822500 flattened [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_5 ( Dropout ) ( None, 100 ) 0 dense_1 [ 0 ][ 0 ] __________________________________________________________________________________________________ dense_2 ( Dense ) ( None, 50 ) 5050 dropout_5 [ 0 ][ 0 ] __________________________________________________________________________________________________ dropout_6 ( Dropout ) ( None, 50 ) 0 dense_2 [ 0 ][ 0 ] __________________________________________________________________________________________________ n_outputs0 ( Dense ) ( None, 1 ) 51 dropout_6 [ 0 ][ 0 ] __________________________________________________________________________________________________ n_outputs1 ( Dense ) ( None, 1 ) 51 dropout_6 [ 0 ][ 0 ] ================================================================================================== Total params: 2 ,973,828 Trainable params: 2 ,973,828 Non-trainable params: 0 __________________________________________________________________________________________________ None Using catalog /home/arl/mycar/data/msp-car-2/catalog_22.catalog Records # Training 11696 Records # Validation 2924 Epoch 1 /100 2021 -07-26 20 :22:55.471623: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcublas.so.10 2021 -07-26 20 :22:55.802565: I tensorflow/stream_executor/platform/default/dso_loader.cc:44 ] Successfully opened dynamic library libcudnn.so.7 2021 -07-26 20 :22:56.515413: W tensorflow/stream_executor/gpu/asm_compiler.cc:116 ] *** WARNING *** You are using ptxas 9 .1.108, which is older than 9 .2.88. ptxas 9 .x before 9 .2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors. You do not need to update to CUDA 9 .2.88 ; cherry-picking the ptxas binary is sufficient. 2021 -07-26 20 :22:56.559204: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314 ] Internal: ptxas exited with non-zero error code 65280 , output: ptxas fatal : Value 'sm_75' is not defined for option 'gpu-name' Relying on driver to perform ptx compilation. Modify $PATH to customize ptxas location. This message will be only logged once. 92 /92 [==============================] - ETA: 0s - loss: 0 .6304 - n_outputs0_loss: 0 .3238 - n_outputs1_loss: 0 .3066 Epoch 00001 : val_loss improved from inf to 0 .59133, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 29s 310ms/step - loss: 0 .6304 - n_outputs0_loss: 0 .3238 - n_outputs1_loss: 0 .3066 - val_loss: 0 .5913 - val_n_outputs0_loss: 0 .3121 - val_n_outputs1_loss: 0 .2793 Epoch 2 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .5150 - n_outputs0_loss: 0 .2730 - n_outputs1_loss: 0 .2419 Epoch 00002 : val_loss improved from 0 .59133 to 0 .39368, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 11s 117ms/step - loss: 0 .5150 - n_outputs0_loss: 0 .2730 - n_outputs1_loss: 0 .2419 - val_loss: 0 .3937 - val_n_outputs0_loss: 0 .2108 - val_n_outputs1_loss: 0 .1828 Epoch 3 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .3885 - n_outputs0_loss: 0 .2088 - n_outputs1_loss: 0 .1797 Epoch 00003 : val_loss improved from 0 .39368 to 0 .34087, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 11s 117ms/step - loss: 0 .3885 - n_outputs0_loss: 0 .2088 - n_outputs1_loss: 0 .1797 - val_loss: 0 .3409 - val_n_outputs0_loss: 0 .1923 - val_n_outputs1_loss: 0 .1486 Epoch 4 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .3449 - n_outputs0_loss: 0 .1870 - n_outputs1_loss: 0 .1579 Epoch 00004 : val_loss improved from 0 .34087 to 0 .30588, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 11s 119ms/step - loss: 0 .3449 - n_outputs0_loss: 0 .1870 - n_outputs1_loss: 0 .1579 - val_loss: 0 .3059 - val_n_outputs0_loss: 0 .1771 - val_n_outputs1_loss: 0 .1288 Epoch 5 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .3161 - n_outputs0_loss: 0 .1763 - n_outputs1_loss: 0 .1397 Epoch 00005 : val_loss improved from 0 .30588 to 0 .28650, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 10s 113ms/step - loss: 0 .3161 - n_outputs0_loss: 0 .1763 - n_outputs1_loss: 0 .1397 - val_loss: 0 .2865 - val_n_outputs0_loss: 0 .1722 - val_n_outputs1_loss: 0 .1143 Epoch 6 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .2876 - n_outputs0_loss: 0 .1633 - n_outputs1_loss: 0 .1243 Epoch 00006 : val_loss improved from 0 .28650 to 0 .26754, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 10s 110ms/step - loss: 0 .2876 - n_outputs0_loss: 0 .1633 - n_outputs1_loss: 0 .1243 - val_loss: 0 .2675 - val_n_outputs0_loss: 0 .1623 - val_n_outputs1_loss: 0 .1053 Epoch 7 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .2612 - n_outputs0_loss: 0 .1508 - n_outputs1_loss: 0 .1103 Epoch 00007 : val_loss improved from 0 .26754 to 0 .25034, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 10s 111ms/step - loss: 0 .2612 - n_outputs0_loss: 0 .1508 - n_outputs1_loss: 0 .1103 - val_loss: 0 .2503 - val_n_outputs0_loss: 0 .1551 - val_n_outputs1_loss: 0 .0952 Epoch 8 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .2476 - n_outputs0_loss: 0 .1435 - n_outputs1_loss: 0 .1041 Epoch 00008 : val_loss improved from 0 .25034 to 0 .24291, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 10s 109ms/step - loss: 0 .2476 - n_outputs0_loss: 0 .1435 - n_outputs1_loss: 0 .1041 - val_loss: 0 .2429 - val_n_outputs0_loss: 0 .1524 - val_n_outputs1_loss: 0 .0905 Epoch 9 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .2283 - n_outputs0_loss: 0 .1323 - n_outputs1_loss: 0 .0960 Epoch 00009 : val_loss improved from 0 .24291 to 0 .22718, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 10s 110ms/step - loss: 0 .2283 - n_outputs0_loss: 0 .1323 - n_outputs1_loss: 0 .0960 - val_loss: 0 .2272 - val_n_outputs0_loss: 0 .1450 - val_n_outputs1_loss: 0 .0821 Epoch 10 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .2183 - n_outputs0_loss: 0 .1267 - n_outputs1_loss: 0 .0916 Epoch 00010 : val_loss did not improve from 0 .22718 92 /92 [==============================] - 10s 109ms/step - loss: 0 .2183 - n_outputs0_loss: 0 .1267 - n_outputs1_loss: 0 .0916 - val_loss: 0 .2305 - val_n_outputs0_loss: 0 .1471 - val_n_outputs1_loss: 0 .0834 Epoch 11 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .2022 - n_outputs0_loss: 0 .1187 - n_outputs1_loss: 0 .0835 Epoch 00011 : val_loss improved from 0 .22718 to 0 .21581, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 10s 110ms/step - loss: 0 .2022 - n_outputs0_loss: 0 .1187 - n_outputs1_loss: 0 .0835 - val_loss: 0 .2158 - val_n_outputs0_loss: 0 .1375 - val_n_outputs1_loss: 0 .0783 Epoch 12 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1921 - n_outputs0_loss: 0 .1085 - n_outputs1_loss: 0 .0836 Epoch 00012 : val_loss did not improve from 0 .21581 92 /92 [==============================] - 10s 110ms/step - loss: 0 .1921 - n_outputs0_loss: 0 .1085 - n_outputs1_loss: 0 .0836 - val_loss: 0 .2185 - val_n_outputs0_loss: 0 .1382 - val_n_outputs1_loss: 0 .0802 Epoch 13 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1826 - n_outputs0_loss: 0 .1056 - n_outputs1_loss: 0 .0770 Epoch 00013 : val_loss did not improve from 0 .21581 92 /92 [==============================] - 10s 110ms/step - loss: 0 .1826 - n_outputs0_loss: 0 .1056 - n_outputs1_loss: 0 .0770 - val_loss: 0 .2198 - val_n_outputs0_loss: 0 .1394 - val_n_outputs1_loss: 0 .0804 Epoch 14 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1771 - n_outputs0_loss: 0 .1009 - n_outputs1_loss: 0 .0762 Epoch 00014 : val_loss did not improve from 0 .21581 92 /92 [==============================] - 10s 111ms/step - loss: 0 .1771 - n_outputs0_loss: 0 .1009 - n_outputs1_loss: 0 .0762 - val_loss: 0 .2167 - val_n_outputs0_loss: 0 .1389 - val_n_outputs1_loss: 0 .0778 Epoch 15 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1676 - n_outputs0_loss: 0 .0959 - n_outputs1_loss: 0 .0718 Epoch 00015 : val_loss improved from 0 .21581 to 0 .20899, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 10s 112ms/step - loss: 0 .1676 - n_outputs0_loss: 0 .0959 - n_outputs1_loss: 0 .0718 - val_loss: 0 .2090 - val_n_outputs0_loss: 0 .1345 - val_n_outputs1_loss: 0 .0745 Epoch 16 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1608 - n_outputs0_loss: 0 .0910 - n_outputs1_loss: 0 .0698 Epoch 00016 : val_loss did not improve from 0 .20899 92 /92 [==============================] - 10s 110ms/step - loss: 0 .1608 - n_outputs0_loss: 0 .0910 - n_outputs1_loss: 0 .0698 - val_loss: 0 .2097 - val_n_outputs0_loss: 0 .1348 - val_n_outputs1_loss: 0 .0748 Epoch 17 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1534 - n_outputs0_loss: 0 .0870 - n_outputs1_loss: 0 .0664 Epoch 00017 : val_loss improved from 0 .20899 to 0 .20324, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 10s 112ms/step - loss: 0 .1534 - n_outputs0_loss: 0 .0870 - n_outputs1_loss: 0 .0664 - val_loss: 0 .2032 - val_n_outputs0_loss: 0 .1329 - val_n_outputs1_loss: 0 .0703 Epoch 18 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1490 - n_outputs0_loss: 0 .0846 - n_outputs1_loss: 0 .0644 Epoch 00018 : val_loss improved from 0 .20324 to 0 .19965, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 10s 111ms/step - loss: 0 .1490 - n_outputs0_loss: 0 .0846 - n_outputs1_loss: 0 .0644 - val_loss: 0 .1997 - val_n_outputs0_loss: 0 .1309 - val_n_outputs1_loss: 0 .0688 Epoch 19 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1452 - n_outputs0_loss: 0 .0828 - n_outputs1_loss: 0 .0624 Epoch 00019 : val_loss improved from 0 .19965 to 0 .19877, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 10s 111ms/step - loss: 0 .1452 - n_outputs0_loss: 0 .0828 - n_outputs1_loss: 0 .0624 - val_loss: 0 .1988 - val_n_outputs0_loss: 0 .1294 - val_n_outputs1_loss: 0 .0694 Epoch 20 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1353 - n_outputs0_loss: 0 .0747 - n_outputs1_loss: 0 .0606 Epoch 00020 : val_loss did not improve from 0 .19877 92 /92 [==============================] - 10s 110ms/step - loss: 0 .1353 - n_outputs0_loss: 0 .0747 - n_outputs1_loss: 0 .0606 - val_loss: 0 .2004 - val_n_outputs0_loss: 0 .1312 - val_n_outputs1_loss: 0 .0692 Epoch 21 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1319 - n_outputs0_loss: 0 .0731 - n_outputs1_loss: 0 .0588 Epoch 00021 : val_loss improved from 0 .19877 to 0 .19564, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 10s 111ms/step - loss: 0 .1319 - n_outputs0_loss: 0 .0731 - n_outputs1_loss: 0 .0588 - val_loss: 0 .1956 - val_n_outputs0_loss: 0 .1252 - val_n_outputs1_loss: 0 .0704 Epoch 22 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1299 - n_outputs0_loss: 0 .0713 - n_outputs1_loss: 0 .0585 Epoch 00022 : val_loss improved from 0 .19564 to 0 .19422, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 10s 111ms/step - loss: 0 .1299 - n_outputs0_loss: 0 .0713 - n_outputs1_loss: 0 .0585 - val_loss: 0 .1942 - val_n_outputs0_loss: 0 .1259 - val_n_outputs1_loss: 0 .0683 Epoch 23 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1231 - n_outputs0_loss: 0 .0684 - n_outputs1_loss: 0 .0548 Epoch 00023 : val_loss improved from 0 .19422 to 0 .19270, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 10s 111ms/step - loss: 0 .1231 - n_outputs0_loss: 0 .0684 - n_outputs1_loss: 0 .0548 - val_loss: 0 .1927 - val_n_outputs0_loss: 0 .1245 - val_n_outputs1_loss: 0 .0682 Epoch 24 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1239 - n_outputs0_loss: 0 .0673 - n_outputs1_loss: 0 .0566 Epoch 00024 : val_loss did not improve from 0 .19270 92 /92 [==============================] - 10s 110ms/step - loss: 0 .1239 - n_outputs0_loss: 0 .0673 - n_outputs1_loss: 0 .0566 - val_loss: 0 .1969 - val_n_outputs0_loss: 0 .1283 - val_n_outputs1_loss: 0 .0686 Epoch 25 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1200 - n_outputs0_loss: 0 .0650 - n_outputs1_loss: 0 .0550 Epoch 00025 : val_loss did not improve from 0 .19270 92 /92 [==============================] - 10s 111ms/step - loss: 0 .1200 - n_outputs0_loss: 0 .0650 - n_outputs1_loss: 0 .0550 - val_loss: 0 .1990 - val_n_outputs0_loss: 0 .1284 - val_n_outputs1_loss: 0 .0706 Epoch 26 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1171 - n_outputs0_loss: 0 .0636 - n_outputs1_loss: 0 .0535 Epoch 00026 : val_loss did not improve from 0 .19270 92 /92 [==============================] - 10s 111ms/step - loss: 0 .1171 - n_outputs0_loss: 0 .0636 - n_outputs1_loss: 0 .0535 - val_loss: 0 .1929 - val_n_outputs0_loss: 0 .1250 - val_n_outputs1_loss: 0 .0678 Epoch 27 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1167 - n_outputs0_loss: 0 .0638 - n_outputs1_loss: 0 .0529 Epoch 00027 : val_loss did not improve from 0 .19270 92 /92 [==============================] - 10s 112ms/step - loss: 0 .1167 - n_outputs0_loss: 0 .0638 - n_outputs1_loss: 0 .0529 - val_loss: 0 .1937 - val_n_outputs0_loss: 0 .1269 - val_n_outputs1_loss: 0 .0668 Epoch 28 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1123 - n_outputs0_loss: 0 .0610 - n_outputs1_loss: 0 .0513 Epoch 00028 : val_loss improved from 0 .19270 to 0 .19161, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 10s 112ms/step - loss: 0 .1123 - n_outputs0_loss: 0 .0610 - n_outputs1_loss: 0 .0513 - val_loss: 0 .1916 - val_n_outputs0_loss: 0 .1230 - val_n_outputs1_loss: 0 .0686 Epoch 29 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1086 - n_outputs0_loss: 0 .0584 - n_outputs1_loss: 0 .0501 Epoch 00029 : val_loss improved from 0 .19161 to 0 .18655, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 10s 110ms/step - loss: 0 .1086 - n_outputs0_loss: 0 .0584 - n_outputs1_loss: 0 .0501 - val_loss: 0 .1865 - val_n_outputs0_loss: 0 .1216 - val_n_outputs1_loss: 0 .0650 Epoch 30 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1093 - n_outputs0_loss: 0 .0593 - n_outputs1_loss: 0 .0500 Epoch 00030 : val_loss did not improve from 0 .18655 92 /92 [==============================] - 10s 109ms/step - loss: 0 .1093 - n_outputs0_loss: 0 .0593 - n_outputs1_loss: 0 .0500 - val_loss: 0 .1936 - val_n_outputs0_loss: 0 .1240 - val_n_outputs1_loss: 0 .0696 Epoch 31 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1077 - n_outputs0_loss: 0 .0578 - n_outputs1_loss: 0 .0499 Epoch 00031 : val_loss did not improve from 0 .18655 92 /92 [==============================] - 10s 110ms/step - loss: 0 .1077 - n_outputs0_loss: 0 .0578 - n_outputs1_loss: 0 .0499 - val_loss: 0 .1889 - val_n_outputs0_loss: 0 .1222 - val_n_outputs1_loss: 0 .0667 Epoch 32 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1026 - n_outputs0_loss: 0 .0551 - n_outputs1_loss: 0 .0475 Epoch 00032 : val_loss improved from 0 .18655 to 0 .18343, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 10s 111ms/step - loss: 0 .1026 - n_outputs0_loss: 0 .0551 - n_outputs1_loss: 0 .0475 - val_loss: 0 .1834 - val_n_outputs0_loss: 0 .1206 - val_n_outputs1_loss: 0 .0629 Epoch 33 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .1022 - n_outputs0_loss: 0 .0545 - n_outputs1_loss: 0 .0477 Epoch 00033 : val_loss did not improve from 0 .18343 92 /92 [==============================] - 10s 110ms/step - loss: 0 .1022 - n_outputs0_loss: 0 .0545 - n_outputs1_loss: 0 .0477 - val_loss: 0 .1843 - val_n_outputs0_loss: 0 .1191 - val_n_outputs1_loss: 0 .0652 Epoch 34 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .0995 - n_outputs0_loss: 0 .0529 - n_outputs1_loss: 0 .0466 Epoch 00034 : val_loss improved from 0 .18343 to 0 .18117, saving model to ./models/msp-car-2.h5 92 /92 [==============================] - 10s 110ms/step - loss: 0 .0995 - n_outputs0_loss: 0 .0529 - n_outputs1_loss: 0 .0466 - val_loss: 0 .1812 - val_n_outputs0_loss: 0 .1166 - val_n_outputs1_loss: 0 .0646 Epoch 35 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .0989 - n_outputs0_loss: 0 .0526 - n_outputs1_loss: 0 .0463 Epoch 00035 : val_loss did not improve from 0 .18117 92 /92 [==============================] - 10s 110ms/step - loss: 0 .0989 - n_outputs0_loss: 0 .0526 - n_outputs1_loss: 0 .0463 - val_loss: 0 .1835 - val_n_outputs0_loss: 0 .1177 - val_n_outputs1_loss: 0 .0657 Epoch 36 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .0972 - n_outputs0_loss: 0 .0514 - n_outputs1_loss: 0 .0458 Epoch 00036 : val_loss did not improve from 0 .18117 92 /92 [==============================] - 10s 111ms/step - loss: 0 .0972 - n_outputs0_loss: 0 .0514 - n_outputs1_loss: 0 .0458 - val_loss: 0 .1838 - val_n_outputs0_loss: 0 .1198 - val_n_outputs1_loss: 0 .0641 Epoch 37 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .0959 - n_outputs0_loss: 0 .0509 - n_outputs1_loss: 0 .0450 Epoch 00037 : val_loss did not improve from 0 .18117 92 /92 [==============================] - 10s 109ms/step - loss: 0 .0959 - n_outputs0_loss: 0 .0509 - n_outputs1_loss: 0 .0450 - val_loss: 0 .1830 - val_n_outputs0_loss: 0 .1191 - val_n_outputs1_loss: 0 .0639 Epoch 38 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .0934 - n_outputs0_loss: 0 .0496 - n_outputs1_loss: 0 .0438 Epoch 00038 : val_loss did not improve from 0 .18117 92 /92 [==============================] - 10s 110ms/step - loss: 0 .0934 - n_outputs0_loss: 0 .0496 - n_outputs1_loss: 0 .0438 - val_loss: 0 .1845 - val_n_outputs0_loss: 0 .1185 - val_n_outputs1_loss: 0 .0660 Epoch 39 /100 92 /92 [==============================] - ETA: 0s - loss: 0 .0923 - n_outputs0_loss: 0 .0477 - n_outputs1_loss: 0 .0446 Epoch 00039 : val_loss did not improve from 0 .18117 92 /92 [==============================] - 10s 110ms/step - loss: 0 .0923 - n_outputs0_loss: 0 .0477 - n_outputs1_loss: 0 .0446 - val_loss: 0 .1818 - val_n_outputs0_loss: 0 .1186 - val_n_outputs1_loss: 0 .0632 WARNING: CPU random generator seem to be failing, disable hardware random number generation WARNING: RDRND generated: 0xffffffff 0xffffffff 0xffffffff 0xffffffff ( donkey ) arl@arl1:~/mycar$","title":"ls -1 ~/mycar/data/msp-car-2/images | wc -l"},{"location":"training-logs/msp-car-2/#checking-the-models","text":"1 ls -l models/* returns 1 2 3 4 5 6 7 8 9 10 ls -l models/* -rw-r--r-- 1 arl arl 32317 Jul 26 20:30 models/database.json -rw-r--r-- 1 arl arl 35773936 Jul 26 20:17 models/msp-car-1-gpu.h5 -rw-r--r-- 1 arl arl 27506 Jul 26 20:17 models/msp-car-1-gpu.png -rw-r--r-- 1 arl arl 23659 Jul 26 19:57 models/msp-car-1.png -rw-r--r-- 1 arl arl 35773936 Jul 26 20:29 models/msp-car-2.h5 -rw-r--r-- 1 arl arl 25670 Jul 26 20:30 models/msp-car-2.png -rw-r--r-- 1 arl arl 22616 Feb 2 2020 models/mypilot.h5_loss_acc_0.040245.png -rw-r--r-- 1 arl arl 26687 Feb 2 2020 models/mypilot.h5_loss_acc_0.042222.png -rw-r--r-- 1 arl arl 11939744 Feb 2 2020 models/ref-model.h5","title":"Checking the models"}]}