{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the AI Racing League","text":"<p>Promoting equity and innovation in AI education.</p> <p></p> <p>The AI Racing League is a fun way to learn the concepts behind artificial intelligence!  We learn AI by teaching small remote-controlled (RC) cars to drive around a track autonomously.  The cars use a low-cost Raspberry Pi computer (or NVIDIA Nano) with a camera.  Students drive around the track and gather a set of images to \"train\" a neural network that can be used to automatically steer the car.</p> <p>In addition to teaching machine learning, this course also teaches concepts like Python programming, computer vision, data science and generative AI. Our curriculum is inspired by the DonkeyCar and the CoderDojo mentoring system.  We feature a wide variety of lesson plans from the 5th grade up to college-level participants.</p> <p>Our secret sauce is to combine the right hardware, software, mentors and a flexible learning curriculum to create fun events that students love to participate in.</p> <p>Notice</p> <p>We are terminating our support for the NVIDIA Nano.  This is driven by the fact that the Raspberry Pi 4 with 8GB is selling at $75 and the Nano 4GB is now $150.  There are considerable problems with the end-of-life of the NVIDIA support for the JetPack libraries.</p>"},{"location":"#current-status","title":"Current Status","text":"<p>We are in the process of restarting our events starting in the fall of 2023.  Please contact Dan McCreary if you would like to participate.</p> <p>The following organizations have expressed an interest in helping out:</p> <ol> <li>Code Savvy - Contact: Valarie Lockhart - The AI Racing League runs as a project under Code Savvy.  They are a 503C not-for-profit organization.</li> <li>Minneanalytics - Contact: Dan Atkins - contact Dan Atkins directly if you are looking for funding for your team.</li> <li>Washburn High School - Contact: Peter Grul</li> <li>Best Buy Education - Contact: Wes Strait - Wes and his team have run several events using the Amazon deep racer.</li> <li>Minnesota STEM Partnership - Contact: Michael Wulf - Michael created his first AI Racing League team in the summer of 2021.</li> <li>Create Minneapolis Contacts: Krista Stommes and Rob</li> </ol>"},{"location":"#mission","title":"Mission","text":"<p>The mission of the AI Racing League is to create and deliver educational materials that will make fun AI training accessible to everyone. We place a special focus on students from disadvantaged communities including women and minorities.  We work as a sub-project of the CodeSavvy not-for-profit organization and we adhere to their guidelines for the quality and security of our students.  This means that all our volunteers have background checks and we limit the student-to-mentor ratios to no more than three students per mentor.  We are committed to equal-opportunity mentoring.  We strive to recruit, train and retain the best mentors we can find.</p> <p>We are inspired by the values behind the CoderDojo mentoring system and their innovative use of flexible concept cards.  We attempt to publish concept cards that provide a flexible and agile training environment for a wide variety of learners.</p> <p>We believe strongly in student-led initiatives and project-based learning.  We feel students learn the most when they are building things together in teams.  We believe our curriculum should be broad to support a wide variety of students from begging Python to advanced AI.</p> <p>Rather than force students down a single path of learning, we believe our instructors should be more like travel guides to help students explore their areas of interest.  Our curriculum needs to adapt to single-hour events up to multi-year mentoring.  See Rhizomatic Learning for what inspires us.</p>"},{"location":"#checkout-our-other-sites","title":"Checkout Our Other Sites:","text":"<ul> <li>AI Racing League</li> <li>Beginning Python</li> <li>[MBOT]</li> <li>MicroPython for Kids</li> <li>ChatGPT for Teachers</li> <li>AI for Kids</li> <li>MBOT</li> <li>IoT Hackday - costume contest</li> </ul>"},{"location":"#other-resources","title":"Other Resources","text":"<ul> <li>Education Material: If you would like to teach AI Racing in the classroom, at a meetup or even in a corporation check out our resources here</li> <li>Resources: Want to connect or contribute to the community check us out here</li> <li>About us: Want to know more about us? Check us out</li> </ul>"},{"location":"about/","title":"About the AI Racing League","text":"<p>The AI Racing League is a fun way to learn the concepts behind artificial intelligence!  We learn AI by teaching small remote-controlled (RC) cars to drive around a track autonomously.  The cars use a low-cost Raspberry Pi computer (or NVIDIA Nano) with a camera.  Students drive around the track and gather a set of images to \"train\" a neural network that can be used to automatically steer the car.</p> <p>In addition to teaching machine learning, this course also teaches concepts like Python programming, computer vision, data science and generative AI. Our curriculum is inspired by the DonkeyCar and the CoderDojo mentoring system.  We feature a wide variety of lesson plans from the 5th grade up to college-level participants.</p> <p>Our secret sauce is to combine the right hardware, software, mentors and a flexible learning curriculum to create fun events that students love to participate in.</p>"},{"location":"about/#diy-robocars-and-donkey-cars","title":"DIY Robocars and Donkey Cars","text":"<p>DIY Robocars and the Donkey Cars are the communities that inspired the AI Racing League.  We are grateful for all they have done and their willingness to share their knowledge through open source software.</p>"},{"location":"about/#coder-dojo","title":"Coder Dojo","text":"<p>A community of over 2,300 free, open and local programming clubs for young people 58,000 young people are being creative with technology with the help of 12,000 volunteers in 94 countries</p> <ul> <li>Visit https://coderdojo.com</li> </ul>"},{"location":"about/#code-savvy","title":"Code Savvy","text":"<p>Code Savvy strives to make kids and teens more code-savvy through creative educational programs and services. We incubate and support community-based programs that bring technology and know-how to local kids and educators, all the while championing gender and ethnic diversity. Code Savvy is dedicated to ensuring the next generation of computer science professionals represents the billions of users of tomorrow\u2019s innovative technologies. Visit https://codesavvy.org</p>"},{"location":"about/#licensing","title":"Licensing","text":"<p>Like all CoderDojo-created content, you are free to use this content in K-12 noncommercial educational settings for teaching without paying license fees.  We also encourage our community to create variations and help us enlarge the curriculum.  We always appreciate attribution!</p> <p>Details of the license terms are here:</p> <p>Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)</p>"},{"location":"ai-assisted-learning/","title":"AI Assisted Learning Call To Action","text":"<p>ChatGPT Prompt: How Minnesota Can Enhance the K-12 Curriculum to Create More Tech Workers</p>"},{"location":"ai-assisted-learning/#1-early-introduction-to-computer-science","title":"1. Early Introduction to Computer Science","text":"<ul> <li>Start with basic coding and computer literacy classes in elementary school.</li> <li>Familiarize children with tech concepts from a young age.</li> </ul>"},{"location":"ai-assisted-learning/#2-enhanced-tech-curriculum","title":"2. Enhanced Tech Curriculum","text":"<ul> <li>Offer courses in programming languages, web development, app development, data science, and more during middle and high school.</li> </ul>"},{"location":"ai-assisted-learning/#3-project-based-learning","title":"3. Project-Based Learning","text":"<ul> <li>Introduce tech-based projects for students to solve real-world problems.</li> <li>Promote practical experience and real-world application understanding.</li> </ul>"},{"location":"ai-assisted-learning/#4-tech-clubs-and-competitions","title":"4. Tech Clubs and Competitions","text":"<ul> <li>Encourage extracurricular activities like coding clubs and robotics teams.</li> <li>Promote participation in national and international tech competitions.</li> </ul>"},{"location":"ai-assisted-learning/#5-professional-development-for-teachers","title":"5. Professional Development for Teachers","text":"<ul> <li>Regularly train teachers on the latest tech advancements.</li> <li>Ensure effective teaching of tech concepts to students.</li> </ul>"},{"location":"ai-assisted-learning/#6-partnerships-with-tech-companies","title":"6. Partnerships with Tech Companies","text":"<ul> <li>Collaborate with tech companies for internships, mentorship programs, and guest lectures.</li> <li>Expose students to the tech industry and potential career paths.</li> </ul>"},{"location":"ai-assisted-learning/#7-tech-career-awareness","title":"7. Tech Career Awareness","text":"<ul> <li>Introduce students to various tech careers, from software engineering to UI/UX design to cybersecurity.</li> <li>Organize career fairs and talks.</li> </ul>"},{"location":"ai-assisted-learning/#8-incorporate-soft-skills","title":"8. Incorporate Soft Skills","text":"<ul> <li>Focus on soft skills like problem-solving, critical thinking, teamwork, and communication.</li> <li>Highlight their importance in the tech industry.</li> </ul>"},{"location":"ai-assisted-learning/#9-diversity-and-inclusion","title":"9. Diversity and Inclusion","text":"<ul> <li>Ensure tech education promotes diversity and is inclusive.</li> <li>Reach out to underrepresented groups in tech, including girls, minority students, and those from low-income backgrounds.</li> </ul>"},{"location":"ai-assisted-learning/#10-update-infrastructure","title":"10. Update Infrastructure","text":"<ul> <li>Ensure schools are equipped with updated computers, software, and fast internet connectivity.</li> <li>Support a robust tech education environment.</li> </ul>"},{"location":"ai-assisted-learning/#11-hands-on-experiences","title":"11. Hands-on Experiences","text":"<ul> <li>Organize field trips to tech companies, hackathons, and workshops.</li> <li>Provide practical experiences outside the classroom.</li> </ul>"},{"location":"ai-assisted-learning/#12-online-resources","title":"12. Online Resources","text":"<ul> <li>Provide access to online platforms for self-paced learning, coding practice, and challenges.</li> </ul>"},{"location":"ai-assisted-learning/#13-continuous-evaluation","title":"13. Continuous Evaluation","text":"<ul> <li>Regularly assess and update the tech curriculum.</li> <li>Align with industry needs and technological advancements.</li> </ul> <p>By implementing these changes and continually evaluating the curriculum's effectiveness, Minnesota can foster a tech-savvy generation ready to take on roles in the ever-evolving tech industry.</p>"},{"location":"business-plan-2020/","title":"AI Racing League Business Plan for 2020","text":""},{"location":"business-plan-2020/#summary","title":"Summary","text":"<p>This business plan governs the AI Racing League for the calendar year 2020. We are operating as a project within CodeSavvy. CodeSavvy is a register 501C organization. CodeSavvy's mission is to promote coding skills in youth with a focus on  promoting mentoring and training for girls and disavantaged youth.</p>"},{"location":"business-plan-2020/#brief-history","title":"Brief History","text":"<p>The AI Racing League was founded in the summary of 2019.  We were inspired by the viral global DonkeyCar project.  We wondered if the DonkeyCar racing events could be used to teach AI in the classroom. The goals of the founders were to promote fun events that taught AI to girls and disavantaged youth.  We recieved an initial round of $9K in funding from the Optum Technology Social Responsibility and lauched our first event in August of 2019.  This event was done at the International School of Minnesota and attracted members from the AI Research community, schools, educators and students.</p> <p>Since August we have participated in over a dozen events promoting AI instruction.  We have trained and initial set of approximatly 50 mentors that are familair with the use of the DonkeyCar</p>"},{"location":"business-plan-2020/#status-as-of-january-2020","title":"Status as of January 2020","text":""},{"location":"business-plan-2020/#inventory-of-assets","title":"Inventory of Assets","text":"<ol> <li>2 GPU Servers</li> <li>10 DonkeyCars</li> <li>AI Racing League Web Site</li> <li>AI Racing League Concept Cards</li> <li>AI Racing League Concept Dependancy Graph</li> <li>Documented best practices (Lessons learned, SD image inventory etc.)</li> <li>Miscellenous training material</li> <li>Marketing materials including testimonials</li> <li>Post on social media</li> </ol>"},{"location":"business-plan-2020/#goals-for-2020","title":"Goals for 2020","text":""},{"location":"business-plan-2020/#financial-goals","title":"Financial Goals","text":""},{"location":"college-project/","title":"Sample College Project","text":"<p>Many colleges have \"Senior Seminar\" courses that allow students to spend time on projects that they are interested in pursuing.  This is an outline of a sample college-level project that a team of six students will spend about 60 hours each on.</p>"},{"location":"college-project/#equipment","title":"Equipment","text":"<ol> <li>At least two Donkey Cars</li> <li>A PC with a GPU such as a NVIDIA GeForce GTX series card.</li> <li>A track</li> </ol> <p>We can currently supply both the cars and GPU.  The school will need to purchase its own track or borrow one from a local school such as Washburn.  Optum also has a track that is not being used.  The price for a track is around $175.</p>"},{"location":"college-project/#key-learning-objectives","title":"Key Learning Objectives","text":"<ol> <li>Machine Learning Processes - learn the basic steps of machine learning including collecting data, building a model and using the model to make predictions.</li> <li>Computer Vision - learn how computer vision systems gather images and can predict things such as throttle speed and turning from these images.  This course uses the powerful open-source Open Computer Vision Python libraries.</li> <li>Data Analysis - use Jupyter Notebooks to analyze data to find bad data and remove it from training sets.</li> <li>Raspberry Pi Single Board Computers - learn how to set up and configure the Raspberry Pi Model 4 single-board computers.</li> <li>Motors and Servos - learn how to control motors and servos using pulse width modulation</li> <li>Calibration - learn how to calibrate PWM signals to properly set ranges for motors and servos</li> <li>UNIX Shell - learn how to use the UNIX shell to set up manage your car.</li> <li>GPU Configuration - learn how to set up and configure a NVIDIA GPU on a Linux operating system</li> <li>Generative AI - learn to use generative AI tools to write Python code and Jupyter Notebooks.  Use generative AI to debug problems and accelerate development.</li> </ol>"},{"location":"college-project/#sample-lessons","title":"Sample Lessons","text":"<p>Each lesson is designed to provide both theoretical knowledge and practical experience, ensuring a comprehensive understanding of the topics covered.</p> <p>Title: AI Racking League - 60 Hours</p>"},{"location":"college-project/#lesson-1-introduction-to-donkeycars-and-project-overview-6-hours","title":"Lesson 1: Introduction to DonkeyCars and Project Overview (6 hours)","text":""},{"location":"college-project/#theory","title":"Theory","text":"<ul> <li>Introduction to the course, objectives, and equipment.</li> <li>Overview of DonkeyCar: components, functionality, and potential applications.</li> <li>Formation of project teams and defining team roles.</li> </ul>"},{"location":"college-project/#hands-on","title":"Hands On","text":"<ul> <li>Initial setup of DonkeyCars and PCs.</li> <li>Basic operation and control test of the DonkeyCar.</li> </ul>"},{"location":"college-project/#lesson-2-machine-learning-basics-6-hours","title":"Lesson 2: Machine Learning Basics (6 hours)","text":""},{"location":"college-project/#theory_1","title":"Theory","text":"<ul> <li>Introduction to machine learning: concepts and processes.</li> <li>Data collection methods for DonkeyCar.</li> <li>Building a simple predictive model.</li> </ul>"},{"location":"college-project/#hands-on_1","title":"Hands On","text":"<ul> <li>Collecting initial data sets using DonkeyCars.</li> <li>Initial model testing on PCs using the collected data.</li> </ul>"},{"location":"college-project/#lesson-3-computer-vision-fundamentals-6-hours","title":"Lesson 3: Computer Vision Fundamentals (6 hours)","text":""},{"location":"college-project/#theory_2","title":"Theory","text":"<ul> <li>Introduction to computer vision and its applications in autonomous vehicles.</li> <li>Basics of OpenCV and image processing techniques.</li> </ul>"},{"location":"college-project/#hands-on_2","title":"Hands On","text":"<ul> <li>Implementing basic computer vision algorithms on DonkeyCar images.</li> <li>Capturing and processing images using the Raspberry Pi and GPU.</li> </ul>"},{"location":"college-project/#lesson-4-data-analysis-with-jupyter-notebooks-6-hours","title":"Lesson 4: Data Analysis with Jupyter Notebooks (6 hours)","text":""},{"location":"college-project/#theory_3","title":"Theory","text":"<ul> <li>Introduction to Jupyter Notebooks and Python for data analysis.</li> <li>Data cleaning and preprocessing techniques.</li> </ul>"},{"location":"college-project/#hands-on_3","title":"Hands On","text":"<ul> <li>Analyzing and cleaning DonkeyCar data using Jupyter Notebooks on PCs.</li> <li>Identifying patterns and anomalies in the data sets.</li> </ul>"},{"location":"college-project/#lesson-5-raspberry-pi-configuration-6-hours","title":"Lesson 5: Raspberry Pi Configuration (6 hours)","text":""},{"location":"college-project/#theory_4","title":"Theory","text":"<ul> <li>Overview of Raspberry Pi Model 4.</li> <li>Setting up the Raspberry Pi for DonkeyCars.</li> <li>Basic programming and interface interaction.</li> </ul>"},{"location":"college-project/#hands-on_4","title":"Hands On","text":"<ul> <li>Configuring and testing Raspberry Pi on DonkeyCars.</li> <li>Writing simple control programs for the DonkeyCar.</li> </ul>"},{"location":"college-project/#lesson-6-motors-and-servos-control-6-hours","title":"Lesson 6: Motors and Servos Control (6 hours)","text":""},{"location":"college-project/#theory_5","title":"Theory","text":"<ul> <li>Understanding motors and servos in robotics.</li> <li>Principles of pulse width modulation (PWM).</li> </ul>"},{"location":"college-project/#hands-on_5","title":"Hands On","text":"<ul> <li>Programming PWM for motor and servo control on the DonkeyCar.</li> <li>Testing motor responses and fine-tuning control parameters.</li> </ul>"},{"location":"college-project/#lesson-7-calibration-of-donkeycars-6-hours","title":"Lesson 7: Calibration of DonkeyCars (6 hours)","text":""},{"location":"college-project/#theory_6","title":"Theory","text":"<ul> <li>The importance of calibration in robotics.</li> <li>Step-by-step calibration of motors and servos.</li> </ul>"},{"location":"college-project/#hands-on_6","title":"Hands On","text":"<ul> <li>Calibrating and testing DonkeyCars.</li> <li>Implementing calibration routines and evaluating performance.</li> </ul>"},{"location":"college-project/#lesson-8-unix-shell-and-system-management-6-hours","title":"Lesson 8: UNIX Shell and System Management (6 hours)","text":""},{"location":"college-project/#theory_7","title":"Theory","text":"<ul> <li>Basics of UNIX shell and command-line tools.</li> <li>Managing DonkeyCar systems using UNIX commands.</li> <li>Scripting for automation and efficiency.</li> </ul>"},{"location":"college-project/#hands-on_7","title":"Hands On","text":"<ul> <li>Shell exercises on DonkeyCar systems.</li> <li>Setting up automated scripts for data collection and analysis.</li> </ul>"},{"location":"college-project/#lesson-9-gpu-configuration-and-optimization-6-hours","title":"Lesson 9: GPU Configuration and Optimization (6 hours)","text":""},{"location":"college-project/#theory_8","title":"Theory","text":"<ul> <li>Introduction to NVIDIA GPUs and their role in machine learning.</li> <li>Setting up and configuring a GPU on a Linux system.</li> <li>GPU optimization techniques for better performance.</li> </ul>"},{"location":"college-project/#hands-on_8","title":"Hands On","text":"<ul> <li>Configuring and testing GPU on project PCs.</li> <li>Running machine learning models on GPU for enhanced performance.</li> </ul>"},{"location":"college-project/#lesson-10-generative-ai-and-project-completion-6-hours","title":"Lesson 10: Generative AI and Project Completion (6 hours)","text":""},{"location":"college-project/#theory_9","title":"Theory","text":"<ul> <li>Introduction to generative AI and its applications in coding.</li> <li>Using generative AI tools for debugging and development.</li> </ul>"},{"location":"college-project/#hands-on_9","title":"Hands On","text":"<ul> <li>Applying generative AI for coding and debugging on the DonkeyCar project.</li> <li>Finalizing and testing the DonkeyCar models and systems.</li> </ul> <p>At the end of the course, students will be asked to give a presentation and demonstration to the other students in the course.</p>"},{"location":"college-project/#assessments","title":"Assessments","text":"<p>The student will be asked to create a GitHub repository and keep their documentation, notebooks and code on that site.  The use of mkdocs is strongly encouraged.  Evaluators will look for signs of high-quality documentation that could be reused by future students in their learning and problem-solving</p> <p>Students will be asked to describe the problems they had and how they overcame these challenges.  They will also reflect on if they effectively used generative AI to solve problems.</p> <p>Students will each be asked if they would recommend working with their teammates in the future.  Higher ratings will be reflected in the student's course evaluations.</p>"},{"location":"command-line-tips/","title":"Command Line Tips","text":""},{"location":"command-line-tips/#get-setup","title":"Get Setup","text":"<pre><code>git config --global user.name \"Joe Smith\"\ngit config --global user.email \"Joe.Smith123@gmail.com\"\ngit config --global credential.helper store\n</code></pre>"},{"location":"command-line-tips/#raspberry-pi-command-line-tips","title":"Raspberry Pi Command Line Tips","text":"<p>From Terminal, you can open the current directory in the File Manager using the <code>xdg-open</code> command.  This is similar to the Mac open command.</p> <pre><code>$ xdg-open .\n</code></pre>"},{"location":"command-line-tips/#see-if-the-pwm-board-is-working","title":"See If the PWM Board Is Working","text":"<pre><code>i2cdetect -l\n</code></pre> <p>This should return <pre><code>i2c-1   i2c   bcm2835 (i2c@7e804000)   I2C adapter\n</code></pre></p> <pre><code>i2cdetect -y 1\n</code></pre> <pre><code>     0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f\n00:          -- -- -- -- -- -- -- -- -- -- -- -- -- \n10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n40: 40 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n70: 70 -- -- -- -- -- -- --                         \n</code></pre> <p>Note that the line 40 and 70 has values under column 0 (I2C bus 1) If you unplug the data you should get:</p> <pre><code>     0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f\n00:          -- -- -- -- -- -- -- -- -- -- -- -- -- \n10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n70: -- -- -- -- -- -- -- --                        \n</code></pre>"},{"location":"command-line-tips/#sd-card-speed-test","title":"SD Card Speed Test","text":"<p>Home -&gt; Accessories -&gt; Raspberry Pi Diagnostics</p> <pre><code>Raspberry Pi Diagnostics - version 0.9\nSat Jul  3 14:25:23 2021\n\nTest : SD Card Speed Test\nRun 1\nprepare-file;0;0;20628;40\nseq-write;0;0;21999;42\nrand-4k-write;0;0;4498;1124\nrand-4k-read;8695;2173;0;0\nSequential write speed 21999 KB/sec (target 10000) - PASS\nRandom write speed 1124 IOPS (target 500) - PASS\nRandom read speed 2173 IOPS (target 1500) - PASS\nTest PASS\n</code></pre>"},{"location":"contacts/","title":"AI Racing League Contacts","text":"<ul> <li>Dan McCreary - LinkedIn</li> </ul>"},{"location":"curriculum/","title":"Curriculum","text":"<p>We have a long term vision of using an intelligent agent that will recommend the right content for each of our students based on their current knowledge and their learning goals.</p> <p></p>"},{"location":"curriculum/#beginning-green-concepts","title":"Beginning (Green) Concepts","text":""},{"location":"curriculum/#batteries","title":"Batteries","text":""},{"location":"curriculum/#motors","title":"Motors","text":""},{"location":"curriculum/#donkey-car","title":"Donkey Car","text":""},{"location":"curriculum/#activity","title":"Activity","text":"<p>Go to the donkey car station and look at the sample Donkey Car. Ask a mentor to show you the parts.</p>"},{"location":"curriculum/#questions","title":"Questions","text":"1) What are the key parts of the Donkey Car? <p>  The key parts are: * RC Car chassis * Nvidia Jetson Nano * Servo controller * Camera * Battery for the Nano  </p> 2) How do the front wheels turn? <p>  A 180 degree servo is used to steer the car  </p> 3) Can you find an electric motor? <p>  There is only a single motor in the RC chassis  </p> 4) Can you find a battery?  Are their multiple batteries?  <p>  There are two batteries - one for the motor and one for the Jetson Nano  </p> 5) Where is the Jetson Nano (computer)? <p>  It is right on top!  </p> 6) Where is the camera?  Is it on the front or back of the car? <p>  The camera is on the top facing forward  </p> 7) What happens to the opposite wheel when you turn it? <p>  The transmission makes the wheels turn in opposite direction when one wheel is turned. - Is this correct?  </p> 8) How much does a Donkey Car cost?  <p>  The car costs around $250 each.  The RC chassis cost about $110.  </p> 9) Why do you think they call it a \u201cDonkey Car\u201d? <p>  They call it a \"Donkey Car\" because, like a Donkey, it is functional but not very sleek.  </p>"},{"location":"curriculum/#intermediate-concepts","title":"Intermediate Concepts","text":""},{"location":"curriculum/#machine-learning","title":"Machine Learning","text":""},{"location":"curriculum/#activity_1","title":"Activity","text":"<p>Go to the machine learning station and watch the demos. Ask about the difference between if-else statements and machine learning.</p>"},{"location":"curriculum/#questions_1","title":"Questions","text":"1) What is Machine Learning? How does it differ from traditional rule-based programming? <p>  Machine learning is a method of data analysis that automates analytical model building, based on the idea that systems can learn from data. Rule-based programming is built off of if-else statements in code, and therefore every possible situation has to be thought of in advance by the programmer. Therefore, machine learning is well suited for situations where all possible inputs may not be defined.  </p> 2) How does a computer learn? <p>  The computer learns through a process called training. Training is the process of adjusting a mathematical formula by feeding it data and adjusting the formula until it produces the desired output.  </p> 3) What are the major groups of machine learning? <p>  There are 5 major groups of algorithms within machine learning.  They are: * The connectionists (Neural Networks) * The analogizers (Support Vector Machines) * The Bayesians (Bayes\u2019 Theorem) * The evolutionaries (Genetic Algorithms) * The symbolists (Inverse Deduction)  </p> 4) Applications of machine learning are everywhere, what are some examples? <p>  Some applications of machine learning are: * Voice Assistants (Siri, Alexa, etc.) * Translation * Self-Driving Cars  </p>"},{"location":"curriculum/#blue-concepts","title":"Blue Concepts","text":""},{"location":"curriculum/#black-concepts","title":"Black Concepts","text":""},{"location":"demo/","title":"Demos","text":"<p>Although our students love hands-on learning with our DonkeyCars, there are other aspects of Artificial Intelligence we like to discuss in our classes.  Here are some demos we use in our classrooms.</p>"},{"location":"demo/#the-teachable-machine-by-google","title":"The Teachable Machine by Google","text":"<p>This demo works with almost any PC that has a built-in video camera.  You give it a set of images or pictures, or sounds and you build a model that predicts what a new images our sounds might be.  This is called a \"classification\" model.</p> <ul> <li>Teachable Machine</li> </ul> <p>Much of our classroom work is centered around the hot topic of Deep Learning.  But AI is much more than just Deep Learning.  Here are a few other areas to consider.  (Taken from the book The Master Algorithm)</p>"},{"location":"demo/#5-camps-of-machine-learning-demos","title":"5 Camps of Machine Learning Demos","text":""},{"location":"demo/#connectionists-neural-networks","title":"Connectionists (Neural Networks)","text":"<p>Check Out TensorFlow Playground</p>"},{"location":"demo/#analogizers-support-vector-machines","title":"Analogizers (Support Vector Machines)","text":"<p>TODO: FIND GOOD DEMO</p>"},{"location":"demo/#bayesians-bayes-theorem","title":"Bayesians (Bayes\u2019 Theorem)","text":"<p>Check Out A Bayes' Theorem Example</p>"},{"location":"demo/#evolutionaries-genetic-algorithms","title":"Evolutionaries (Genetic Algorithms)","text":"<p>Watch This Animation Learn to Walk</p>"},{"location":"demo/#symbolists-inverse-deduction","title":"Symbolists (Inverse Deduction)","text":"<p>Look at this Decision Tree Demo</p>"},{"location":"faqs/","title":"AI Racing League Frequently Asked Questions","text":""},{"location":"faqs/#how-much-does-it-cost","title":"How much does it cost?","text":"<p>All our events are free.</p> <p>However, ff you want to build your own car you are welcome to bring these to the events.  Parts for a DonkeyCar typically run about $250 US. See our Car Parts Lists for details.</p>"},{"location":"faqs/#what-do-i-have-to-know-before-i-come","title":"What do I have to know before I come?","text":"<p>Nothing!  We have material for beginners without any prior knowledge of AI.</p>"},{"location":"faqs/#what-car-part-hardware-do-you-use","title":"What car part hardware do you use?","text":"<p>We use mostly NVIDIA Nano and Raspberry Pi 4 for our single board computers.  We use a wide variety of RC-car engines but the [Exceed Magnet] 1/16 scale RC car is a low-cost standard.  See our Car Parts Lists for details.  Typical car parts cost around $250 US.</p>"},{"location":"faqs/#what-gpus-do-you-use-and-how-much-do-they-cost","title":"What GPUs do you use and how much do they cost?","text":"<p>We use a standard PC chassis running Lunix with a NVIDIA GPU such as a GTX 2080.  These PCs can be purchased for around $1,500.  Se our GPU Parts List for details.</p>"},{"location":"faqs/#how-do-i-sign-up-as-a-student","title":"How do I sign up as a student?","text":"<p>The best way to get involved is by signing up as a student at the CoderDojo Twin Cities web site:</p> <p>Coderdojotc.org</p>"},{"location":"faqs/#how-do-i-become-a-mentor","title":"How do I become a mentor?","text":"<p>The best way to get involved is by signing up as a mentor at the CoderDojo Twin Cities web site:</p> <p>https://www.coderdojotc.org/mentor_signup/</p>"},{"location":"faqs/#how-do-i-start-my-own-chapter-of-the-ai-racing-league","title":"How do I start my own chapter of the AI Racing League","text":"<p>Please connect with Dan McCreary on LinkedIn and indicate in the note you would like to start your own chapter.  Be sure to include information about your leadership and technical background and any related experience working with STEM programs.</p>"},{"location":"faqs/#can-i-get-a-grant-to-purchase-hardware-for-our-school-or-club","title":"Can I get a grant to purchase hardware for our school or club?","text":"<p>We are working on arranging a grant application process.  The best way to start this process is to gather a small group of volunteers that can create a sustainable club. Include people that have a combination of fundraising, technology, education and marketing skills. Reach out to local school administration officials to build a community of science/math and STEM educators.  Network with local companies that are trying to build local talent in AI and machine learning.</p> <p>Please contact Dan McCreary on LinkedIn for details.</p>"},{"location":"glossary/","title":"AI Racing League Glossary of Term","text":""},{"location":"glossary/#calibration","title":"Calibration","text":"<p>A step in setting up a DonkeyCar where around five values configuration file is created that reflect the physical aspects of the RC car.  There are three parameters for the throttle and two parameters for the steering.  It is important to get these five parameters correct so you can precisely drive your DonkeyCar.</p>"},{"location":"glossary/#catalog-file","title":"Catalog File","text":"<p>A format of storing our image-related throttle and steering data in line-oriented file where each line contains the serialized JSON information when the image was captured.</p> <p>Note that the catalog files are not pure JSON files.  Only the data within each line is a valid data JSON object.</p> <p>The catalog file formats have changed between DonkeyCar releases.  The current version is called V2 format.</p>"},{"location":"glossary/#coderdojo","title":"CoderDojo","text":"<p>An international program of over 2,300 coding clubs that uses data-driven practices to get students interested in coding.  Many of the aspects of the AI Racing League uses these same principals.</p> <p>Key aspects of CoderDojo are:</p> <ul> <li>No fees for events - accessible to all</li> <li>Mentor ratios of no more than three students per mentor</li> <li>Project-based learning</li> <li>Focus on getting students to work in teams (social learning)</li> <li>Student-directed projects</li> <li>Focus on programs for girls in coding and underprivileged youth</li> </ul> <p>Their main web site is: http://coderdojo.com/</p>"},{"location":"glossary/#coderdojo-twin-cities-python-labs","title":"CoderDojo Twin Cities Python Labs","text":"<p>These labs are a set of free-courses to learn Python using fun turtle graphics.  There is no experience needed.</p> <ul> <li>Link to CoderDojo Twin Cities Python Labs</li> </ul>"},{"location":"glossary/#coderdojo-twin-cities-micropython-labs","title":"CoderDojo Twin Cities MicroPython Labs","text":"<p>These labs are a set of free-courses to learn MicroPython.  You should have a background in Python to use these labs.  There are lessons in sensors, motors and robots.</p> <ul> <li>Link to CoderDojo Twin Cities MicroPython Labs</li> </ul>"},{"location":"glossary/#coderdojo-base-micropython-robot","title":"CoderDojo Base MicroPython Robot","text":"<p>This is a $25 robot that you build and program with MicroPython.  If you are working on a project that will lead up to a full DonkeyCar, this is an ideal project to get you started. The robot will get you familiar with concepts like PWM, motor controllers and sensors.</p> <ul> <li>Blog article</li> <li>Microsite Raspberry Pi Pico MicroPython Base Robot</li> </ul>"},{"location":"glossary/#code-savvy","title":"Code Savvy","text":"<p>Code Savvy is a not-for-profit organization with 501(c)3 status from the IRS that the AI Racing League works as a sub-project.  All the AI Racing League financials are organized under a Code Savvy program.  Donations to the AI Racing League should be done though Code Savvy donations.  Questions about Code Savvy can be sent to kidscode@codesavvy.org</p> <ul> <li>Code Savvy Web Site</li> </ul>"},{"location":"glossary/#concept-cards","title":"Concept Cards","text":"<p>These are small laminated cards that have concepts information on them that students can learn.  The idea is one-concept per card.  See the CoderDojo TC Guide for Authoring Concept Cards</p>"},{"location":"glossary/#donkey-car","title":"Donkey Car","text":"<p>This is a trademarked name of a car that is used at our events.  The name implies \"ugly\" so you know that they are not designed to look pretty, just functional cars with a camera on the front.</p> <ul> <li>DonkeyCar web site</li> </ul>"},{"location":"glossary/#gpu-server","title":"GPU Server","text":"<p>Each of the AI Racing League events usually has at least one GPU server for training our models.  These are typically small portable PCs with a GPU card in them.  The entire GPU server cost around $1,200 each and can train a 20,000 image data set in under five minutes.</p> <p>We typically suggest that clubs writing grants use a NVIDIA GEFORCE RTX 2070 8GB or similar card since it is both fast enough for 10-team events but cost effective that schools can afford them.  These card are often available used on e-Bay for a few hundred dollars.</p> <p>Note that we have tried to use cloud-based services at some of our events but we can't be guaranteed that there is enough WiFi bandwidth to move large datasets and models to and from the cloud.  We feel that the tasks involved in setting up the GPU server is also a valuable skill for our students.</p> <ul> <li>See the GPU Parts List for a list of components.</li> </ul>"},{"location":"glossary/#electronic-speed-control","title":"Electronic Speed Control","text":"<p>An electronic circuit that controls and regulates the speed of an electric motor. It also can reverse the direction of the motor.  Our ESC</p> <ul> <li>Wikipedia Page on Electronic Speed Control</li> </ul>"},{"location":"glossary/#fifteen-degree-camera-angle","title":"Fifteen Degree Camera Angle","text":"<p>The angle our cameras need to point down to have a good view of the road ahead.</p>"},{"location":"glossary/#normalized","title":"Normalized","text":"<p>Values that have been converted into a standard that can be used across many situations.</p> <p>For example, we don't store the exact PWM ratios of the throttle and steering values in our catalog files.  We convert these values into ranges from 0.0 to 1.0 so that all our machine learning models can share them.  This is why we also need the configuration values when the drive commands are used to convert the normalized values back to the appropriate PWM ranges unique to each car.</p>"},{"location":"glossary/#pulse-width-modulation","title":"Pulse Width Modulation","text":"<p>The way that we control the [Electronic Speed Controller] (ESC) and the servo by sending digital square waves with a variable ratio of the width of the positive part of the square wave.</p> <ul> <li>Wikipeda Page on Pulse-width modulation</li> </ul>"},{"location":"glossary/#tracks","title":"Tracks","text":"<p>We want our clubs to all have affordable but high-quality tracks that are easy to roll up and store.  Our suggestion is to find used billboard vinyl in a dark color (black or dark blue) and then use white and yellow tape to place down the lines.</p> <ul> <li>https://billboardtarps.com/product-category/billboard-vinyl/</li> <li>YouTube Video</li> </ul>"},{"location":"glossary/#training-step","title":"Training Step","text":"<p>The step in DonkeyCar setup where we take approximately 20,000 small image files and the throttle and steering information with each image to build a deep neural network.  The training step requires us to move the data off the DonkeyCar's SD card and transfer the data to a more powerful GPU server.  Using a typical $1,200 GPU server we can build a model file in around five minutes.  This file is then transferred back to the DonkeyCar for autonomous driving.</p>"},{"location":"glossary/#tubs","title":"Tubs","text":"<p>This is the term that the DonkeyCar software uses to store training data.  Each tub consists of a catalog of information about the drive and the images associated with that drive.</p> <p>Note that the format of the tubs changes over time so old tubs formats may need to be converted to newer formats.</p> <ul> <li>Script to Convert Tugs from V1 format to V2 Format</li> <li>DonkeyCar Catalog Format</li> </ul>"},{"location":"glossary/#tub-catalog-format","title":"Tub Catalog Format","text":"<p>Each Tub is a directory (folder) has two components:</p> <ol> <li>A sub folder called \"images\" that contains the jpeg images gathered during a training run.  There are typically 10,000 to 20,000 small images in a tub image folder.</li> <li>A file that describes the data about all the images called a Catalog file.  The Catalog file is similar to a JSON file but it has no root data elements.</li> </ol>"},{"location":"glossary/#nvidia-nano","title":"NVIDIA Nano","text":"<p>One of the two single board computers we use in our DonkeyCars.  The current Nanos have 4GB RAM and a GPU for accelerating real-time inference.</p> <p>The full product name is the NVIDIA Jetson Nano.</p> <p>The price for a 4GB Nano is around $99 but they occasionally go on sale for $79.  The Nano became available for sale in the US in April of 2019.  A 2GB version has also been sold for $59 but the lack of RAM memory makes it difficult to use for many of our AI Racing League events and we don't recommend it.</p> <p>Note that we do not use the Nano for training.  We transfer the data to a GPU server that has more parallel cores for training.</p>"},{"location":"google-analytics/","title":"Google Analytics","text":"<p>Our Google Tracking ID is: G-RL4MZ0MHZ4</p> <p>You can see the activity here:</p> <p>Google Dashboard</p>"},{"location":"google-analytics/#how-we-enabled-google-analytics","title":"How We Enabled Google Analytics","text":"<p>mkdocs material supports Google Analysis.  We only need to add four lines to our mkdocs.yml configuration file.</p> <pre><code>extra:\n  analytics:\n    provider: google\n    property: G-RL4MZ0MHZ4\n</code></pre> <p>See our mkdocs.yml on GitHub here:</p> <p>mkdocs.yml file in GitHub</p> <p>The following line is placed into each HTML web page in the site branch: <pre><code>&lt;script async=\"\" src=\"https://www.googletagmanager.com/gtag/js?id=G-RL4MZ0MHZ4\"&gt;&lt;/script&gt;\n</code></pre></p>"},{"location":"hackathon/","title":"Hackathon Ideas","text":"<p>Here are a set of ideas that can be used to plan a Hackathon around the DonkeyCar. Of course, if people are not familiar with the DonkeyCar, just getting it to work is a good project!  These are more for teams that are extending the DonkeyCar software.</p>"},{"location":"hackathon/#beginner-projects-done-in-under-one-day","title":"Beginner Projects - done in under one day","text":"<ol> <li>Ceate a UNIX shell script for managing the tub and model files.  The script should present a menu of options that includes seending one or more tubs to a server for training and getting the model file back.  Bonus points  for listing models and sizes.</li> <li>Build an RGB strip interface to display car and GPU status. The car status can move from red to green as the number of images gathered reaches a 10K image target.  Use the Arduino Moving Rainbow site as your guide.  Background with Arduino is helpful.</li> <li>Build an RGB strip interface to display the GPU training status. The RGB strip should change from red to green as the model error approaches a reasonable value like 0.01. Background with Arduino is helpful. You will also need to modify the TensorFlow interface to write error values to a serial port using python.</li> </ol>"},{"location":"hackathon/#medium-projects-could-be-done-with-prep-over-a-weekend","title":"Medium Projects - could be done with prep over a weekend","text":"<ol> <li>Ceate a web interface for managing the tub and model files. The web interface should present a menu of options that includes seending one or more tubs to a server for training and getting the model file back.  Bonus points for listing models and sizes.</li> <li>Create a Jypyter notebook for analysing data and managing the tub and model files. The web interface should present a menu of options that includes seending one or more tubs to a server for training and getting the model file back.  Bonus points for listing models and sizes.</li> </ol>"},{"location":"hackathon/#advanced-projects","title":"Advanced Projects","text":"<ol> <li>Create a mobile application to drive the car during training. This is the best practice for many low-cost robots like the MiP. Use the tilt APIs to make the car easy to steer.</li> <li>Create small and large versions of the DonkeyCar.  The small version should work on a 12x16 track and the large version should work outdoors on grass. Use a Raspberry Pi or Nvidia Nano.</li> <li>Create a learning management system for running AI Racing League events. Create a list of concepts and load a dependancy list of concepts into a graph database like TigerGraph.  Create a quiz that attendees use to enter their learning objectives and background.  The app should then recomend what content they should use, what tables they should visit, and what mentors they should connect with in the right order. Bonus points of you build a cell phone front end.</li> </ol>"},{"location":"hackathon/#car-to-gpu-file-transfer-scripts","title":"Car to GPU File Transfer Scripts","text":"<p>Make it easy to transfer DonkeyCar test data to our GPU server.  Start with a UNIX shell script that compresses the tub file and puts the data on a jump drive.  Then work on using SSH to copy the files to the GPU server.  Then add configuration of the Avahi application and the mDNS protocols to autodiscover the ARL GPU servers and prompte the user.</p>"},{"location":"hackathon/#mobile-app-to-drive-the-car","title":"Mobile App to Drive The Car","text":"<p>Most robot systems like the MIP have a simple mobile application for driving your robot around.  There are two modes: A tilt mode (where you steer by tilting the phone) and a pressure mode where you can control the speed and direction by pressing on a virtual joystick.  The problem we have with the current DonkeyCar 3.X system is that the web-based application is difficult to use.  The tilt mode does not work on web browsers.  We suggest you use a program like AppInventor for Android or Google Flutter and Dash building mobile apps.</p>"},{"location":"hackathon/#leaderboard-web-page","title":"Leaderboard Web Page","text":"<p>Create a web application that tracks what teams are in the lead.  The app should be a single-page application that allows team scores to be updated on a web form.  The leaderboard can also be \"smart\" a look for the team config files on each DonkeyCar on the local-area network.</p>"},{"location":"hackathon/#oled-extension","title":"OLED Extension","text":"<p>Add a low-cost OLED screen to each car using the SPI bus.  Have the OLED screen show key parameters such as hostname, static IP address, disk space free, training data size etc.  Bonus points for a mode button to cycle through screens.  See Dan McCreary for the hardware.</p>"},{"location":"hackathon/#led-strips-for-training-server-status","title":"LED Strips for Training Server Status","text":"<p>Add an low-cost WS-2811-B LED strip to the GPU server.  Make the strip blue when idle, red when you start training an new model, and have it fade to green as the model converges.  See Dan McCreary for the hardware.</p>"},{"location":"hackathon/#training-graph","title":"Training Graph","text":"<p>As students walk in, give them a tablet to register.  It will also ask them basic questions. It will then ask them how long they will be there.  It will then suggest a set of activities and some concepts to master.  The graph is a dependacy graph of all the concepts we teach at the event.  Also suggest a probability they will have fun at the event.</p>"},{"location":"hackathon/#single-source-publishing-for-concept-cards","title":"Single Source Publishing for Concept Cards","text":"<p>Our cards need to be authored in MarkDown but we want to disply on the web, in PPT and with PDF.  To do this we want to adopt a single-source publishing pipeline.</p>"},{"location":"learning-strategy/","title":"AI Racing League Educational Philosophy","text":"<p>The AI Racing League educational philosophy is founded on the following values:</p> <ol> <li>Equal Opportunity - we work hard to keep all AI Racing League activities free and we work hard to encourage girls to be part of our programs.</li> <li>Student driven - we ask each student what their learning goals are and adapt to their needs</li> <li>Project based - we prefer to let students define their own projects and we find resources to support them.  If learning about how to get a car to drive by itself that is great, but if they have other objectives we work hard to connect them with the mentor that will help them build whatever they are interested in building.</li> <li>Teamwork - we want students to work in teams so they learn not just about AI, but how to work with others.</li> <li>Flexibility - we want to be known as one of the most flexible learning organizations around.  We don't force everyone to learn the same things.  We take students in for 20 minutes or two years.  We adapt to the needs of our students and we design our curriculum around their needs.</li> <li>Agility - in software there is a strong concept of \"Agile Development\" where team constantly get feedback on what works and adapt their goals every two weeks to meet the needs of their users.  We use these same strategies to develop flexible content to meet the ever-changing interests of our students.</li> </ol> <p>Our Curriculum is based around building a series of concept cards that adhere to the \"one concept per card\" rule.  Each card is a 5.5in X 8in laminated card with questions or challenges on the front and answers on the back.  Concept cards have three difficulty levels with different colored borders.</p> <ol> <li>Green Borders - Beginner cards that anyone can start with at any time</li> <li>Blue Borders - Intermediate - where you may need to know some beginning concepts before you start</li> <li>Black Borders - Advanced - where you need to master several intermediate concepts before you take these on</li> </ol> <p>Our goal is to keep the concepts as \"flat\" as possible without a deep level of dependency.  We try to keep at least half of our cards mostly green beginner cards.</p> <p>Students will walk into the AI Racing League and see a stack of cards.  They will pick up one card or a set of cards and work on these.  When they are done they return the cards and select another set of cards.</p> <p>Because of our </p> <p>Concept Cards in Google Docs</p>"},{"location":"learning-strategy/#engineering-challenges","title":"Engineering Challenges","text":"<p>To develop a world class curriculum, we need to partner with senior engineers and curriculum developers.  Here are some of the challenges we need to address.</p>"},{"location":"learning-strategy/#challenge-1-make-it-easy-for-short-term-learning","title":"Challenge #1: Make it easy for short term learning","text":"<p>Engineers with experience in both hardware and software can build their own DonkeyCar from parts in a few weeks, our goal is to allow students from a wide variety of backgrounds to be able to participate in events in a flexible way.  A typical CoderDojo event typically only lasts two hours and students may not have the appropriate background in hardware, Python programming or UNIX.</p>"},{"location":"learning-strategy/#challenge-2-on-site-traning-hardware","title":"Challenge #2: On site traning hardware","text":"<p>Many people that are building DonkeyCars use a standard Mac or PC laptop.  These systems take up to two hours to train a typical model - too long for many events.  One solution would be to leverage clound-based GPUs to accelerate learning.  This option typically requires transferring around 1/2 GB of images up to the clound for training the models.  Models, which can typically be 10MB, then need to be transferred back from the clound to the local car.  Our challenge here is that many locations may not have high-bandwith uploading and downloading services that could handle this traffic.</p> <p>One solution is to acquire some robust GPUs that students can use to quickly train complex models - typically in 15 to 20 minutes.  This hardware needs to be easy to use - for example we need to do folder-based drag and drops and press a single button to begin training.</p>"},{"location":"media/","title":"Media","text":"<p>Ready, Set, Algorithms! Teams Learn AI by Racing Cars Morningstar, Liberty Mutual workers are coming up with business ideas after  exploring machine learning via mini self-driving vehicles</p>"},{"location":"presentations/","title":"AI Racing League Presentations","text":"<p>These presentations are all licensed under our creative commons share alike non-commercial with attribution licenses.</p> <ol> <li>Welcome to the AI Racing League - slides used for the kickoff of a six week summer camp on building DonkeyCars</li> <li>AI Racing League Code Savvy - presented to out state Minnesota educators in April 2021</li> <li>November 2019 Update - presentations done in November of 2019 after our fourth \"sprint\" making improvements on our process.</li> </ol>"},{"location":"resources/","title":"Community","text":"<p>Here are some sites that are of interest:</p> <ul> <li>CoderDojo Twin Cities - where you can sign up to be a mentor or student</li> <li>Twin Cities AI Racing League Meetup Site - where we announce our public meetings</li> </ul>"},{"location":"resources/#donkeycar-hardware","title":"DonkeyCar Hardware","text":"<ul> <li>DonkeyCar web site</li> <li>Donkey Car Nano Setup Page</li> <li>DonkeyCar Assembly Video - Chris Anderson's detailed assembly video from 2018.</li> </ul>"},{"location":"resources/#track-options","title":"Track Options","text":""},{"location":"resources/#hardware-options","title":"Hardware Options","text":"<p>Raspberry Pi 3, 4, the Nvidia Nano, the Nvdia DX2, and the Intel Mobius Neural Stick The base DonkeyCar today uses the Raspberry Pi 3+ which has a list price of $35.  This hardware is just barly able to process images in real-time.  Small changes in lighting will throw the car off the track.  The new Raspberry Pi 4 with 4GB RAM is a new option.</p> <p>The Nvidia Nano on the other hand has 128 CUDA core processors and has more than enough power to drive around a track in real time with varied lighting conditions.  This is the hardware we have used for our first generation cars in the AI Racing League.</p> <p>There are also college-level autonomous driving teams that use the more expensive Nvidia DX2 hardware.</p>"},{"location":"resources/#nvidia-nano","title":"Nvidia Nano","text":"<p>Jetson Nano References</p> <ul> <li>Joseph Bastulli PyTorch Nano</li> <li>Nvidia Jetson Developer Kit</li> <li>Nvidia Jetson Nano Kaya Video</li> <li>Adding a Joystick to your DonkeyCar - From Dan McCreary's Blog</li> </ul>"},{"location":"resources/#videos","title":"Videos","text":"<ul> <li>Video of Wide Track</li> <li>PID Theory and Steering - why using machine learning is easier than setting PID parameters.  This is covered in control theory.</li> <li>Real time optimal control of an autonomous RC car with minimum-time maneuvers - nice video of optimization of driving algorithm using a \"U\" shaped track.</li> <li>Sparkfun Autonomous Vehicle Race from 2016</li> <li>Ed Murphy on Maker Faire</li> </ul>"},{"location":"six-week-curriculum/","title":"Sample Six-Week Curriculum","text":"<p>This is a sample suggested curriculum for a six week AI Racing League summer school project.  The students would all meet together for two hours, once a week.  There are then homework assignments. The students don't need any prior experience.</p>"},{"location":"six-week-curriculum/#week-1-overview-and-unboxing","title":"Week 1: Overview and Unboxing","text":"<ol> <li>Slides: Welcome to the AI Racing League? Link to Slides</li> <li>What is the DonkeyCar?</li> <li>Lab: Unbox the car (requires tools such as cable tie cutter and screwdrivers)</li> <li>What is AI? What is Machine Learning?</li> <li>What is Python?</li> <li>Introduction to Python course</li> <li>Motors and servos (demo of car driving with the motors and servos being controlled by RC)</li> <li>Make sure students know how to turn on the ESC and listen for the startup beep sound</li> </ol> <p>See the suggested parts list for week 1</p>"},{"location":"six-week-curriculum/#week-2-booting-a-raspberry-pi-unix-calibration-intro-to-python-and-raspberry-pi","title":"Week 2: Booting a Raspberry Pi, UNIX, Calibration, Intro to Python and Raspberry Pi","text":"<ol> <li>Booting a Raspberry Pi from Micro SD card</li> <li>What is a Raspberry Pi?</li> <li>How much RAM do we need?</li> <li>Why is 4GB important for the AI Racing League?</li> <li>What is a micro SD card?  How big a card do we need? 32GB vs 65GB vs 128GB</li> <li>Can we train our model on a Pi?  Training vs. Inference - when do we need a GPU?</li> <li>What is an Operating System Image file?</li> <li>How do we create an image file?</li> <li>Download a Raspberry Pi image Raspberry Pi Imager</li> <li>Burn a microSD card with that image - include customization</li> <li>Use the microSD card to boot your Raspberry Pi (requires 4GB Raspberry Pi Pico, keyboard, mouse, power supply, monitor)</li> <li>Configure Pi desktop - learn how to use menus, add bookmarks to the web browser, manage bookmarks</li> <li>Start Python IDE</li> <li>Run \"hello world\" in Python</li> <li>Open a Terminal and type \"ls\"</li> <li>Download the DonkeyCar software</li> <li>Get familiar with the folder layout</li> <li>Verify the connections from the Pi to the PWM card and the DonkeyCar</li> <li>Run the calibration command, write down the numbers for throttle and steering</li> </ol>"},{"location":"six-week-curriculum/#week-3-python-configuration-drive","title":"Week 3: Python, Configuration, Drive","text":"<ol> <li>More Python labs - get as far as possible through the Introduction to Python class</li> <li>Get familiar with the Donkey Car configuration file</li> <li>Focus on the key parameters for calibration</li> <li>Find the Drive command</li> <li>Discuss options for controlling the car: Joystick vs Web Application</li> <li>Backup Career Exploration: What is a Software Engineer?</li> <li>Backup Lab: Google Teachable Machines</li> </ol>"},{"location":"six-week-curriculum/#week-4-gather-image-data-and-analyze-quality-with-jupyter-notebooks","title":"Week 4: Gather Image Data and Analyze Quality with Jupyter Notebooks","text":"<ol> <li>Drive around the track and gather image data</li> <li>Look at the image data in the tubs</li> <li>Run a basic Python program to count the number of files</li> <li>Learn about a Jupyter Notebook</li> <li>Backup Career Exploration: What is a Data Scientist?</li> </ol>"},{"location":"six-week-curriculum/#week-5-the-gpu-and-training","title":"Week 5: The GPU and Training","text":"<ol> <li>Learn about the GPU - what are GPU cores? - Why is training time faster?</li> <li>What is a conda environment for Python?</li> <li>What is Miniconda Download here</li> <li>Activating conda environments</li> <li>Verifying that the GPU setting are correct</li> <li>Run a test program on the GPU</li> <li>Learn how to transfer files from the car's memory to the GPU (compress tubs, copy to jump drive)</li> <li>What is a model file?  How big is your model?  What are model parameters?</li> <li>Backup: What is Bias in AI?  How to we detect it?  How dow we measure it?</li> </ol>"},{"location":"six-week-curriculum/#week-6-using-the-model-to-drive-autonomously","title":"Week 6: Using the Model to Drive Autonomously","text":"<ol> <li>Put the model file on the Donkey car</li> <li>Run the drive with model command</li> <li>Change the configuration files</li> <li>Evaluate image quality</li> </ol>"},{"location":"admin/01-intro/","title":"League Administers Page","text":""},{"location":"admin/01-intro/#sd-image","title":"SD Image","text":"<p>This section show how leage administrators can create their own SD image files.</p>"},{"location":"admin/02-sd-image/","title":"Creating a League SD Image","text":"<p>Many times teams will not have the time to build their own image during the time allocated for an event.  It typically takes 2-4 hours to create a DonkeyCar image that is ready to drive.  To get around this problem, leagues frequently create their own \"reference image\" that are given to teams.</p>"},{"location":"admin/02-sd-image/#checklist-for-the-league-image","title":"Checklist for the League Image","text":"<ol> <li>Bookmark bar in the browser is visible (check Show Bookmarks Bar)</li> <li>Bookmark bar is populated with:</li> <li>You League Homepage (use GitHub Pages and mkdocs for best pratices)</li> <li>Links to Team pages with sample myconfig.py files for each team's car</li> <li>Link to the AI Racing League site (https://www.coderdojotc.org/ai-racing-league/)</li> <li>Links to the DonkeyCar site</li> <li>Links to the Raspberry Pi or NVIDIA Nano site</li> <li>All operating system files updates before match</li> <li>Auto-update disabled - you don't what gigabyte uploads when people insert their image</li> <li>All Python libraries Updates</li> <li>Verify Python release such as Python 3.7 using <code>python --version</code></li> <li>Run \"pip freeze\" to get a list of the libraries you have tested on</li> <li>All DonkeyCar libraries updated</li> <li>In the \"donkeycar\" repo run a \"git pull\" to update the latest code</li> <li>Make sure you ONLY change the myconfig.py - changes to config.py will be lost</li> </ol> <p>## Things to Remove from your Image</p> <ol> <li>Personal information (from your github file in .gitinfo)</li> <li>Change Hostname to be Generic (arl)</li> <li>Remove your home or school default wifi settings</li> </ol>"},{"location":"admin/02-sd-image/#burning-an-league-reference-image","title":"Burning an League Reference Image","text":"<ol> <li>Take the image out of your car</li> <li>Take it to a Mac/PC and copy the image to your harddrive using the dd Command</li> <li>Insert a black MicroSD card</li> <li>Copy the image on your PC's Harddrive to the new MicroSD Using DD or a GUI tool like belana Etcher</li> </ol>"},{"location":"admin/02-sd-image/#using-the-gnome-partition-editor-gpartd","title":"Using the GNome Partition Editor (gpartd)","text":""},{"location":"admin/02-sd-image/#reference","title":"Reference","text":"<ol> <li>DonkeyCar Release Process</li> </ol>"},{"location":"admin/03-purchasing-guide/","title":"AI Racing League Purchasing Guide","text":""},{"location":"admin/03-purchasing-guide/#donkeycars","title":"DonkeyCars","text":""},{"location":"admin/03-purchasing-guide/#microcontrollers","title":"Microcontrollers","text":""},{"location":"admin/03-purchasing-guide/#raspberry-pi","title":"Raspberry Pi","text":""},{"location":"admin/03-purchasing-guide/#nvidia-nano","title":"NVIDIA Nano","text":""},{"location":"admin/03-purchasing-guide/#gpus","title":"GPUs","text":"<p>Note</p> <p>The AI Racing League ONLY uses this for training our models.  We don't need elaborate CPU overclocking and a water cooled CPU.  We don't need powerful CPU and lots of RAM.  We just need to be able to train a 20K image model within around 5-10 minutes.  Most GPUs can do this.</p>"},{"location":"admin/03-purchasing-guide/#portable-case","title":"Portable Case","text":"<p>We wanted a small lightweight case with a handle and tempered glass sides so our teams can see what is inside.  The price is around $110.00.</p> <p></p> <p>Lian Li TU150 Mini ITX Desktop Case</p>"},{"location":"admin/03-purchasing-guide/#motherboard","title":"Motherboard","text":""},{"location":"admin/03-purchasing-guide/#ram","title":"RAM","text":""},{"location":"admin/03-purchasing-guide/#gpu","title":"GPU","text":""},{"location":"admin/03-purchasing-guide/#solid-state-drive","title":"Solid State Drive","text":""},{"location":"admin/04-tools/","title":"AI Racing League Tools","text":"<p>Here are some handy tools to help you with your cars.</p>"},{"location":"admin/04-tools/#usb-voltage-current-meter","title":"USB Voltage Current Meter","text":"<p>Many USB battery packs don't maintain a voltage of 5.1v when under load.  To monitor this, you need to watch the voltage level that arrives at the single-board computer input voltage.  This needs to include any voltage drop over the USB power cable.</p> <p></p> <p>Sample item on e-Bay</p> <p>This item uses a four semi-double loop integral converter ICs, with a precision bandgap reference.  The vendor claims the device has:</p> <ol> <li>range accuracy and high precision</li> <li>stable and reliable performance</li> <li>strong anti~interference ability</li> <li>tolarance for high temperatures</li> </ol> <p>Specifications:</p> <ol> <li>Current range: 0~5A</li> <li>Voltage range : 3 ~ 7.5V </li> <li>Resolution : Voltage 10mV, current 1mA</li> <li>Error: voltage &lt;\u00b1 1% current &lt;\u00b1 2%</li> <li>Voltage drop when measuring current: 200mV</li> </ol> <p>Note the voltage drop when current is passing through the device. In practice, the exact voltage drop varies with the current.  For a typical project drawing 300 milliamps, the voltage drop is around 100vV.</p>"},{"location":"admin/car-box-checklist/","title":"AI Racing League Car Box Checklist","text":""},{"location":"admin/car-box-checklist/#donkey-car-nvidia-kit","title":"Donkey Car NVIDIA Kit","text":"<p>Car Name: ____ Mac Address: ____ Static IP Address: ___</p> <ol> <li>[ ] RC Car with the following components</li> <li> <p>[ ] RC Car Battery Charger (7.2v NiMh)</p> </li> <li> <p>[ ] Pi Camera Module V2 with 3D printed chassis</p> </li> <li>[ ] 128GB micro SD card (inserted in Nvidia Nano) (2)</li> <li>[ ] RC Car Battery (7.2v NiMh)</li> <li>[ ] Nvidia Nano with 4GB RAM</li> <li> <p>[ ] Ankar 5v 6800mHA battery with charging cable - note draws 900ma when charging so use a 1ft high current USB cable.</p> </li> <li> <p>[ ] 2.5 amp 5v barrel connector for desktop use of Nvidia</p> </li> <li>[ ] Jumper for enabling the barrele connector</li> <li>[ ] WiFi Dongle (plugged into the car) or AC8265</li> <li>[ ] Logitech F710 Joystick (dongle plugged into the car) - names on both Joystick and dongle</li> <li>[ ] Mouse</li> </ol> <p>Optional Accessories (not in the box) 1. [ ] Keyboard 1. [ ] External Monitor</p> <p>Nvidia Nano Serial Number: ________ Nvidia Nano Purchase Date: December 12, 2019</p>"},{"location":"admin/car-box-checklist/#raspberry-pi-donkeycar-kit","title":"Raspberry Pi DonkeyCar Kit","text":"<ol> <li>[ ] RC Car with separate ESC and Servo Connectors</li> <li>[ ] RC Car Battery Charger (7.2v NiMh)</li> <li>[ ] 7.2 volt battery with Tamiya connector</li> <li>[ ] 3D Printed Chassis </li> <li>[ ] with 3 large screws for roll-bar to base</li> <li>[ ] 8 smaller screws for attaching the PI and PWM board to base</li> <li>[ ] Raspberry Pi</li> <li>[ ] Board with 4GB RAM</li> <li>[ ] 3A Power with USB C Connector</li> <li>[ ] Keyboard</li> <li>[ ] Mouse</li> <li>[ ] VGA Connector</li> <li>[ ] PWM Board</li> <li>[ ] Camera</li> <li>[ ] 4 Female-Female Dupont Connectors</li> <li>[ ] USB Battery Pack (6,000 to 10,000 milliamp hours)</li> <li>[ ] 1 ft USB A to USB C</li> <li>[ ] Extra 7.2 volt battery for RC Car with mini Tamiya</li> </ol>"},{"location":"admin/car-box-checklist/#miscellaneous-parts","title":"Miscellaneous Parts","text":"<ol> <li>Digital Volt-Ohm Meter ($10)</li> <li>Extra Tamiya Style Connectors Amazon Link</li> </ol>"},{"location":"admin/car-parts-list/","title":"DonkeyCar Nvidia Nano Parts List","text":"<p>We have looked at many variations of parts and decided to go with the Nvidia Nano, a TP-Link WiFi dongle and the Logitech F710 Joystick.  Here are our recomended parts.</p> <p>We are also looking into getting the wide-angle (160 degree) cameras but we have not tested these enough.</p> Part Name Description Price Link Note 128GB microSD card Samsung 128GB 100MB/s (U3) MicroSDXC Evo Select Memory Card with Adapter (MB-ME128GA/AM) $20 https://www.amazon.com/Samsung-MicroSD-Adapter-MB-ME128GA-AM/dp/B06XWZWYVP MicroCenter in St. Louis Park has these for about 1/2 the prices Camera Raspberry Pi Camera Module V2-8 Megapixel,1080p $30 https://www.amazon.com/Raspberry-Pi-Camera-Module-Megapixel/dp/B01ER2SKFS MUST be Module V2. The V1 will NOT work with the Nano. Dupont Connectors (F-F) EDGELEC 120pcs 20cm Dupont Wire Female to Female Breadboard Jumper Wires 3.9 inch 1pin-1pin 2.54mm Connector Multicolored Ribbon Cables DIY Arduino Wires 10 15 20 30 40 50 100cm Optional $8 for 120 https://www.amazon.com/EDGELEC-Breadboard-1pin-1pin-Connector-Multicolored/dp/B07GCY6CH7 Only need one of these Nvidia Nano Single Board Computer NVIDIA Jetson Nano Developer Kit $99 https://www.amazon.com/NVIDIA-Jetson-Nano-Developer-Kit/dp/B07PZHBDKT Ships in two days Power for Pi - 6700mAh Anker [Upgraded to 6700mAh] Astro E1 Candy-Bar Sized Ultra Compact Portable Charger, External Battery Power Bank, with High-Speed Charging PowerIQ Technology $24 https://www.amazon.com/Anker-Upgraded-Candy-Bar-High-Speed-Technology/dp/B06XS9RMWS I like this one but there are other variations. Some are rated at 10,000 mAh Power Supply for Nano SMAKN DC 5V/4A 20W Switching Power Supply Adapter 100-240 Ac(US) $10 https://www.amazon.com/SMAKN-Switching-Supply-Adapter-100-240/dp/B01N4HYWAM Note that this is a 4A 12V power supply. RC Car 1/16 2.4Ghz Exceed RC Magnet Electric Powered RTR Off Road Truck Stripe Blue NEW $119 https://www.ebay.com/itm/1-16-2-4Ghz-Exceed-RC-Magnet-Electric-Powered-RTR-Off-Road-Truck-Stripe-Blue-NEW/223337258165 E-Bay Wifi USB Dongle N150 USB wireless WiFi network Adapter for PC with SoftAP Mode - Nano Size, Compatible with Linux Kernal 2.6.18~4.4.3 (TL-WN725N) $7 https://www.amazon.com/TP-Link-TL-WN725N-wireless-network-Adapter/dp/B008IFXQFU/ I purchased one at Microcenter and it worked out-of-the-box on the Nano.  The Ubuntu drivers are pre-loaded! Servo Module HiLetgo 2pcs PCA9685 16 Channel 12-Bit PWM Servo Motor Driver IIC Module for Arduino Robot $10 for 2 https://www.amazon.com/gp/product/B07BRS249H/ref=ppx_yo_dt_b_asin_title_o00_s00?ie=UTF8&amp;psc=1 Note the quantity is 2 USB Power Cable Anker [4-Pack] Powerline Micro USB (1ft) - Charging Cable $10 for 4 https://www.amazon.com/gp/product/B015XR60MQ/ref=ppx_yo_dt_b_asin_title_o02_s00 Note the quantity is 4 but you only need one Replacement Battery 7.2V 1100mAh 6x 2/3A Rechargeable Ni-MH RC Battery Pack w/Small Tamiya Connector 10cmX3cmX1.5cm $9.88 + $2.39 Shipping https://www.ebay.com/i/183877810537 Takes several weeks to ship from China.  We are looking for a local supplier.  Some replacements (Airsoft guns) have reverse polarity."},{"location":"admin/gpu-parts/","title":"AI Racing League GPU Components","text":""},{"location":"admin/gpu-parts/#design-goals","title":"Design Goals","text":"<p>We wanted to create a local training system that had fast training times but was portable so that we can easily carry it in a car and ship it to remote events.  We can't assume any connectivity to the Internet for our events since some of them might be held in parking lots with no network access.  Here are our design objectives.</p> <p>We also drive to remote events and the equipment needs to be outside overnight in freezing weather.  This rules out using any water-cooled hardware which gets easily damaged in freezing weather.</p>"},{"location":"admin/gpu-parts/#fast-training-times","title":"Fast Training Times","text":"<p>We want students to be able to drive around a track 20 times (10 times clockwise and 10 times counterclockwise) and generate a reasonable sized data set of 20 frames per second and 224X224 images.  This ends up being about 10,000 images.  The sizes are a bit larger for larger tracks and slower drivers.</p>"},{"location":"admin/gpu-parts/#why-we-like-the-nvidia-rtx-2070","title":"Why We Like the NVIDIA RTX 2070","text":"<p>We want to train with this data set in under five minutes.  This means that we want to use a GPU card that has about 2000 CUDA cores. An example of this is the Nvidia GeForce GTX graphic cards. The RTX 2070 which currently has a list price of around $500.  There are many people that are upgrading their video game systems and are selling these GPUs used on eBay and Craigslist.com for a few hundred dollars.</p> <p>A higher cost option is the NVIDIA RTX 2080 which has a retail list price of around $1,200 USD.  The benchmarks for image training for these two boards were done by Dr Donald Kinghorn in March of 2019.  [His analysis] (https://www.pugetsystems.com/labs/hpc/TensorFlow-Performance-with-1-4-GPUs----RTX-Titan-2080Ti-2080-2070-GTX-1660Ti-1070-1080Ti-and-Titan-V-1386/) shows that a single GTX 2080 Ti can process about 293 images per second.  The GTX 2070 only does about 191 images per second.  But for about 1/3 of the price it is still a good value.</p>"},{"location":"admin/gpu-parts/#small-and-lightweight","title":"Small and Lightweight","text":"<p>We originally were \"gifted\" a somewhat old GPU server used in a data center for training deep learning models.  Although the sever was \"free\", it was over 70 pounds and had far more capability for RAM and power then we needed at events. Based in this experience we opted to build a much smaller system using a mini enclosure with a handle. We selected the Mini ITX Desktop Case and determined that we could still fit the GPU in this case.</p>"},{"location":"admin/gpu-parts/#rugged","title":"Rugged","text":"<p>Must be able to take the bumps of shipping and be able to be left out in a car overnight in freezing temperatures. This was a requirement for remote events in rural Minnesota communities.  We opted for a full SSD drive to keep the moving parts to a minimum.</p>"},{"location":"admin/gpu-parts/#easy-to-ship-to-remote-sites","title":"Easy to ship to remote sites","text":"<p>We had to be able to put the unit is a remote shipping case.  We are still looking for low-cost cases that are lightweight but protective.</p>"},{"location":"admin/gpu-parts/#visibility","title":"Visibility","text":"<p>We wanted students to be able to look into the case and see the parts.  There is a trend to also purchase RGB LED versions of components which we thought we could program to change from RED to Green during the training process as the model converges.  We have not found a good API for the parts so a simple $5 LED strip on a Arduino Nano might be a better idea.  See the Moving Rainbow project for sample designs.  We create these at the IoT hackthons each year.</p>"},{"location":"admin/gpu-parts/#sample-parts-list","title":"Sample Parts List","text":""},{"location":"admin/gpu-parts/#2023-update","title":"2023 Update","text":"<p>PCPartPicker Part List $769 with Monitor by Neal Kelly</p> Part Name Description Price Link CPU AMD Ryzen 5 3600 3.6 GHz 6-Core Processor $95.00 Amazon Motherboard MSI A520M-A PRO Micro ATX AM4 Motherboard $101.11 Amazon Memory Silicon Power SP016GBLFU320X02 16 GB (1 x 16 GB) DDR4-3200 CL22 Memory $23.99 Amazon Storage TEAMGROUP MP33 512 GB M.2-2280 PCIe 3.0 X4 NVME Solid State Drive $22.49 Amazon Video Card Asus Dual GeForce RTX 3060 V2 OC Edition GeForce RTX 3060 12GB 12 GB Video Card $299.99 Amazon Case Thermaltake Versa H18 MicroATX Mini Tower Case $49.99 Amazon Power Supply be quiet! Pure Power 11 400 W 80+ Gold Certified ATX Power Supply $89.69 Amazon Monitor Acer V227Q Abmix 21.5\" 1920 x 1080 75 Hz Monitor $87.29 Amazon Total $769.55 Part Name Description Price Link Note CPU AMD Ryzen 5 3600 3.6 GHz 6-Core Processor $189.99 Motherboard Gigabyte X570 I AORUS PRO WIFI Mini ITX AM4 $219.99 RAM Corsair Vengeance RGB Pro 32 GB (2 x 16 GB) DDR4-3200 Memory $162.99 Link Notes Storage Gigabyte AORUS NVMe Gen4 1 TB M.2-2280 NVME Solid State Drive $209.99 Link Notes Cooling Be quiet! Dark Rock Pro 4, BK022, 250W TDP $89.90 https://www.amazon.com/dp/B07BY6F8D9/ref=cm_sw_r_cp_api_i_PYp-DbFCY51CH Avoid liquid cooler GPU Card NVIDIA GeForce RTX 2070 Ti 8 GB $499.99 https://www.nvidia.com/en-us/geforce/graphics-cards/rtx-2070-super/ $500 price is a lower cost alternative Case Lian Li TU150 Mini ITX Desktop Case $109.99 Link We love the handle on this small case and the glass side panel. Power Supply Corsair SF 600W 80+ Gold SFX Power Supply $114.99 Link 600W is an overkill <p>Note that this motherboard does come with builtin WiFi. The external antenna must be connected but it is easy to get lost in transport.  You might want to get a few additional WiFi antennas like these RP-SMA Male Antenna We also think we could get buy with a smaller and lighter power supply, but the 600W model gives the system the opportunity to add external devices that might draw more power.</p>"},{"location":"admin/gpu-parts/#assembly","title":"Assembly","text":"<p>There are several good videos on YouTube that show how to assemble custom systems. You can also use a search engine to find videos for each of the parts.  The Liquid coolers can be tricky to install correctly if you don't have experience.  We also recommend reading the user manauals for each of the parts.  They are usually on line.</p> <p>Jon Herke's Tiny Monster</p>"},{"location":"admin/gpu-parts/#installing-nvidia-drivers-on-ubuntu","title":"Installing NVIDIA Drivers on Ubuntu","text":"<p>Installing NVIDIA drivers on Ubuntu is notoriously painful and difficult. One mis-step and you can't get to the monitor and have to ssh in to fix things. Make sure to setup ssh before you install the NVIDIA drivers.</p> <p>We used the UNIX command line to install the NVIDIA drivers. The GUI tool on Ubuntu did not work for us in some settings. See NVIDIA Driver Install.</p> <p>A guide to do this is here: Installation of Nvidia Drivers on Ubuntu 18</p>"},{"location":"admin/gpu-shell/","title":"Shell Commands for the GPU Server","text":"<p>The following is a list of shell commands for the AI Racing League GPU Server.  We have moved all the commands for setting up the NVIDIA GPU to the file NVIDIA Driver Install.</p> <p>The samples below are run if you are on the GPU running the Terminal shell or you have logged on using the secure shell program.</p>"},{"location":"admin/gpu-shell/#secure-shell-login","title":"Secure Shell Login","text":"<pre><code>$ ssh arl@arl1.local\n</code></pre>"},{"location":"admin/gpu-shell/#check-the-version-of-ubuntu","title":"Check the Version of Ubuntu","text":"<pre><code>$ lsb_release -a\n</code></pre> <p>Response:</p> <pre><code>No LSB modules are available.\nDistributor ID: Ubuntu\nDescription:    Ubuntu 18.04.3 LTS\nRelease:    18.04\nCodename:   bionic\n</code></pre>"},{"location":"admin/gpu-shell/#list-the-cpu-information","title":"List the CPU Information","text":"<pre><code>lscpu\n</code></pre> <p>Response:</p> <pre><code>Architecture:        x86_64\nCPU op-mode(s):      32-bit, 64-bit\nByte Order:          Little Endian\nCPU(s):              12\nOn-line CPU(s) list: 0-11\nThread(s) per core:  2\nCore(s) per socket:  6\nSocket(s):           1\nNUMA node(s):        1\nVendor ID:           AuthenticAMD\nCPU family:          23\nModel:               113\nModel name:          AMD Ryzen 5 3600 6-Core Processor\nStepping:            0\nCPU MHz:             2195.902\nCPU max MHz:         3600.0000\nCPU min MHz:         2200.0000\nBogoMIPS:            7187.07\nVirtualization:      AMD-V\nL1d cache:           32K\nL1i cache:           32K\nL2 cache:            512K\nL3 cache:            16384K\nNUMA node0 CPU(s):   0-11\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate sme ssbd mba sev ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif umip rdpid overflow_recov succor smca\n</code></pre> <p>The key is that we have 12 CPUs and each CPU has two threads.  That means that we have 24 threads that run concurrent operations on this server.  This is plenty of capacity for our GPU server.</p>"},{"location":"admin/gpu-shell/#ram","title":"RAM","text":"<pre><code>free -m\n</code></pre> <p>Response:</p> <pre><code>              total        used        free      shared  buff/cache   available\nMem:          32124        1627       28879          75        1618       30019\nSwap:          2047           0        2047\n</code></pre> <p>This indicates we have 32 GB RAM.  The GPU server should have a minimum of 8 GB of RAM.</p>"},{"location":"admin/gpu-shell/#disk-space","title":"Disk Space","text":"<pre><code>df -h /\n</code></pre> <p>Response:</p> <pre><code>Filesystem      Size  Used Avail Use% Mounted on\n/dev/nvme0n1p3  229G  178G   40G  82% /\n</code></pre> <p>This shows we have a total of 229 gigabytes of RAM and we have 40 gigabytes available.  We will need about 4 GB for each training set we store.</p>"},{"location":"admin/gpu-shell/#per-user-disk-usage","title":"Per User Disk Usage","text":"<pre><code>du -hs /home/* 2&gt;/dev/null\n</code></pre> <p>Response:</p> <pre><code>8.5G    /home/arl\n1.4G    /home/dan\n16K /home/dan2\n155G    /home/donkey\n</code></pre>"},{"location":"admin/gpu-shell/#add-a-new-gpu-server-user","title":"Add A New GPU Server User","text":"<p><pre><code>adduser donkey\n</code></pre> You can also allow the user to have \"sudo\" rights by using the following command: <pre><code>sudo usermod -aG sudo donkey\n</code></pre></p>"},{"location":"admin/gpu-shell/#change-the-hostname","title":"Change the Hostname","text":"<pre><code>sudo vi /etc/hostname\n</code></pre> <p>Change the name to \"gpu-server2\" or a similar name.</p>"},{"location":"admin/gpu-shell/#gpu-commands","title":"GPU Commands","text":""},{"location":"admin/gpu-shell/#display-type","title":"Display Type","text":"<pre><code>sudo lshw -class display\n</code></pre> <p>Response: <pre><code>       description: VGA compatible controller\n       product: TU102 [GeForce RTX 2080 Ti Rev. A]\n       vendor: NVIDIA Corporation\n       physical id: 0\n       bus info: pci@0000:09:00.0\n       version: a1\n       width: 64 bits\n       clock: 33MHz\n       capabilities: pm msi pciexpress vga_controller bus_master cap_list rom\n       configuration: driver=nvidia latency=0\n       resources: irq:101 memory:f6000000-f6ffffff memory:e0000000-efffffff memory:f0000000-f1ffffff ioport:e000(size=128) memory:c0000-dffff\n</code></pre></p> <p>Note that line 2 lists the product of TU102 [GeForce RTX 2080 Ti Rev. A] but it does not indicate how much RAM we have on the defice.</p>"},{"location":"admin/gpu-shell/#nvidia-gpu-configuration","title":"NVIDIA GPU Configuration","text":"<pre><code>nvidia-smi\n</code></pre> <p>Result: <pre><code>Mon Sep 25 14:17:46 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  GeForce RTX 208...  Off  | 00000000:09:00.0 Off |                  N/A |\n| 40%   32C    P8    18W / 260W |    184MiB / 11018MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A      1026      G   /usr/lib/xorg/Xorg                 35MiB |\n|    0   N/A  N/A      2315      G   /usr/lib/xorg/Xorg                 90MiB |\n|    0   N/A  N/A      2509      G   /usr/bin/gnome-shell               10MiB |\n|    0   N/A  N/A      4118      G   ...eLanguageDetectionEnabled       34MiB |\n+-----------------------------------------------------------------------------+\n</code></pre></p> <p>This display is difficult to read.  But the key item is in the second column under the <code>Memory-Usage</code> area:</p> <p><code>184MiB / 11018MiB</code></p> <p>This says that we are using 184 megabytes and have 11.018 Gigabyte to use to run our machine learning tasks.</p>"},{"location":"admin/gpu-shell/#nvidia-gpu-monitor","title":"NVIDIA GPU Monitor","text":"<p>The runs similar to the UNIX top command, but for the GPU.</p> <pre><code>watch -d -n 0.5 nvidia-smi\n</code></pre>"},{"location":"admin/gpu-shell/#nvidia-gpu-utilization","title":"NVIDIA GPU Utilization","text":"<p>This shows the GPU running at 42% utilization during the training process.</p> <pre><code>nvidia-smi\nMon Jul 26 20:24:16 2021       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA GeForce ...  Off  | 00000000:09:00.0  On |                  N/A |\n| 41%   49C    P2   136W / 260W |  10892MiB / 11016MiB |     42%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A      1327      G   /usr/lib/xorg/Xorg                 18MiB |\n|    0   N/A  N/A      1398      G   /usr/bin/gnome-shell               71MiB |\n|    0   N/A  N/A      1574      G   /usr/lib/xorg/Xorg                155MiB |\n|    0   N/A  N/A      1705      G   /usr/bin/gnome-shell               32MiB |\n|    0   N/A  N/A     23722      G   ...AAAAAAAAA= --shared-files       25MiB |\n|    0   N/A  N/A     27071      G   ...AAAAAAAAA= --shared-files        9MiB |\n|    0   N/A  N/A     32486      C   ...a3/envs/donkey/bin/python    10571MiB |\n+-----------------------------------------------------------------------------+\n</code></pre>"},{"location":"admin/joystick/","title":"Joystick","text":"<p>Logitec F710 Game Controller for DonkeyCar</p> <p>https://docs.donkeycar.com/parts/controllers/</p> <p>Testing to see if the Nano Recognizes the F710 USB Dongle You can use the \"lsusb\" UNIX shell command to list all the USB devices:</p> <p>$ lsusb Bus 002 Device 002: ID 0bda:0411 Realtek Semiconductor Corp.  Bus 002 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub Bus 001 Device 004: ID 0bda:8179 Realtek Semiconductor Corp. RTL8188EUS 802.11n Wireless Network Adapter Bus 001 Device 005: ID 046d:c21f Logitech, Inc. F710 Wireless Gamepad [XInput Mode] Bus 001 Device 002: ID 0bda:5411 Realtek Semiconductor Corp.  Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub</p> <p>Note that the USB device with an ID of 046d:c21f has been found in the 4th line above.  The first ID before the colon is the device manufacturer (Logiteh) and the second is the id of their device (c21f).  Linux looks this number up in their system and then loads the driver for this type of device.</p> <p>The driver will create a device file in the /dev/input directory called js0</p> <p>$ ls -l /dev/input/js0 crw-rw-r--+ 1 root input 13, 0 Aug 16 19:30 /dev/input/js0</p> <p>The \"c\" in the first letter says that this is a character I/O device.</p> <p>$ sudo apt-get install evtest [sudo] password for dan:  Reading package lists... Done Building dependency tree      Reading state information... Done The following packages were automatically installed and are no longer required:   apt-clone archdetect-deb bogl-bterm busybox-static cryptsetup-bin dpkg-repack gir1.2-timezonemap-1.0 gir1.2-xkl-1.0   grub-common kde-window-manager kinit kio kpackagetool5 kwayland-data kwin-common kwin-data kwin-x11 libdebian-installer4   libkdecorations2-5v5 libkdecorations2private5v5 libkf5activities5 libkf5attica5 libkf5completion-data libkf5completion5   libkf5declarative-data libkf5declarative5 libkf5doctools5 libkf5globalaccel-data libkf5globalaccel5   libkf5globalaccelprivate5 libkf5idletime5 libkf5jobwidgets-data libkf5jobwidgets5 libkf5kcmutils-data libkf5kcmutils5   libkf5kiocore5 libkf5kiontlm5 libkf5kiowidgets5 libkf5newstuff-data libkf5newstuff5 libkf5newstuffcore5 libkf5package-data   libkf5package5 libkf5plasma5 libkf5quickaddons5 libkf5solid5 libkf5solid5-data libkf5sonnet5-data libkf5sonnetcore5   libkf5sonnetui5 libkf5textwidgets-data libkf5textwidgets5 libkf5waylandclient5 libkf5waylandserver5 libkf5xmlgui-bin   libkf5xmlgui-data libkf5xmlgui5 libkscreenlocker5 libkwin4-effect-builtins1 libkwineffects11 libkwinglutils11   libkwinxrenderutils11 libqgsttools-p1 libqt5designer5 libqt5help5 libqt5multimedia5 libqt5multimedia5-plugins   libqt5multimediaquick-p5 libqt5multimediawidgets5 libqt5opengl5 libqt5positioning5 libqt5printsupport5 libqt5qml5   libqt5quick5 libqt5quickwidgets5 libqt5sensors5 libqt5sql5 libqt5test5 libqt5webchannel5 libqt5webkit5 libxcb-composite0   libxcb-cursor0 libxcb-damage0 os-prober python3-dbus.mainloop.pyqt5 python3-icu python3-pam python3-pyqt5   python3-pyqt5.qtsvg python3-pyqt5.qtwebkit python3-sip qml-module-org-kde-kquickcontrolsaddons qml-module-qtmultimedia   qml-module-qtquick2 rdate tasksel tasksel-data Use 'sudo apt autoremove' to remove them. The following additional packages will be installed:   evemu-tools libevemu3 The following NEW packages will be installed:   evemu-tools evtest libevemu3 0 upgraded, 3 newly installed, 0 to remove and 7 not upgraded. Need to get 38.2 kB of archives. After this operation, 191 kB of additional disk space will be used. Do you want to continue? [Y/n] y Get:1 http://ports.ubuntu.com/ubuntu-ports bionic/universe arm64 libevemu3 arm64 2.6.0-0.1 [11.0 kB] Get:2 http://ports.ubuntu.com/ubuntu-ports bionic/universe arm64 evemu-tools arm64 2.6.0-0.1 [12.3 kB] Get:3 http://ports.ubuntu.com/ubuntu-ports bionic/universe arm64 evtest arm64 1:1.33-1build1 [14.9 kB] Fetched 38.2 kB in 1s (56.1 kB/s)  debconf: delaying package configuration, since apt-utils is not installed Selecting previously unselected package libevemu3:arm64. (Reading database ... 140149 files and directories currently installed.) Preparing to unpack .../libevemu3_2.6.0-0.1_arm64.deb ... Unpacking libevemu3:arm64 (2.6.0-0.1) ... Selecting previously unselected package evemu-tools. Preparing to unpack .../evemu-tools_2.6.0-0.1_arm64.deb ... Unpacking evemu-tools (2.6.0-0.1) ... Selecting previously unselected package evtest. Preparing to unpack .../evtest_1%3a1.33-1build1_arm64.deb ... Unpacking evtest (1:1.33-1build1) ... Setting up evtest (1:1.33-1build1) ... Processing triggers for libc-bin (2.27-3ubuntu1) ... Processing triggers for man-db (2.8.3-2ubuntu0.1) ... Setting up libevemu3:arm64 (2.6.0-0.1) ... Setting up evemu-tools (2.6.0-0.1) ... Processing triggers for libc-bin (2.27-3ubuntu1) ... dan@danm-nano:~$ </p> <p>Now run it:</p> <p>$ evtest</p> <p>No device specified, trying to scan all of /dev/input/event* Not running as root, no devices may be available. Available devices: /dev/input/event2:  Logitech Gamepad F710</p> <p>Select the device event number [0-2]: 2</p> <p>Logitech Gamepad F710 Input driver version is 1.0.1 Input device ID: bus 0x3 vendor 0x46d product 0xc21f version 0x305 Input device name: \"Logitech Gamepad F710\" Supported events:   Event type 0 (EV_SYN)   Event type 1 (EV_KEY)     Event code 304 (BTN_SOUTH)     Event code 305 (BTN_EAST)     Event code 307 (BTN_NORTH)     Event code 308 (BTN_WEST)     Event code 310 (BTN_TL)     Event code 311 (BTN_TR)     Event code 314 (BTN_SELECT)     Event code 315 (BTN_START)     Event code 316 (BTN_MODE)     Event code 317 (BTN_THUMBL)     Event code 318 (BTN_THUMBR)   Event type 3 (EV_ABS)     Event code 0 (ABS_X)       Value    128       Min   -32768       Max    32767       Fuzz      16       Flat     128     Event code 1 (ABS_Y)       Value   -129       Min   -32768       Max    32767       Fuzz      16       Flat     128     Event code 2 (ABS_Z)       Value      0       Min        0       Max      255     Event code 3 (ABS_RX)       Value    128       Min   -32768       Max    32767       Fuzz      16       Flat     128     Event code 4 (ABS_RY)       Value   -129       Min   -32768       Max    32767       Fuzz      16       Flat     128     Event code 5 (ABS_RZ)       Value      0       Min        0       Max      255     Event code 16 (ABS_HAT0X)       Value      0       Min       -1       Max        1     Event code 17 (ABS_HAT0Y)       Value      0       Min       -1       Max        1 Properties: Testing ... (interrupt to exit)</p> <p>Now as you press any key or move any joystick you will see the events.</p> <p>When I press the yellow Y we see: Event: time 1566006064.962158, type 1 (EV_KEY), code 308 (BTN_WEST), value 1 Event: time 1566006064.962158, -------------- SYN_REPORT ------------ Event: time 1566006065.129981, type 1 (EV_KEY), code 308 (BTN_WEST), value 0 Event: time 1566006065.129981, -------------- SYN_REPORT ------------</p> <p>Blue X Event: time 1566006110.047015, type 1 (EV_KEY), code 307 (BTN_NORTH), value 1 Event: time 1566006110.047015, -------------- SYN_REPORT ------------ Event: time 1566006110.182606, type 1 (EV_KEY), code 307 (BTN_NORTH), value 0 Event: time 1566006110.182606, -------------- SYN_REPORT ------------</p> <p>Red B Event: time 1566006143.423217, type 1 (EV_KEY), code 305 (BTN_EAST), value 1 Event: time 1566006143.423217, -------------- SYN_REPORT ------------ Event: time 1566006143.499642, type 1 (EV_KEY), code 305 (BTN_EAST), value 0 Event: time 1566006143.499642, -------------- SYN_REPORT ------------</p> <p>Green A Event: time 1566006184.060282, type 1 (EV_KEY), code 304 (BTN_SOUTH), value 1 Event: time 1566006184.060282, -------------- SYN_REPORT ------------ Event: time 1566006184.128408, type 1 (EV_KEY), code 304 (BTN_SOUTH), value 0 Event: time 1566006184.128408, -------------- SYN_REPORT ------------</p> <p>Moving the joystick generates: Event: time 1566006255.549652, -------------- SYN_REPORT ------------ Event: time 1566006255.553650, type 3 (EV_ABS), code 1 (ABS_Y), value -10923 Event: time 1566006255.553650, -------------- SYN_REPORT ------------ Event: time 1566006255.557650, type 3 (EV_ABS), code 1 (ABS_Y), value -14264 Event: time 1566006255.557650, -------------- SYN_REPORT ------------ Event: time 1566006255.561652, type 3 (EV_ABS), code 1 (ABS_Y), value -18633</p>"},{"location":"admin/nvidia-driver-install/","title":"Install the NVIDIA Driver","text":"<p>Ideally you should be able to use the Ubuntu \"Software and Updates\" tool to install the NIVIDA driver.  This usually works, but if you get errors, you may need to use the unix shell.</p>"},{"location":"admin/nvidia-driver-install/#nvidia-card-verification","title":"NVIDIA Card Verification","text":"<p>You can first verify that the GPU card has been installed and powered up.  We can use the \"list hardware\" command with the display option:</p> <pre><code>$ sudo lshw -C display\n</code></pre> <pre><code>  *-display UNCLAIMED       \n       description: VGA compatible controller\n       product: GV102\n       vendor: NVIDIA Corporation\n       physical id: 0\n       bus info: pci@0000:09:00.0\n       version: a1\n       width: 64 bits\n       clock: 33MHz\n       capabilities: pm msi pciexpress vga_controller bus_master cap_list\n       configuration: latency=0\n       resources: memory:f6000000-f6ffffff memory:e0000000-efffffff memory:f0000000-f1ffffff ioport:e000(size=128) memory:c0000-dffff\n</code></pre> <p>This shows that there is a GPU card installed but not claimed by the display.</p>"},{"location":"admin/nvidia-driver-install/#nvidia-devices","title":"NVIDIA Devices","text":"<p>You can then use the ubuntu-drivers command to see the devices.</p> <pre><code>$ ubuntu-drivers devices\n== /sys/devices/pci0000:00/0000:00:03.1/0000:09:00.0 ==\nmodalias : pci:v000010DEd00001E07sv000010DEsd000012A4bc03sc00i00\nvendor   : NVIDIA Corporation\ndriver   : nvidia-driver-470 - distro non-free recommended\ndriver   : nvidia-driver-460-server - distro non-free\ndriver   : nvidia-driver-418-server - distro non-free\ndriver   : nvidia-driver-460 - distro non-free\ndriver   : nvidia-driver-450-server - distro non-free\ndriver   : xserver-xorg-video-nouveau - distro free builtin\n\n== /sys/devices/pci0000:00/0000:00:01.2/0000:02:00.0/0000:03:04.0/0000:05:00.0 ==\nmodalias : pci:v00008086d00002723sv00008086sd00000084bc02sc80i00\nvendor   : Intel Corporation\nmanual_install: True\ndriver   : backport-iwlwifi-dkms - distro free\n</code></pre>"},{"location":"admin/nvidia-driver-install/#ubuntu-drivers-autoinstall","title":"Ubuntu Drivers Autoinstall","text":"<pre><code>sudo ubuntu-drivers autoinstall\n</code></pre> <p>This tool will tell you what drivers you need to install.</p> <pre><code>sudo apt-get install nvidia-driver-470\n</code></pre> <p>This will often generate errors but it will indicate what other libraries need to be installed for the 470 driver to work.</p>"},{"location":"admin/nvidia-driver-install/#final-test","title":"Final Test","text":"<p>Now we are ready to probe the full GPU and get all the statistics of what is in the GPU.</p> <pre><code>nvidia-smi\n</code></pre> <pre><code>Thu Jul 22 22:59:36 2021       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA GeForce ...  Off  | 00000000:09:00.0 Off |                  N/A |\n| 41%   36C    P8     2W / 260W |    283MiB / 11016MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A      1327      G   /usr/lib/xorg/Xorg                 18MiB |\n|    0   N/A  N/A      1398      G   /usr/bin/gnome-shell               71MiB |\n|    0   N/A  N/A      1574      G   /usr/lib/xorg/Xorg                 98MiB |\n|    0   N/A  N/A      1705      G   /usr/bin/gnome-shell               91MiB |\n+-----------------------------------------------------------------------------+\n</code></pre> <p>If you don't get this or a similar display, you must continue to search for installation instructions.</p> <p>After you get this screen you can reboot.</p>"},{"location":"admin/nvidia-driver-install/#cuda-version","title":"CUDA Version","text":"<pre><code>nvcc --version\n</code></pre> <p>Results:</p> <pre><code>nvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2017 NVIDIA Corporation\nBuilt on Fri_Nov__3_21:07:56_CDT_2017\nCuda compilation tools, release 9.1, V9.1.85\n</code></pre>"},{"location":"admin/nvidia-driver-install/#cuda-tookkit-install-for-pytorch","title":"CUDA Tookkit Install for PyTorch","text":"<pre><code>conda install cudatoolkit=&lt;CUDA Version&gt; -c pytorch\n</code></pre> <pre><code>conda install cudatoolkit=11 -c pytorch\n</code></pre>"},{"location":"admin/tensorflow-gpu-install/","title":"Tensorflow GPU Software","text":"<p>$ conda install tensorflow-gpu==2.2.0 Collecting package metadata (current_repodata.json): done Solving environment: failed with initial frozen solve. Retrying with flexible solve. Collecting package metadata (repodata.json): done Solving environment: done</p>"},{"location":"admin/tensorflow-gpu-install/#package-plan","title":"Package Plan","text":"<p>environment location: /home/arl/miniconda3/envs/donkey</p> <p>added / updated specs:     - tensorflow-gpu==2.2.0</p> <p>The following packages will be downloaded:</p> <pre><code>package                    |            build\n---------------------------|-----------------\ncudatoolkit-10.1.243       |       h6bb024c_0       347.4 MB\ncudnn-7.6.5                |       cuda10.1_0       179.9 MB\ncupti-10.1.168             |                0         1.4 MB\ntensorflow-2.2.0           |gpu_py37h1a511ff_0           4 KB\ntensorflow-base-2.2.0      |gpu_py37h8a81be8_0       181.7 MB\ntensorflow-gpu-2.2.0       |       h0d30ee6_0           3 KB\n------------------------------------------------------------\n                                       Total:       710.4 MB\n</code></pre> <p>The following NEW packages will be INSTALLED:</p> <p>cudatoolkit        pkgs/main/linux-64::cudatoolkit-10.1.243-h6bb024c_0   cudnn              pkgs/main/linux-64::cudnn-7.6.5-cuda10.1_0   cupti              pkgs/main/linux-64::cupti-10.1.168-0   tensorflow-gpu     pkgs/main/linux-64::tensorflow-gpu-2.2.0-h0d30ee6_0</p> <p>The following packages will be DOWNGRADED:</p> <p>_tflow_select                                   2.3.0-mkl --&gt; 2.1.0-gpu   tensorflow                       2.2.0-mkl_py37h6e9ce2d_0 --&gt; 2.2.0-gpu_py37h1a511ff_0   tensorflow-base                  2.2.0-mkl_py37hd506778_0 --&gt; 2.2.0-gpu_py37h8a81be8_0</p> <p>Proceed ([y]/n)? Y</p> <p>Downloading and Extracting Packages tensorflow-base-2.2. | 181.7 MB  | ################################################################################################################################################################ | 100%  cudnn-7.6.5          | 179.9 MB  | ################################################################################################################################################################ | 100%  cupti-10.1.168       | 1.4 MB    | ################################################################################################################################################################ | 100%  tensorflow-2.2.0     | 4 KB      | ################################################################################################################################################################ | 100%  tensorflow-gpu-2.2.0 | 3 KB      | ################################################################################################################################################################ | 100%  cudatoolkit-10.1.243 | 347.4 MB  | ################################################################################################################################################################ | 100%  Preparing transaction: done Verifying transaction: done Executing transaction: done</p>"},{"location":"events/computer-vision/","title":"Computer Vision Table","text":""},{"location":"events/computer-vision/#table-setup","title":"Table Setup","text":"<ol> <li>A Raspberry Pi or NIVIDA Nano with a camera</li> <li>Face detection software running</li> <li>Some OpenCV demos</li> </ol>"},{"location":"events/hardware/","title":"AI Racing League Hardware Table","text":"<p>This table shows participants what hardware is used in the events.</p> <ol> <li>Sample assembled car</li> <li>Batteries</li> <li>Motors</li> <li>Servos</li> <li>PWM Boards</li> <li>Cameras</li> </ol>"},{"location":"events/introduction/","title":"Introduction to AI Racing Leagure Events","text":""},{"location":"events/introduction/#seleting-a-venue","title":"Seleting a Venue","text":""},{"location":"events/introduction/#getting-a-track","title":"Getting a Track","text":""},{"location":"events/introduction/#learning-stations","title":"Learning Stations","text":""},{"location":"events/jupyter-notebooks/","title":"Jupyter Notebooks","text":""},{"location":"events/jupyter-notebooks/#table-setup","title":"Table Setup","text":"<ol> <li>A single PC with a large screen</li> <li>Sample Jupyter Notebooks for analizeing drive information</li> </ol> <p>Question: Have you ever used Jupyter Notebooks?</p> <p>They are used by data scientists to manage data analysis projects.</p> <p>Drive data is a collection of images that has the speed and steering direction associated with it.</p> <p>How would you use them to analyze your drive data?</p>"},{"location":"events/unix/","title":"UNIX Table","text":"<ol> <li>Concept cards for UNIX</li> <li>UNIX Cheat Sheet</li> <li>A Laptop with a large screen</li> </ol>"},{"location":"events/unix/#sample-script","title":"Sample Script","text":"<ol> <li>Are you familar with UNIX shell commands?</li> <li>Open the Terminal or DOS PowerShell</li> <li>Describe how you type in commands with parameters</li> <li>Show how you can use \"Secure Shell\" to get into a Donkey Car</li> <li>Show how you can use \"scp\" to securly copy files to the GPU</li> </ol>"},{"location":"lesson-plans/car-structure/","title":"RC Car Structure","text":""},{"location":"lesson-plans/car-structure/#using-gptchat-to-help-generate-block-diagrams","title":"Using GPTChat To Help Generate Block Diagrams","text":"<p>Generating architecture diagrams can be a slow and painful process. But ChatGPT can help.  here is a process of generating a block diagram using ChatGPT.  We can use the Mermaid Live program to view the diagram.  ChatGPT also has a <code>diagram</code> plugin that will render the image within ChatGPT.</p>"},{"location":"lesson-plans/car-structure/#gptchat-prompt","title":"GPTChat Prompt","text":"<pre><code>Create a mermaid block diagram for the structure of a Donkey Car.  Make sure to label all the components and their connection types.  Return the results in raw mermaid format.  Include the following structures:\n\n1. The main computer (Raspberry Pi or NVIDIA Nano)\n2. The main computer has an I2C connection to the Servo Controller board\n3. The Servo Controller connects to the Steering Servo and the Electronic Speed Controller (ESC) via PWM connections\n4. The ESC connects to the RC Car Battery\n5. THE ESC connects to the Car Motor\n6. The 5v battery powers the main computer\n7. There are two batteries: a 5V battery and a 7V RC Car Battery.\n8. The Servo controller is a PCA9685 board.\n</code></pre>"},{"location":"lesson-plans/car-structure/#chatgpt-response","title":"ChatGPT Response","text":"<pre><code>This code will create a block diagram where:\n\nThe \"Main Computer\" (A) connects to the \"Servo Controller\" (B) via an I2C connection.\nThe \"Servo Controller\" (B) connects to both the \"Steering Servo\" (C) and \"Electronic Speed Controller\" (D) via PWM connections.\nThe \"Electronic Speed Controller\" (D) connects to both the \"RC Car Battery\" (E) and the \"Car Motor\" (F).\nThe \"5V Battery\" (G) connects to the \"Main Computer\" (A) as well as to the \"Positive Connection\" (H) and \"Negative Connection\" (I).\nThe \"RC Car Battery\" (E) also connects to both the \"Positive Connection\" (H) and \"Negative Connection\" (I).\nPlease note that this is a simplified diagram and does not necessarily represent the physical arrangement of the components in an actual Donkey Car setup. This diagram should be taken as an abstract representation of how the components interact.\n</code></pre>"},{"location":"lesson-plans/car-structure/#mermaid-diagram","title":"Mermaid Diagram","text":"<pre><code>graph TB\n  A[\"Main Computer&lt;br&gt;Raspberry Pi or NVIDIA Nano\"]\n  B[\"Servo Controller&lt;br&gt;PCA9685 board\"]\n  C[\"Steering Servo\"]\n  D[\"Electronic Speed Controller&lt;br&gt;(ESC)\"]\n  E[\"RC Car Battery (7V)\"]\n  F[\"Car Motor\"]\n  G[\"5V Battery\"]\n\n  A --I2C--&gt; B\n  B --PWM--&gt; C\n  B --PWM--&gt; D\n  D --- E\n  D --- F\n  G --- A\n</code></pre>"},{"location":"lesson-plans/computer-vision/","title":"AI Racing League Computer Vision Table","text":"<p>Raspberry Pi and the NVIDIA Nano are popular systems for demonstrating various computer vision applications due to their affordability and flexibility.</p>"},{"location":"lesson-plans/computer-vision/#requirements","title":"Requirements","text":"<p>For these lessons, you just need a Raspberry Pi (or Nano) and the attached Camera that we use for all our cars.</p>"},{"location":"lesson-plans/computer-vision/#sample-labs","title":"Sample Labs","text":"<p>Here are several demos we show to high school students using OpenCV and Raspberry Pi:</p>"},{"location":"lesson-plans/computer-vision/#face-detection-and-recognition","title":"Face Detection and Recognition","text":"<p>We can use the built-in Haar cascades in OpenCV for face and eyes detection. For the face recognition part, you can use either OpenCV's built-in algorithms or deep learning-based models such as FaceNet.</p>"},{"location":"lesson-plans/computer-vision/#object-detection","title":"Object Detection","text":"<p>Use pre-trained models from OpenCV's DNN module or TensorFlow's model zoo to recognize multiple objects in real-time.</p>"},{"location":"lesson-plans/computer-vision/#optical-character-recognition-ocr","title":"Optical Character Recognition (OCR):","text":"<p>Combine OpenCV for image processing and Tesseract for character recognition to demonstrate how a device can read text from images or real-time video feed.</p>"},{"location":"lesson-plans/computer-vision/#color-detection","title":"Color Detection","text":"<p>Write a simple program that detects specific colors in real-time. This can be used as a stepping stone to more advanced object-tracking projects.</p> <p>We can also combine this lab with our Raspberry Pi Pico color detection sensors.</p>"},{"location":"lesson-plans/computer-vision/#motion-detection-and-tracking","title":"Motion Detection and Tracking","text":"<p>Implement a simple surveillance system that detects motion and tracks moving objects. This can be a good introduction to video analysis.</p>"},{"location":"lesson-plans/computer-vision/#augmented-reality","title":"Augmented Reality","text":"<p>Show how to overlay graphics on a real-time video feed based on detected features. For example, you can use OpenCV's capabilities for feature detection (like SIFT, SURF, ORB) and perspective transformation to overlay 3D objects on a marker.</p>"},{"location":"lesson-plans/computer-vision/#hand-gesture-recognition","title":"Hand Gesture Recognition","text":"<p>Create a program that recognizes hand gestures and associates them with commands. You could use this to control a game or navigate a user interface.</p>"},{"location":"lesson-plans/computer-vision/#license-plate-recognition","title":"License Plate Recognition","text":"<p>You can implement a simple Automatic Number Plate Recognition (ANPR) system using image processing techniques in OpenCV and OCR.</p>"},{"location":"lesson-plans/computer-vision/#qr-code-and-barcode-scanner","title":"QR Code and Barcode Scanner","text":"<p>Use OpenCV for real-time detection and decoding of QR codes and bar codes.</p> <p>Most of these demonstrations will require additional Python libraries beyond just OpenCV, like NumPy, Pillow, or TensorFlow.</p> <p>For hardware, you will need the Raspberry Pi 3 with 4GB RAM, a camera module, and potentially additional items like a monitor, mouse, and keyboard for a fully interactive setup.</p>"},{"location":"lesson-plans/computer-vision/face-filters/","title":"OpenCV Lab: Fun Face Filters**","text":"<p>Objective: Learn to detect faces using OpenCV and apply fun filters like cat whiskers and ears.</p>"},{"location":"lesson-plans/computer-vision/face-filters/#tools-libraries","title":"Tools &amp; Libraries","text":"<ul> <li>Python</li> <li>OpenCV</li> </ul>"},{"location":"lesson-plans/computer-vision/face-filters/#haar-cascades","title":"Haar Cascades","text":""},{"location":"lesson-plans/computer-vision/face-filters/#understanding-haar-cascades-in-opencv","title":"Understanding Haar Cascades in OpenCV","text":"<p>Haar Cascades are a type of machine learning object detection method used to identify objects in images or video. In OpenCV, Haar Cascades are mainly used to detect faces, but they can also identify other objects like eyes, smiles, and more.  There were named after Hungarian mathematician Alfr\u00e9d Haar who make key contributions to the mathematics of transformation of matrix data in the 1920s.</p> <p>Imagine you have a magic magnifying glass that you move across a photo. Whenever this magnifying glass sees a face, it lights up! That's kind of what Haar Cascades does in OpenCV.</p>"},{"location":"lesson-plans/computer-vision/face-filters/#how-do-harr-cascades-work","title":"How Do Harr Cascades Work?","text":"<ol> <li>Features: Haar Cascades work by looking at simple features in an area of the image, like the bridge of the nose being brighter than the eyes on either side because it sticks out and catches the light.</li> <li>Training: To make Haar Cascades 'learn' these features, it's shown many pictures of faces and non-faces. Through this, it learns what a face typically looks like.</li> <li>Cascade: The term 'cascade' is used because the algorithm employs a series of increasingly complex features to determine if a particular region is a face.</li> </ol>"},{"location":"lesson-plans/computer-vision/face-filters/#building-face-filters-using-haar-cascades","title":"Building Face-Filters Using Haar Cascades:","text":"<p>Creating a Snapchat-like face filter using Haar Cascades involves two main steps:</p> <ol> <li>Face Detection: Detecting the location of the face in an image or video stream.</li> <li>Overlaying the Filter: Once we know where the face is, we can overlay our filter (like cat ears or sunglasses) at the correct position.</li> </ol> <p>Example Python Code:</p> <p>```python import cv2</p>"},{"location":"lesson-plans/computer-vision/face-filters/#load-the-haar-cascade-for-face-detection","title":"Load the Haar cascade for face detection","text":"<p>face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')</p>"},{"location":"lesson-plans/computer-vision/face-filters/#read-the-image","title":"Read the image","text":"<p>img = cv2.imread('your_photo.jpg') gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)</p>"},{"location":"lesson-plans/computer-vision/face-filters/#detect-faces","title":"Detect faces","text":"<p>faces = face_cascade.detectMultiScale(gray, 1.1, 4)</p> <p>for (x, y, w, h) in faces:     # Here you'd typically overlay your filter, e.g., cat ears at coordinates (x, y)     cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)  # Drawing a rectangle around the detected face for now</p> <p>cv2.imshow('Face Detection', img) cv2.waitKey()</p> <p>Resources: - Haarcascades for face and eye detection (provided by OpenCV).</p> <p>Steps:</p> <ol> <li> <p>Setup &amp; Installation</p> <p>Make sure you have Python and OpenCV installed.</p> <p><code>bash pip install opencv-python</code></p> </li> <li> <p>Face Detection</p> </li> </ol> <p>Before adding filters, students should understand face detection.</p> <pre><code>```python\nimport cv2\n\nface_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n\nimg = cv2.imread('path_to_image.jpg')\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\nfaces = face_cascade.detectMultiScale(gray, 1.1, 4)\nfor (x, y, w, h) in faces:\n    cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n\ncv2.imshow('Detected Faces', img)\ncv2.waitKey()\n```\n</code></pre> <ol> <li> <p>Designing the Filters</p> </li> <li> <p>Use any graphic software (like GIMP or Photoshop) to design transparent PNG images of cat whiskers and ears.</p> </li> <li> <p>Ensure the filter graphics are adjustable in size or make several versions to fit different face sizes.</p> </li> <li> <p>Applying the Cat Ears Filter</p> <p>Given the coordinates <code>(x, y, w, h)</code> of the detected face:</p> <p>```python ear_image = cv2.imread('path_to_ear_image.png', -1)  # The -1 reads the alpha channel</p> </li> <li> <p>Applying the Whiskers Filter</p> <p>Similar to the cat ears, you can position whiskers on the cheeks using the <code>(x, y, w, h)</code> of the face.</p> </li> <li> <p>Real-time Filters Application</p> <p>Capture video from the webcam and apply the filters in real-time.</p> <p>```python cap = cv2.VideoCapture(0)</p> <p>while True:     ret, frame = cap.read()     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)</p> <pre><code>faces = face_cascade.detectMultiScale(gray, 1.1, 4)\nfor (x, y, w, h) in faces:\n    # Apply cat ears and whiskers filter here\n\ncv2.imshow('Filters in Action', frame)\nif cv2.waitKey(1) &amp; 0xFF == ord('q'):\n    break\n</code></pre> <p>cap.release() cv2.destroyAllWindows() ```</p> </li> <li> <p>Challenge &amp; Extension</p> <ul> <li>Multiple faces: Modify the code to handle multiple faces in one frame.</li> <li>Other Filters: Allow students to design and implement their own filters.</li> <li>Interactivity: Add buttons to change filters or toggle them on/off.</li> </ul> </li> </ol> <p>Assessment:</p> <ol> <li>Can students successfully detect faces in different images?</li> <li>How accurately do the filters get applied to the face?</li> <li>How creative and effective are student-designed filters?</li> </ol> <p>Note: This is a basic version and doesn't incorporate sophisticated technologies like deep learning which Snapchat uses. It's aimed at high school students to give a foundational understanding.</p>"},{"location":"lesson-plans/computer-vision/face-filters/#resize-the-cat-ear-image-to-fit-the-width-of-the-face","title":"Resize the cat ear image to fit the width of the face","text":"<p>ear_width = w aspect_ratio = ear_image.shape[1] / float(ear_image.shape[0]) ear_height = int(ear_width / aspect_ratio) ear_image = cv2.resize(ear_image, (ear_width, ear_height))</p>"},{"location":"lesson-plans/computer-vision/face-filters/#region-where-we-want-to-place-the-cat-ears-taking-care-not-to-exceed-image-dimensions","title":"Region where we want to place the cat ears (taking care not to exceed image dimensions)","text":"<p>for c in range(0, 3):     img[y:y+ear_image.shape[0], x:x+ear_image.shape[1], c] = img[y:y+ear_image.shape[0], x:x+ear_image.shape[1], c] * (1 - ear_image[:, :, 3] / 255.0) + ear_image[:, :, c] * (ear_image[:, :, 3] / 255.0) ```</p>"},{"location":"lesson-plans/computer-vision/open-cv/","title":"OpenCV Labs","text":"<p>Lesson Plan: Introduction to OpenCV in Python</p>"},{"location":"lesson-plans/computer-vision/open-cv/#setup","title":"Setup","text":"<p>Students should have access to a computer with a webcam and Python installed.  We use Rasberry Pi or NIVIDA Nanos with cameras.</p> <p>Objective: By the end of the lesson, students will be able to explain the purpose and basic functions of OpenCV, and implement some basic image processing tasks.</p>"},{"location":"lesson-plans/computer-vision/open-cv/#1-introduction-10-minutes","title":"1. Introduction (10 minutes)","text":"<ul> <li>Discussion: Ask students if they've ever used Instagram or Snapchat filters, or how computers recognize faces or objects.</li> <li>Explanation: Introduce OpenCV as one of the most powerful libraries used for computer vision tasks. </li> </ul>"},{"location":"lesson-plans/computer-vision/open-cv/#2-brief-history-applications-10-minutes","title":"2. Brief History &amp; Applications (10 minutes)","text":"<ul> <li>Mention OpenCV's origins and its significance in AI and robotics.</li> <li>Showcase a few applications, e.g., facial recognition, self-driving cars, AR filters.</li> </ul>"},{"location":"lesson-plans/computer-vision/open-cv/#3-basics-of-image-representation-10-minutes","title":"3. Basics of Image Representation (10 minutes)","text":"<ul> <li>Discuss how computers see images as matrices of numbers.</li> <li>Quick overview: Images are made of pixels; each pixel has values that determine its color.</li> </ul>"},{"location":"lesson-plans/computer-vision/open-cv/#4-installation-setup-10-minutes","title":"4. Installation &amp; Setup (10 minutes)","text":"<ul> <li> <p>Demo: How to install OpenCV using <code>pip</code>.</p> <p>pip install opencv-python</p> </li> <li> <p>Quick navigation of the OpenCV documentation to encourage self-learning.</p> </li> </ul>"},{"location":"lesson-plans/computer-vision/open-cv/#5-hands-on-lab-1-reading-displaying-and-saving-images-20-minutes","title":"5. Hands-on Lab 1: Reading, Displaying, and Saving Images (20 minutes)","text":"<ul> <li>Exercise: </li> <li>Use OpenCV to read an image.</li> <li>Display the image in a window.</li> <li>Convert the image to grayscale.</li> <li>Save the grayscale image.</li> </ul>"},{"location":"lesson-plans/computer-vision/open-cv/#6-hands-on-lab-2-playing-with-webcam-feed-20-minutes","title":"6. Hands-on Lab 2: Playing with Webcam Feed (20 minutes)","text":"<ul> <li>Exercise:</li> <li>Access the webcam using OpenCV.</li> <li>Display the live video feed.</li> <li>Apply a grayscale filter to the feed.</li> <li>Bonus: Add a button or a keypress event to capture and save a snapshot from the feed.</li> </ul>"},{"location":"lesson-plans/computer-vision/open-cv/#7-introduction-to-basic-image-processing-techniques-15-minutes","title":"7. Introduction to Basic Image Processing Techniques (15 minutes)","text":"<ul> <li>Explanation:</li> <li>Discuss image thresholding, blurring, and edge detection.</li> <li>Showcase examples of each technique.</li> </ul>"},{"location":"lesson-plans/computer-vision/open-cv/#8-hands-on-lab-3-basic-image-processing-30-minutes","title":"8. Hands-on Lab 3: Basic Image Processing (30 minutes)","text":"<ul> <li>Exercise:</li> <li>Use a sample image (or one they choose).</li> <li>Apply and display thresholding.</li> <li>Apply and display blurring.</li> <li>Apply and display edge detection using the Canny edge detector.</li> </ul>"},{"location":"lesson-plans/computer-vision/open-cv/#9-fun-lab-snapchat-like-filters-45-minutes","title":"9. Fun Lab: Snapchat-like Filters (45 minutes)","text":"<ul> <li> <p>Objective: The goal of this lab is to have students use OpenCV to create basic filters for a live webcam feed. </p> </li> <li> <p>Exercise:</p> </li> <li>Access the webcam feed.</li> <li>Overlay cartoon glasses or hats on the user's face using OpenCV's face detection (Haar cascades).</li> <li>Bonus: Let the students get creative, e.g., adding mustaches, changing background, etc.</li> </ul>"},{"location":"lesson-plans/computer-vision/open-cv/#10-discussion-wrap-up-15-minutes","title":"10. Discussion &amp; Wrap-up (15 minutes)","text":"<ul> <li>Reflect on the potential of computer vision and its real-world applications.</li> <li>Discuss the ethical implications, such as privacy concerns with facial recognition.</li> <li>Introduce more advanced topics in OpenCV for those interested (like object recognition, machine learning with OpenCV, etc.)</li> <li>Q&amp;A session.</li> </ul>"},{"location":"lesson-plans/computer-vision/open-cv/#additional-resources-take-home-assignments","title":"Additional Resources &amp; Take-home Assignments:","text":"<ol> <li>Explore More: Give students links to OpenCV tutorials and documentation for further reading.</li> <li>Project: Ask students to work on a mini-project, like a basic digital photo editor using OpenCV, allowing them to apply filters, rotate, and crop images.</li> <li>Challenge: For advanced students, introduce them to object detection using pre-trained models in OpenCV.</li> </ol>"},{"location":"lesson-plans/computer-vision/open-cv/#notes-for-the-instructor","title":"Notes for the Instructor:","text":"<ul> <li>Make sure all students have Python installed and guide them in setting up a virtual environment.</li> <li>Visual aids, like slides with images representing pixel values, will help in explaining image representation.</li> <li>Encourage students to collaborate and share their findings or issues during labs. Pair programming can be useful.</li> <li>Make sure to have a few sample images ready for labs, preferably with varying complexities.</li> </ul>"},{"location":"lesson-plans/data-analysis/","title":"Data Analysis","text":"<p>In these lessons, we learn how to write some basic data analysis Python programs.</p> <p>In the real world, you are often given some data and people ask us \"Tell me what insights you can give me about this data.\"  This forms the basis of a field of data science called \"EDA\" for \"Electronic Data Analysis\".</p> <p>For example, say you are on a project to get cars to drive using machine learning.  What insights can you derive from the sample images and driving data?</p>"},{"location":"lesson-plans/data-analysis/#numpy-profiler","title":"Numpy Profiler","text":"<p>TBD</p>"},{"location":"lesson-plans/data-analysis/01-intro/","title":"AI Racing League Data Analysis","text":""},{"location":"lesson-plans/data-analysis/01-intro/#why-analysis","title":"Why Analysis?","text":"<p>Data analysis is a core part of building accurate models that create high quality predictions.  Here are some sample analytics tasks:</p> <ol> <li>Understand what data we have</li> <li>Browse data sets</li> <li>Run metrics that count the number of items in a dataset</li> <li>Open sample items and view sample images</li> <li>Look for groupings of data</li> <li>Look at averages of values</li> <li>Remove poor training data</li> </ol>"},{"location":"lesson-plans/data-analysis/01-intro/#tools-python-and-jupyter-notebooks","title":"Tools: Python and Jupyter Notebooks","text":""},{"location":"lesson-plans/data-analysis/01-intro/#libraries-os-image-numpy-dataframes","title":"Libraries: os, image, numpy, dataframes","text":"<pre><code>import os\nfrom IPython.display import Image\n\nimage_dir = \"/home/arl/mycar/data/dans-msp/data/images\"\n\nfiles = os.listdir(image_dir)\n# last basement image is 1710\nn = 1710\nfile_n = files[n]\nfile_2 = files[n+1]\nprint(n, file_n)\nfile_path1 = image_dir + '/' + file_n\nfile_path2 = image_dir + '/' + file_2\ni1 = Image(file_path1)\ni2 = Image(file_path2)\nprint(n+1, file_2)\n\ndisplay(i1, i2)\n</code></pre>"},{"location":"lesson-plans/data-analysis/02-listing-files/","title":"Working with Files","text":""},{"location":"lesson-plans/data-analysis/02-listing-files/#listing-files-with-the-os-library","title":"Listing Files with the OS library","text":"<p>Python provides a powerful library for working with Operating System resources like file systems.  We will start out with the <code>listdir()</code> function that lists the files in a directory.</p> <p>Here is program that lists all the tub files in our mycar/data directory:</p> <pre><code>import os\ndata_dir = \"/home/arl/mycar/data/\"\n\nfiles = os.listdir(data_dir)\nfor file in files:\n    print(file)\n</code></pre> <p>returns:</p> <pre><code>a-test-tub\nmy-test-tub\njunk-tub\nproduction-run\ntub-47\n</code></pre>"},{"location":"lesson-plans/data-analysis/02-listing-files/#listing-files-in-a","title":"Listing Files in a","text":"<pre><code>import os\nfrom IPython.display import Image\n\nimage_dir = \"/home/arl/mycar/data/dans-msp/data/images\"\n\nfiles = os.listdir(image_dir)\n# last basement image is 1710\nn = 1710\nfile_n = files[n]\nfile_2 = files[n+1]\nprint(n, file_n)\nfile_path1 = image_dir + '/' + file_n\nfile_path2 = image_dir + '/' + file_2\ni1 = Image(file_path1)\ni2 = Image(file_path2)\nprint(n+1, file_2)\n\ndisplay(i1, i2)\n</code></pre>"},{"location":"lesson-plans/data-analysis/02-listing-files/#list-random-files-in-images-directory","title":"List Random Files In Images Directory","text":"<pre><code>import os\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image\n\nimage_dir = \"/home/arl/mycar/data/dans-msp/data/images\"\nimage_file_name_list = os.listdir(image_dir)\n\nfor index in range(0,10):\n    file_name = image_file_name_list[index]\n    print(file_name)\n</code></pre> <p>returns:</p> <pre><code>10263_cam_image_array_.jpg\n6257_cam_image_array_.jpg\n15248_cam_image_array_.jpg\n3916_cam_image_array_.jpg\n5223_cam_image_array_.jpg\n15765_cam_image_array_.jpg\n8437_cam_image_array_.jpg\n5871_cam_image_array_.jpg\n16681_cam_image_array_.jpg\n15281_cam_image_array_.jpg\n</code></pre> <p>Note that the files are not in any specific order.</p>"},{"location":"lesson-plans/data-analysis/02-listing-files/#show-images-for-10-random-files","title":"Show Images for 10 Random Files","text":"<pre><code>import glob\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\n\nimages = []\nfor img_path in glob.glob('/home/arl/mycar/data/dans-msp/data/images/*.jpg'):\n    images.append(mpimg.imread(img_path))\n\nplt.figure(figsize=(20,10))\ncolumns = 5\nfor i, image in enumerate(images):\n    plt.subplot(len(images) / columns + 1, columns, i + 1)\n    plt.imshow(image)\n</code></pre>"},{"location":"lesson-plans/data-analysis/02-listing-files/#sorting-images-by-file-name","title":"Sorting Images By File Name","text":"<p>We can add an additional step if we want to sort the images by the file name:</p> <pre><code>image_file_name_list = os.listdir(image_dir)\nimage_file_name_list.sort()\n</code></pre>"},{"location":"lesson-plans/data-analysis/02-listing-files/#return-images-based-on-creation-date","title":"Return Images Based On Creation Date","text":"<pre><code>import os\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image\nfrom pathlib import Path\n\nimage_dir = \"/home/arl/mycar/data/dans-msp/data/images\"\npaths = sorted(Path(image_dir).iterdir(), key=os.path.getmtime)\n\nimages = []\n# just get the first 10 items in the list of images\nfor path in paths[:10]:\n    print(path)\n    images.append(mpimg.imread(path))\n\nplt.figure(figsize=(20,10))\ncolumns = 5\nfor i, image in enumerate(images):\n    plt.subplot(len(images) / columns + 1, columns, i + 1)\n    plt.imshow(image)\n</code></pre>"},{"location":"lesson-plans/data-analysis/03-viewing-images/","title":"Viewing Images","text":""},{"location":"lesson-plans/data-analysis/03-viewing-images/#viewing-a-single-jpg-image","title":"Viewing a Single JPG Image","text":""},{"location":"lesson-plans/data-analysis/04-viewing-catalog-files/","title":"Viewing Catalog Files","text":"<p>The data about each image, sometimes called the image \"metadata\", is stored in a file that ends with the file extension .catalog.  If you open these files, you will see a simple layout that looks like the following:</p> <pre><code>{'_index': 16000, '_session_id': '21-07-20_0', '_timestamp_ms': 1626797545360, 'cam/image_array': '16000_cam_image_array_.jpg', 'user/angle': 1.0, 'user/mode': 'user', 'user/throttle': 0.5}\n{'_index': 16001, '_session_id': '21-07-20_0', '_timestamp_ms': 1626797545411, 'cam/image_array': '16001_cam_image_array_.jpg', 'user/angle': 0.37, 'user/mode': 'user', 'user/throttle': 0.7}\n{'_index': 16002, '_session_id': '21-07-20_0', '_timestamp_ms': 1626797545460, 'cam/image_array': '16002_cam_image_array_.jpg', 'user/angle': -0.23, 'user/mode': 'user', 'user/throttle': 0.25}\n</code></pre> <p>This file consists of multiple lines, each line starts and ends with curly braces \"{\" and \"}\".  Within these curly braces are a set of key-value pairs where the label is a string in single quotes followed by a colon, the value and a comma.  This file uses newlines to separate records and a JSON object format within each single line.  Note this is NOT a full JSON file format so you can't just use a standard JSON library to read the catalog file.</p> <p>Here is that format with a the key and value on separate lines to make the line easier to read.</p> <pre><code>{\n'_index': 16000,\n'_session_id': '21-07-20_0',\n'_timestamp_ms': 1626797545360,\n'cam/image_array':\n'16000_cam_image_array_.jpg',\n'user/angle': 1.0,\n'user/mode': 'user',\n'user/throttle': 0.5\n}\n</code></pre> <p>This format is very similar to a JSON file with the following exceptions:</p> <ol> <li>There is no root element to tell us when the JSON file starts and ends</li> <li>There are no commas between the item</li> </ol> <p>Here is what a properly formatted JSON file would look like:</p> <pre><code>{\n\"driveData\": [\n{\n\"_index\": 16000,\n\"_session_id\": \"21-07-20_0\",\n\"_timestamp_ms\": 1626797545360,\n\"cam/image_array\": \"16000_cam_image_array_.jpg\",\n\"user/angle\": 1.0,\n\"user/mode\": \"user\",\n\"user/throttle\": 0.5\n},\n{\n\"_index\": 16001,\n\"_session_id\": \"21-07-20_0\",\n\"_timestamp_ms\": 1626797545411,\n\"cam/image_array\": \"16001_cam_image_array_.jpg\",\n\"user/angle\": 0.37,\n\"user/mode\": \"user\",\n\"user/throttle\": 0.70\n},\n{\n\"_index\": 16002,\n\"_session_id\": \"21-07-20_0\",\n\"_timestamp_ms\": 1626797545460,\n\"cam/image_array\": \"16002_cam_image_array_.jpg\",\n\"user/angle\": -0.23,\n\"user/mode\": \"user\",\n\"user/throttle\": 0.25\n}\n]\n}\n</code></pre> <p>Here is a sample JSON file reader that would read this file:</p> <pre><code># program to read a DonkeyCar Catalog File\nimport os,json\n\n# this program assumes that test.json is in the same directory as this script\n# get the direcotry that this script is running\nscript_dir = os.path.dirname(__file__)\n# get a relative path to the script dir\npath_to_json_file = script_dir + '/test.json'\n\n# Open the JSON test file for read only\nf = open(path_to_json_file, 'r')\n\n# returns JSON object as a dictionary\ndata = json.load(f)\n\n# Iterating through the json file for the items in the drive data dictionary\nfor i in data['driveData']:\n    print(i)\n\n# Close the JSON file\nf.close()\n</code></pre> <p>Note that the open() function reads the file with the \"r\" option which indicates read-only mode.</p> <p>Although this format would make reading the file simple, there are some disadvantages.  The key is that individual lines in the new catalog format are atomic units of storage and the files can be easily split and joined using line-by-line tools.</p>"},{"location":"lesson-plans/data-analysis/04-viewing-catalog-files/#reading-catalog-lines-to-json-objects","title":"Reading Catalog Lines to JSON Objects","text":"<p>To read in the values of the catalog file we will open using a line-oriented data structure assuming that there is a newline at the end of each record.  We can then just the json library's <code>loads()</code> function which will convert each line to a JSON object.</p> <p>Sample Objects.json file:</p> <pre><code>{\"name\":\"Ann\",\"age\":15}\n{\"name\":\"Peggy\",\"age\":16}\n{\"name\":\"Rima\",\"age\":13}\n{\"name\":\"Sue\",\"age\":14}\n</code></pre> <pre><code>import os, json    \njson_file = \"objects.json\"\nscript_dir = os.path.dirname(__file__)\n\n# get a relative path to the script dir\npath_to_catalog_file = script_dir + '/' + json_file\n\nf = open(path_to_catalog_file)\nlines = f.readlines()\n\ncount = 0\n# Convert each line to a JSON object\nfor line in lines:\n    line_in_json = json.loads(line)\n    count += 1\n    print(count, ' ', end='')\n    print(line_in_json)\n    # the result is a Python dictionary\n    print(line_in_json['name'])\n    print(\"Name:\", line_to_json[\"name\"] )\n    print(\"Age:\", line_to_json[\"age\"] )\n</code></pre> <p>Returns</p> <pre><code>1  {'name': 'Ann', 'age': 15}\nName: Ann\nAge: 15\n2  {'name': 'Peggy', 'age': 16}\nName: Peggy\nAge: 16\n3  {'name': 'Rima', 'age': 13}\nName: Rima\nAge: 13\n4  {'name': 'Sue', 'age': 14}\nName: Sue\nAge: 14\n</code></pre>"},{"location":"lesson-plans/data-analysis/04-viewing-catalog-files/#sample-catalog-reader-program","title":"Sample CataLog Reader Program","text":"<pre><code># program to read a DonkeyCar Catalog File\nimport os,json\n\n# this program assumes that test.catalog is in the same directory as this script\n# get the direcotry that this script is running\nscript_dir = os.path.dirname(__file__)\n\n# get a relative path to the script dir\npath_to_catalog_file = script_dir + '/test.catalog'\n\nf = open(path_to_catalog_file)\nlines = f.readlines()\n\ncount = 0\n# Convert each line to a JSON object\nfor line in lines:\n    # each line as a JSON dictionary object\n    j = json.loads(line)\n    count += 1\n    print('\\n\\nline:', count)\n    # print(j)\n    print(\"Index:\", j[\"_index\"] )\n    print(\"Session:\", j[\"_session_id\"] )\n    print(\"Timestamp:\", j[\"_timestamp_ms\"] )\n    print(\"cam/image_array:\", j[\"cam/image_array\"] )\n    print(\"user/angle:\", j[\"user/angle\"] )\n    print(\"user/mode:\", j[\"user/mode\"] )\n    print(\"user/throttle:\", j[\"user/throttle\"] )\n</code></pre> <p>returns:</p> <pre><code>line: 1\nIndex: 16000\nSession: 21-07-20_0\nTimestamp: 1626797545360\ncam/image_array: 16000_cam_image_array_.jpg\nuser/angle: 1.0\nuser/mode: user\nuser/throttle: 0.31\n\n\nline: 2\nIndex: 16001\nSession: 21-07-20_0\nTimestamp: 1626797545411\ncam/image_array: 16001_cam_image_array_.jpg\nuser/angle: 0.3715323343607898\nuser/mode: user\nuser/throttle: 0.31\n\n\nline: 3\nIndex: 16002\nSession: 21-07-20_0\nTimestamp: 1626797545460\ncam/image_array: 16002_cam_image_array_.jpg\nuser/angle: 0.2371288186284982\nuser/mode: user\nuser/throttle: 0.31\n</code></pre>"},{"location":"lesson-plans/data-analysis/04-viewing-catalog-files/#reference","title":"Reference","text":"<p>The Python class that creates version 2 of the catalog files is here</p>"},{"location":"lesson-plans/data-analysis/05-catalog-statistics/","title":"Catalog Statistics","text":"<p>Now that we know how to reach each item in the tub catalog, we can now do some simple statistics on this data.  For example we can calculate the average throttle and steering angle and create some plots of the distribution of these values.</p>"},{"location":"lesson-plans/data-analysis/05-catalog-statistics/#calculating-average-throttle-and-angle","title":"Calculating Average Throttle and Angle","text":"<p>When we drive around the track each image records both the throttle and steering values at the instant the image was taken by the camera.  Although the values sent to the Electronic Speed Controller (ESC) and the servo are unique to every car, instead we store values that have been converted to a range between 0 and 1.  Both these values are Normalized to values of between 0 and 1.</p> <pre><code># program to read a DonkeyCar Catalog File\nimport os,json\n\n# this program assumes that test.catalog is in the same directory as this script\n# get the direcotry that this script is running\nscript_dir = os.path.dirname(__file__)\n\n# get a relative path to the script dir\npath_to_catalog_file = script_dir + '/test.catalog'\n\nf = open(path_to_catalog_file)\nlines = f.readlines()\n# create a dictionary object\ndict = {}\ncount = 0\ntotal_throttle = 0\ntotal_angle = 0\n# Add each line to our dictionary\nfor line in lines:\n    # each line as a JSON dictionary object\n    j = json.loads(line)\n    count += 1\n    dict.update(json.loads(line))\n    total_throttle += j[\"user/throttle\"]\n    total_angle += j[\"user/angle\"]\nprint(count, \"items in dictionary\")\nprint(\"Average throttle: \", round(total_throttle/count, 3))\nprint(\"Average angle:\", round(total_angle/count, 3))\n</code></pre> <p>returns:</p> <pre><code>100 items in dictionary\nAverage throttle:  0.31\nAverage angle: 0.53\n</code></pre> <p>These values look reasonable.  Our throttle should be between 0 and 1 and our average steering should be around 0.5.  If we drive in a pure circle only in a single direction the average angle will be offset from the 0.5 center value.</p>"},{"location":"lesson-plans/data-analysis/05-catalog-statistics/#viewing-min-and-max-values","title":"Viewing Min and Max values","text":"<pre><code># program to read a DonkeyCar Catalog File\nimport os,json\n\n# this program assumes that test.catalog is in the same directory as this script\n# get the direcotry that this script is running\nscript_dir = os.path.dirname(__file__)\n\n# get a relative path to the script dir\npath_to_catalog_file = script_dir + '/test.catalog'\n\nf = open(path_to_catalog_file)\nlines = f.readlines()\n# create a dictionary object\ndict = {}\ncount = 0\n\ntotal_throttle = 0\nmin_throttle = 1\nmax_throttle = 0\n\ntotal_angle = 0\nmin_angle = 1\nmax_angle = 0\n\n# Add each line to our dictionary\nfor line in lines:\n    # each line as a JSON dictionary object\n    j = json.loads(line)\n    count += 1\n    dict.update(json.loads(line))\n    total_throttle += j[\"user/throttle\"]\n    total_angle += j[\"user/angle\"]\n\n    # check for min and max throttle\n    if j[\"user/throttle\"] &lt; min_throttle:\n        min_throttle = j[\"user/throttle\"]\n    if j[\"user/throttle\"] &gt; max_throttle:\n        max_throttle = j[\"user/throttle\"]\n\n    if j[\"user/angle\"] &lt; min_angle:\n        min_angle = j[\"user/angle\"]\n    if j[\"user/angle\"] &gt; max_angle:\n        max_angle = j[\"user/angle\"]\n\nprint('\\n')\nprint(count, \"items in catalog\")\n\nprint(\"Min throttle:\", round(min_throttle, 3))\nprint(\"Average throttle: \", round(total_throttle/count, 3))\nprint(\"Max throttle:\", round(max_throttle, 3))\n\nprint(\"Min angle:\", round(min_throttle, 3))\nprint(\"Average angle:\", round(total_angle/count, 3))\nprint(\"Max angle:\", round(max_angle, 3))\nprint('\\n')\n</code></pre> <p>returns:</p> <pre><code>100 items in catalog\nMin throttle: -0.31\nAverage throttle:  0.308\nMax throttle: 0.5\nMin angle: -0.31\nAverage angle: 0.534\nMax angle: 1.0\n</code></pre>"},{"location":"lesson-plans/data-analysis/05-catalog-statistics/#converting-the-dictionary-to-a-dataframe","title":"Converting the Dictionary to a DataFrame","text":"<pre><code>df = pd.DataFrame(list(dict.items()))\nprint(df)\n</code></pre> <p>returns</p> <pre><code>                0                           1\n0           _index                       16099\n1      _session_id                  21-07-20_1\n2    _timestamp_ms               1626797880229\n3  cam/image_array  16099_cam_image_array_.jpg\n4       user/angle                     0.56914\n5        user/mode                        user\n6    user/throttle                   0.0632649\n</code></pre>"},{"location":"lesson-plans/data-analysis/05-catalog-statistics/#plotting-steering-distributions","title":"Plotting Steering Distributions","text":""},{"location":"lesson-plans/data-analysis/06-cleaning-datasets/","title":"Cleaning Datasets","text":"<p>Up until now, we have only been viewing metrics and files.  These are all read-only operations. Now we will write our first programs that change the tub datasets.</p>"},{"location":"lesson-plans/data-analysis/06-cleaning-datasets/#splitting-datasets","title":"Splitting Datasets","text":"<p>In this lab we will assume that we want to break our data into two distinct subsets and place them in different \"tubs\", which are just directories that contain both the catalogs and images for a dataset.</p> <p>You can begin by taking a single dataset in a tub and then duplicating that tub.  You can then selectively remove the data from the two tubs to effectively split the tubs.</p> <p>The UNIX shell command to copy an entire directly is the \"cp\" command with the \"-r\" option for recursive copy.</p> <pre><code>cp -r from-dir to-dir\n</code></pre> <p>You can also add the \"-v\" option to see what files are being copied.</p>"},{"location":"lesson-plans/jupyter-notebooks/","title":"Introduction to Jupyter Notebooks and Basic Data Analysis","text":""},{"location":"lesson-plans/jupyter-notebooks/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this lesson, students should be able to:</p> <ol> <li>Understand what Jupyter Notebooks are and how to use them effectively.</li> <li>Load and inspect data from CSV and JSON files in a Jupyter Notebook.</li> <li>Perform basic data analysis using Python libraries such as pandas and matplotlib.</li> <li>Apply their knowledge to analyze data from Donkey Car projects.</li> </ol>"},{"location":"lesson-plans/jupyter-notebooks/#required-materials-and-preparation","title":"Required Materials and Preparation","text":"<ul> <li>Access to a computer with Python, Jupyter Notebooks, pandas, and matplotlib installed. </li> <li>Sample data sets from Donkey Car projects in both CSV and JSON formats.</li> <li>Previous knowledge in Python programming.</li> </ul>"},{"location":"lesson-plans/jupyter-notebooks/#lesson-breakdown","title":"Lesson Breakdown","text":"<p>Lesson 1: Introduction to Jupyter Notebooks (2 hours)</p> <p>1.1 Lecture: What is a Jupyter Notebook? (30 mins)</p> <ul> <li>Definition and purpose</li> <li>Features and benefits of using Jupyter Notebooks</li> </ul> <p>1.2 Hands-on Activity: Getting Started with Jupyter Notebook (90 mins)</p> <ul> <li>Launching a Jupyter Notebook</li> <li>Familiarizing with the interface</li> <li>Creating, editing, and executing cells</li> <li>Markdown syntax and use</li> <li>Saving and sharing Jupyter Notebooks</li> </ul> <p>Lesson 2: Data Loading and Inspection in Jupyter Notebooks (2 hours)</p> <p>2.1 Lecture: Basics of pandas (30 mins)</p> <ul> <li>Overview of pandas</li> <li>Creating dataframes</li> <li>Basic dataframe operations</li> </ul> <p>2.2 Hands-on Activity: Loading and Inspecting Data (90 mins)</p> <ul> <li>Reading data from CSV and JSON files with pandas</li> <li>Inspecting data: checking the dimensions, viewing the first/last few rows, data types</li> <li>Data summary statistics: using describe()</li> </ul> <p>Lesson 3: Basic Data Analysis in Jupyter Notebooks (3 hours)</p> <p>3.1 Lecture: Data Analysis with pandas (30 mins)</p> <ul> <li>Filtering and selecting data</li> <li>Grouping and aggregation</li> <li>Basic plotting with pandas</li> </ul> <p>3.2 Hands-on Activity: Basic Data Analysis (150 mins)</p> <ul> <li>Practical exercises for data selection, filtering, and aggregation</li> <li>Creating basic plots to visualize data insights</li> <li>Exploring the data to answer exploratory questions</li> </ul> <p>Lesson 4: Data Analysis of Donkey Car Project Data (3 hours)</p> <p>4.1 Recap: Overview of the Donkey Car project (30 mins)</p> <ul> <li>Overview of the Donkey Car project and the associated datasets</li> </ul> <p>4.2 Hands-on Activity: Donkey Car Data Analysis (150 mins)</p> <ul> <li>Loading and inspecting Donkey Car project datasets</li> <li>Performing exploratory data analysis: answering specific questions, making plots, extracting insights</li> <li>Discussion: Sharing insights, potential improvements for the Donkey Car project based on the data</li> </ul>"},{"location":"lesson-plans/jupyter-notebooks/#evaluation","title":"Evaluation","text":"<p>Students' understanding will be evaluated through their participation in the hands-on activities and the insights they generate from the Donkey Car project's data analysis. An end-of-unit quiz will also be provided to assess their theoretical understanding and practical skills in Jupyter Notebooks and data analysis.</p>"},{"location":"lesson-plans/jupyter-notebooks/#extension-activities","title":"Extension Activities","text":"<ul> <li>Advanced Data Analysis: Introduce students to more advanced data analysis techniques such as correlation analysis, data normalization, and pivot tables.</li> <li>Data Visualization: Teach students about more complex visualizations using libraries such as seaborn or plotly. </li> <li>Machine Learning Introduction: Provide a brief overview of how the data they have analyzed could be used to train a machine learning model.</li> </ul>"},{"location":"lesson-plans/object-detection/","title":"Objrct Detection","text":"<p>Edje Electronics Github Repo</p>"},{"location":"proposal-generator/","title":"Proposal Generator","text":"<p>The following is a proposal generator using ChatGPT.</p> <p>The prompt includes a template of the format and the content.</p>"},{"location":"proposal-generator/#prompt","title":"Prompt","text":"<pre><code>Create a document in raw markdown format.\n\nUse the following template:\n\n**Title:** Faculty Sponsored Design Project Proposal\n\n**Course Name:** EE 4951 \u2013 Senior Design\n\n**Semester:** Spring 2024\n\n**Project Name:** AI Racing League\n\n**Advisor name:** Dan McCreary\n\n**Email:** Dan.McCreary@gmail.com\n\n**Phone:** 612-625-7322\n\n## Project Motivation and Background\n\n[Single Paragraph]\n\n## Project Goal and Objective\n\n[Single Paragraph]\n\n## Project Specifications\n\n[Detailed Project Specification 1 to 2 Pages]\n\n## Expected Deliverables for Final Product\n\n[Single Paragraph and Numbered Bullet List]\n\n## Technical Skills Required\n\n[Numbered Bullet List of Technical Skills]\n\n## Expected Budget\n\n[Short Description of Costs - Bullet List]\n</code></pre>"},{"location":"proposal-generator/proposal/","title":"Generative AI Project Description","text":"<p>The following was mostly generated by ChatGPT using the content and template provided.  See the dialog for details.</p> <p>Title: Faculty-Sponsored Design Project Proposal</p> <p>Course Name: EE 4951 \u2013 Senior Design</p> <p>Semester: Spring 2024</p> <p>Project Name: AI Racing League</p> <p>Advisor name: Dan McCreary</p> <p>Email: Dan.McCreary@gmail.com</p> <p>Phone: 612-625-7322</p>"},{"location":"proposal-generator/proposal/#project-motivation-and-background","title":"Project Motivation and Background","text":"<p>The AI Racing League project aims to provide students with hands-on experience in the fields of machine learning, computer vision, and robotic control. The project will involve developing autonomous vehicles (Donkey Cars) capable of competing in a racing environment. This initiative is motivated by the increasing relevance of AI in various aspects of technology and the need for practical, real-world application skills in our graduates.</p>"},{"location":"proposal-generator/proposal/#project-goal-and-objective","title":"Project Goal and Objective","text":"<p>The primary goal of this project is to design, build, and test autonomous racing vehicles using Donkey Cars. Students will learn to apply machine learning algorithms, computer vision techniques, and hardware control to enable these vehicles to navigate a racetrack autonomously. The objective is to prepare students for careers in AI and robotics by providing a comprehensive learning experience that includes both theory and practice.</p>"},{"location":"proposal-generator/proposal/#project-specifications","title":"Project Specifications","text":"<p>Equipment:</p> <ol> <li>At least two Donkey Cars</li> <li>PCs with NVIDIA GeForce GTX series GPUs</li> <li>A racetrack (to be purchased or borrowed)</li> </ol>"},{"location":"proposal-generator/proposal/#key-learning-objectives","title":"Key Learning Objectives:","text":"<ol> <li>Machine Learning Processes</li> <li>Computer Vision</li> <li>Data Analysis with Jupyter Notebooks</li> <li>Raspberry Pi Configuration</li> <li>Motor and Servo Control</li> <li>Calibration Techniques</li> <li>UNIX Shell Usage</li> <li>GPU Configuration and Optimization</li> <li>Generative AI Usage</li> </ol>"},{"location":"proposal-generator/proposal/#sample-lessons","title":"Sample Lessons:","text":"<ol> <li>Introduction to DonkeyCars</li> <li>Basics of Machine Learning and Computer Vision</li> <li>Data Analysis and Cleaning Techniques</li> <li>Raspberry Pi Setup and Programming</li> <li>Motor and Servo Control Using PWM</li> <li>Calibration of Autonomous Vehicles</li> <li>UNIX Shell for System Management</li> <li>GPU Setup for Enhanced Performance</li> <li>Generative AI for Coding and Problem-Solving</li> </ol>"},{"location":"proposal-generator/proposal/#expected-deliverables-for-final-product","title":"Expected Deliverables for Final Product","text":"<ol> <li>Fully functional autonomous Donkey Cars capable of racing on a track.</li> <li>Comprehensive documentation of the design and development process.</li> <li>A presentation and demonstration of the project at the end of the course.</li> </ol>"},{"location":"proposal-generator/proposal/#technical-skills-required","title":"Technical Skills Required","text":"<ol> <li>Basic understanding of basic electronics and hardware control.</li> <li>Basic understanding of Python programming.</li> <li>Familiarity with machine learning concepts.</li> <li>Skills in data analysis and processing.</li> <li>Knowledge of computer vision basics would be helpful.</li> <li>Experience with Raspberry Pi or similar single-board computers would be helpful.</li> </ol>"},{"location":"proposal-generator/proposal/#expected-budget","title":"Expected Budget","text":"<ol> <li>Donkey Cars and GPUs: Currently available.</li> <li>Racetrack: Approximately $175 (purchase or borrow).</li> </ol>"},{"location":"quizzes/gpu-shell-commands/","title":"GPU Shell Commands Quiz","text":"<p>When you use a new GPU server at an AI Racing League event there are many questions you need to have answered about your GPU server.</p> <p>Here is a handy quiz you can use to check your knowledge.  The answers to the questions are listed below.</p>"},{"location":"quizzes/gpu-shell-commands/#questions","title":"Questions","text":"<p>Question 1: How would you log into the GPU server using the secure shell program?</p> <p>A) <code>$ login arl@arl1.local</code> B) <code>$ ssh arl@arl1.local</code> C) <code>$ enter arl@arl1.local</code> D) <code>$ connect arl@arl1.local</code></p> <p>Question 2: How would you check the version of Ubuntu on the GPU server?</p> <p>A) <code>$ version -a</code> B) <code>$ lsb_release -a</code> C) <code>$ ubuntu_version -all</code> D) <code>$ check_ubuntu -a</code></p> <p>Question 3: What information does the <code>lscpu</code> command provide?</p> <p>A) It provides the CPU information. B) It lists the amount of RAM on the server. C) It checks the disk space. D) It shows per-user disk usage.</p> <p>Question 4: Which command is used to check the total RAM on the GPU server?</p> <p>A) <code>$ free -m</code> B) <code>$ checkram -m</code> C) <code>$ listram -m</code> D) <code>$ raminfo -m</code></p> <p>Question 5: What does the command <code>df -h /</code> provide?</p> <p>A) It lists per user disk usage. B) It adds a new GPU server user. C) It checks the disk space. D) It monitors the NVIDIA GPU.</p> <p>Question 6: How can a new GPU server user be added?</p> <p>A) <code>$ adduser &lt;username&gt;</code> B) <code>$ newuser &lt;username&gt;</code> C) <code>$ createuser &lt;username&gt;</code> D) <code>$ useradd &lt;username&gt;</code></p> <p>Question 7: How can you give a user \"sudo\" rights?</p> <p>A) <code>$ sudo usermod -aG sudo &lt;username&gt;</code> B) <code>$ sudo addrights -aG sudo &lt;username&gt;</code> C) <code>$ sudo giverights -aG sudo &lt;username&gt;</code> D) <code>$ sudo addrules -aG sudo &lt;username&gt;</code></p> <p>Question 8: How can the hostname be changed?</p> <p>A) <code>$ sudo vi hostname</code> B) <code>$ sudo edit hostname</code> C) <code>$ sudo change hostname</code> D) <code>$ sudo alter hostname</code></p> <p>Question 9: What does the command <code>watch -d -n 0.5 nvidia-smi</code> do?</p> <p>A) It adds a new GPU server user. B) It runs similar to the UNIX top command, but for the GPU. C) It checks the version of Ubuntu. D) It lists the CPU information.</p> <p>Question 10: How would you check the NVIDIA GPU utilization?</p> <p>A) <code>$ checkgpu</code> B) <code>$ nvidia-smi</code> C) <code>$ gpu-utilization</code> D) <code>$ utilization nvidia</code></p>"},{"location":"quizzes/gpu-shell-commands/#answers","title":"Answers","text":"<ol> <li>B) <code>$ ssh arl@arl1.local</code></li> <li>B) <code>$ lsb_release -a</code></li> <li>A) It provides the CPU information.</li> <li>A) <code>$ free -m</code></li> <li>C) It checks the disk space.</li> <li>A) <code>$ adduser &lt;username&gt;</code></li> <li>A) <code>$ sudo usermod -aG sudo &lt;username&gt;</code></li> <li>A) <code>$ sudo vi hostname</code></li> <li>B) It runs similar to the UNIX top command, but for the GPU.</li> <li>B) <code>$ nvidia-smi</code></li> </ol>"},{"location":"setup/battery-options/","title":"Battery Options","text":""},{"location":"setup/battery-options/#dual-batteries","title":"Dual Batteries","text":"<p>In general, most of our cars use two separate batteries:</p> <ol> <li>One battery for the power to the drive motors and steering servos</li> <li>One battery to power the single-board computer and power the sensors like the camera</li> </ol> <p>Getting batteries charged before each event requires some strong organizational skills.  Although the LiPo batteries retain a charge for a long time, the RC car batteries must be fully charged the night before each event.</p> <p></p>"},{"location":"setup/battery-options/#18650-standards","title":"18650 Standards","text":"<p>The 18650 is a standard cell size for 3.7 volt Lithium batteries (18mmX65mm).  The problem is that if these batteries go below 3.2 volts they become damaged and can't be recharged. This is why you should never use these batteries without charge protection circuits.</p> <p>A typical 18650 battery produces 4.2 V when fully charged with a nominal voltage of 3.7 V.</p> <p>The TP4056 is a low-cost charger chip for one (1) cell. If you plan to charge multiple cells simultaneously, each charging slot for a cell needs its own TP4056.</p> <p>Warning</p> <p>Never connect a 5V power supply directly to a lithium cell. Even though the cells have built-in protection, they are not chargers. It is the last line of protection to prevent catastrophic failures by disconnecting the cell under abnormal conditions - unless the protection does not work.</p>"},{"location":"setup/battery-options/#boost-and-buck-converters","title":"Boost and Buck Converters","text":"<p>These are low-cost DC-to-DC voltage converters that allow you to use standard 3.7-volt cells that drive a 5-volt system.  Boost converters increase the voltage and buck converters lower the voltage.  You can purchase basic devices for $2, but fancier versions have displays that show both the input and output voltages.</p> <ol> <li>Boost Converters on eBay</li> <li>Buck Converters on eBay</li> </ol>"},{"location":"setup/battery-options/#computer-batteries","title":"Computer Batteries","text":"<p>We initially used several Ankar 2,000 milliamp-hour batteries for powering the cars.  The batteries would last for the entire single-day events as long as they were charged before the event and not used to power the cars when not running on the tracks.  The batteries typically could be recharged over 100 times before they had to be replaced.</p> <p></p> <p>Many teams use larger (and heavier) 10,000 milliamp-hour Lithium batteries.  These are especially useful when learning to drive around the track.  Teams also loved the fact that the INIU batteries showed the percent power remaining.</p> <p>The INIU Portable Charger, Slimmest 10000mAh 5V/3A Power Bank is also a popular choice since they only cost about $19.</p>"},{"location":"setup/battery-options/#traditional-rc-car-batteries","title":"Traditional RC Car Batteries","text":"<p>The traditional RC car batteries are based on standard \"Safe\" battery chemistry that can be fully drained and are not a strong fire hazard.  The problem is that they take a long time to charge (typically overnight) and they don't store a lot of power so they typically only last a few minutes at high speed.</p>"},{"location":"setup/battery-options/#camera-lipo-batteries","title":"Camera LiPo Batteries","text":"<p>A newer option is lower-cost rechargeable camera batteries that are built around  the Sony NP-F standard.  These are widely regarded as safe by the camera industry and there are many low-cost smart batteries and chargers available for around $10 each. They produce a stable 7.4-volt output that is perfect for our DC motors in RC cars.</p> <p>The only consideration is getting cables that convert the batteries to use the RC car standards.  I have made my own cables, but the quality of the connectors on the Donkey Car still leaves some things to be desired.  The ends of the connectors are easy to pull off.  I think that soldering a new barrel connector on all the RC cars might be another option.</p> <p>The one other challenge is trying to find a low-cost connection plate.  Right now the lowest cost plate is about $15 on Adafruit.  I hope we can find lower-cost options or use a 3D printer to print our own.</p> <p></p> <p>Adafruit Camcorder Battery Holder/Adapter for Sony NP-F750 w/ DC Jack $15</p> <p>Two batteries and charger on e-Bay for $23</p> <p>3D Print Thingiverse NP-F Battery Wall Mount with Soft Lock</p> <p>Neewer Battery Connection Plate for $27 on Amazon</p> <p>The camera industry has also developed standard 7.4-volt battery packs that enclose low-cost 18650 battery cells.  These standards have electronics built in to protect from drainage. Here is a list of parts that were recommended by the NVIDIA Jetson Hacks team:</p> <ol> <li>SmallRig NP-F Battery Plate Adapter</li> <li>Generic NP-F 550 2 Cell Battery</li> <li>Generic NP-F 750 4 Cell Battery</li> <li>Antigravity Micro-Start XP-1 Battery</li> <li>Artman NP-F550 Battery and Dual USB Charger for Sony NP F550, F570, F530, F330, F970, F960, F750, F770, CCD-SC55, TR516, TR716, TR818, TR910 (2-Pack 2900mAh) - great deal - two batteries and charger for $28.99</li> <li>Kastar Battery LCD Dual Charger for Sony NP-F330 NP-F530 NP-F550 NP-F750 NP-F770 - - two batteries and charger for $28.99</li> </ol>"},{"location":"setup/battery-options/#battery-cables","title":"Battery Cables","text":"<p>Several participants used long battery cables with a small wire gauge.  These cables caused voltage drops that made the cars stop working.  We told all teams to use short 8-inch battery cables and most of these problems went away.  We strongly suggest you build your own cables and use 18-gauge wire.</p> <p>Sample 1ft Charging Cable</p>"},{"location":"setup/battery-options/#connectors-and-crimping-tool","title":"Connectors and Crimping Tool","text":"<ul> <li>Ferrule Crimping Tool Kit</li> </ul>"},{"location":"setup/battery-options/#74-volt-lipo-batteries","title":"7.4 Volt LiPo Batteries","text":"<p>You can also purchase \"raw\" 7.4-volt LiPo batteries that are designed to work with RC cars.  I have not purchased these directly so I can't attest to their reliability or safety.  I will say that the power connector on them appears to be incompatible with our current cars.</p> <p>1500mAh 7.4V 2S 18650 Battery</p>"},{"location":"setup/battery-options/#references","title":"References","text":"<p>NVIDIA Jetson on Battery Power - a nice video by </p>"},{"location":"setup/building-arl-image/","title":"Building an AI Racing League Image for the Car","text":"<p>This process takes about four hours and requires close attention to detail.</p>"},{"location":"setup/building-arl-image/#install-the-latest-nvidia-image","title":"Install The Latest NVIDIA image","text":""},{"location":"setup/building-arl-image/#enable-ssh","title":"Enable SSH","text":"<p>This allows you to do a remote login to the car</p> <pre><code>sudo systemctl enable ssh\nsudo systemctl start ssh\n</code></pre> <p>Test ssh:</p> <pre><code>ssh arl@arl-01\n</code></pre> <pre><code>Welcome to Ubuntu 18.04.6 LTS (GNU/Linux 4.9.253-tegra aarch64)\n\n * Documentation:  https://help.ubuntu.com\n * Management:     https://landscape.canonical.com\n * Support:        https://ubuntu.com/advantage\nThis system has been minimized by removing packages and content that are\nnot required on a system that users do not log into.\n\nTo restore this content, you can run the 'unminimize' command.\n\nExpanded Security Maintenance for Infrastructure is not enabled.\n\n0 updates can be applied immediately.\n\nEnable ESM Infra to receive additional future security updates.\nSee https://ubuntu.com/esm or run: sudo pro status\n</code></pre>"},{"location":"setup/building-arl-image/#add-a-swap-file","title":"Add a swap file","text":"<p>The Jetson Hacks GitHub repo contains a nice UNIX script that will add a swap file.</p> <pre><code>git clone https://github.com/JetsonHacksNano/installSwapfile\ncd installSwapfile\n./installSwapfile.sh\n</code></pre> <p>You should reboot the Nano after you add this file.</p>"},{"location":"setup/building-arl-image/#add-htop","title":"Add HTOP","text":"<p>The standard UNIX <code>top</code> command does not display the load across multiple cores.  The much more modern <code>htop</code> is preferred and has better use of color.</p> <pre><code>sudo apt-get install htop\n</code></pre>"},{"location":"setup/building-arl-image/#customize-browser-add-the-bookmark-bar","title":"Customize Browser Add the Bookmark Bar","text":"<ol> <li>AI Racing League</li> <li>Donkey Car Docs</li> </ol>"},{"location":"setup/building-arl-image/#remove-unused-software","title":"Remove Unused Software","text":"<p>Ubuntu comes with several \"Office\" applications for doing word-processing, spreadsheets and presentations.  These applications take up a lot of disk space so we will remove them.</p> <pre><code>sudo apt-get remove --purge libreoffice*\nsudo apt-get clean\nsudo apt-get autoremove\n</code></pre>"},{"location":"setup/building-arl-image/#upgrade-local-libraries","title":"Upgrade Local Libraries","text":""},{"location":"setup/building-arl-image/#download-donkey-car-dependencies","title":"Download Donkey Car Dependencies","text":"<p>sudo apt-get update -y sudo apt-get upgrade -y sudo apt-get install -y libhdf5-serial-dev hdf5-tools libhdf5-dev zlib1g-dev zip libjpeg8-dev liblapack-dev libblas-dev gfortran sudo apt-get install -y python3-dev python3-pip sudo apt-get install -y libxslt1-dev libxml2-dev libffi-dev libcurl4-openssl-dev libssl-dev libpng-dev libopenblas-dev sudo apt-get install -y git nano sudo apt-get install -y openmpi-doc openmpi-bin libopenmpi-dev libopenblas-dev</p>"},{"location":"setup/building-arl-image/#monitoring-activity","title":"Monitoring Activity","text":""},{"location":"setup/building-arl-image/#version-checks","title":"Version Checks","text":"<pre><code>pip3 --version\n</code></pre> <pre><code>pip 21.3.1 from /home/arl/env/lib/python3.6/site-packages/pip (python 3.6)\n</code></pre>"},{"location":"setup/calibrate/","title":"Calibrate","text":"<pre><code>$ donkey calibrate --channel 0 --bus=1\n</code></pre> <p>Result</p> <pre><code>________             ______                   _________              \n___  __ \\_______________  /___________  __    __  ____/_____ ________\n__  / / /  __ \\_  __ \\_  //_/  _ \\_  / / /    _  /    _  __ `/_  ___/\n_  /_/ // /_/ /  / / /  ,&lt;  /  __/  /_/ /     / /___  / /_/ /_  /    \n/_____/ \\____//_/ /_//_/|_| \\___/_\\__, /      \\____/  \\__,_/ /_/     \n                                 /____/                              \n\nusing donkey v4.2.1 ...\nsombrero enabled\ninit PCA9685 on channel 0 address 0x40 bus 1\nUsing PWM freq: 60\nTraceback (most recent call last):\n  File \"/home/pi/env/bin/donkey\", line 33, in &lt;module&gt;\n    sys.exit(load_entry_point('donkeycar', 'console_scripts', 'donkey')())\n  File \"/home/pi/projects/donkeycar/donkeycar/management/base.py\", line 500, in execute_from_command_line\n    c.run(args[2:])\n  File \"/home/pi/projects/donkeycar/donkeycar/management/base.py\", line 219, in run\n    c = PCA9685(channel, address=address, busnum=busnum, frequency=freq)\n  File \"/home/pi/projects/donkeycar/donkeycar/parts/actuator.py\", line 30, in __init__\n    self.pwm = Adafruit_PCA9685.PCA9685(address=address)\n  File \"/home/pi/env/lib/python3.7/site-packages/Adafruit_PCA9685/PCA9685.py\", line 75, in __init__\n    self.set_all_pwm(0, 0)\n  File \"/home/pi/env/lib/python3.7/site-packages/Adafruit_PCA9685/PCA9685.py\", line 111, in set_all_pwm\n    self._device.write8(ALL_LED_ON_L, on &amp; 0xFF)\n  File \"/home/pi/env/lib/python3.7/site-packages/Adafruit_GPIO/I2C.py\", line 114, in write8\n    self._bus.write_byte_data(self._address, register, value)\n  File \"/home/pi/env/lib/python3.7/site-packages/Adafruit_PureIO/smbus.py\", line 327, in write_byte_data\n    self._device.write(data)\nOSError: [Errno 121] Remote I/O error\nsombrero disabled\n</code></pre>"},{"location":"setup/calibrate/#diagnostics","title":"Diagnostics","text":""},{"location":"setup/calibrate/#i2c-detect","title":"I2C Detect","text":"<pre><code>i2cdetect -y 1\n     0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f\n00:          -- -- -- -- -- -- -- -- -- -- -- -- -- \n10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n40: 40 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n70: 70 -- -- -- -- -- -- --                        \n</code></pre>"},{"location":"setup/calibrate/#i2c-device-file","title":"I2C Device File","text":"<pre><code>ls -ld /dev/i2*\ncrw-rw---- 1 root i2c 89, 1 Jul  3 13:17 /dev/i2c-1\n</code></pre>"},{"location":"setup/calibrate/#i2c-functions-enabled","title":"I2C Functions Enabled","text":"<pre><code> i2cdetect -F 1\n</code></pre> <p>returns:</p> <pre><code>Functionalities implemented by /dev/i2c-1:\nI2C                              yes\nSMBus Quick Command              yes\nSMBus Send Byte                  yes\nSMBus Receive Byte               yes\nSMBus Write Byte                 yes\nSMBus Read Byte                  yes\nSMBus Write Word                 yes\nSMBus Read Word                  yes\nSMBus Process Call               yes\nSMBus Block Write                yes\nSMBus Block Read                 no\nSMBus Block Process Call         no\nSMBus PEC                        yes\nI2C Block Write                  yes\nI2C Block Read                   yes\n</code></pre> <p>Note that both SMBus Block Read and SMBus Block Process Call are set to no.  The rest are yes.</p>"},{"location":"setup/calibrate/#upgrade-to-python-370","title":"Upgrade to Python 3.70","text":"<pre><code>python3 -m virtualenv -p python3.7 env --system-site-packages\n</code></pre> <pre><code>created virtual environment CPython3.7.3.final.0-32 in 2535ms\n  creator CPython3Posix(dest=/home/pi/env, clear=False, no_vcs_ignore=False, global=True)\n  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/home/pi/.local/share/virtualenv)\n    added seed packages: pip==21.1.2, setuptools==57.0.0, wheel==0.36.2\n  activators BashActivator,CShellActivator,FishActivator,PowerShellActivator,PythonActivator,XonshActivator\n</code></pre> <pre><code>python --version\n</code></pre> <pre><code>Python 3.7.3\n</code></pre>"},{"location":"setup/camera-testing/","title":"Testing the Camera","text":"<p>To test the camera and cable, we need a command that captures video from a CSI camera connected to an NVIDIA Jetson Nano, converts the video format and resolution, and then displays the video on the screen.  We will use the GStreamer command first.</p>"},{"location":"setup/camera-testing/#gstreamer-test-on-the-nano","title":"GStreamer Test on the Nano","text":"<pre><code>gst-launch-1.0 nvarguscamerasrc ! \\\n'video/x-raw(memory:NVMM),width=3820, height=2464, framerate=21/1, format=NV12' ! \\\nnvvidconv flip-method=0 ! \\\n'video/x-raw,width=960, height=616' ! \nnvvidconv ! \\\nnvegltransform ! \\\nnveglglessink -e\n</code></pre> <p>This command is a GStreamer command used to test the functionality of a camera on a NVIDIA Jetson Nano device. GStreamer is a multimedia framework that provides a pipeline for media data. The <code>gst-launch-1.0</code> utility is used to build and run basic GStreamer pipelines.</p> <p>On newer systems with two cameras you can add: <code>sensor-id=0</code> to make sure you are getting camera 0.</p> <p>This guide explains how to clone a Raspberry Pi microSD card using the <code>dd</code> command on UNIX and UNIX-like systems.</p>"},{"location":"setup/camera-testing/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Testing the Camera</li> <li>GStreamer Test on the Nano</li> <li>Table of Contents</li> <li>Identify the microSD Card Device</li> <li>Unmount the microSD Card</li> <li>Create a Backup Image</li> <li>Copy the Backup Image to New microSD Cards</li> <li>Expand the Filesystem (Optional)<ul> <li>Why \"!\" and not \"|\"?</li> <li>Flip Modes</li> </ul> </li> <li>Resources<ul> <li>Dan's Blog</li> <li>Jetson Hacks Blog</li> </ul> </li> </ul>"},{"location":"setup/camera-testing/#identify-the-microsd-card-device","title":"Identify the microSD Card Device","text":"<ol> <li> <p>First, identify the device name for your microSD card. Use either <code>lsblk</code> or <code>df</code> to list devices and their mount points.</p> <pre><code>lsblk\n</code></pre> <p>Or:</p> <pre><code>df -h\n</code></pre> </li> <li> <p>Look for the device corresponding to your microSD card. It's generally something like <code>/dev/sdX</code> or <code>/dev/mmcblkX</code>, where <code>X</code> is a letter.</p> <p>\u26a0\ufe0f Caution: Be very careful to identify the correct device, as choosing the wrong one could result in data loss.</p> </li> </ol>"},{"location":"setup/camera-testing/#unmount-the-microsd-card","title":"Unmount the microSD Card","text":"<ol> <li> <p>Before copying data, unmount the partitions of the microSD card to ensure that no data is being read or written during the cloning process.</p> <pre><code>sudo umount /dev/sdX*\n</code></pre> </li> </ol>"},{"location":"setup/camera-testing/#create-a-backup-image","title":"Create a Backup Image","text":"<ol> <li> <p>Use the <code>dd</code> command to create an image file of the microSD card.</p> <pre><code>sudo dd if=/dev/sdX of=raspberrypi_backup.img bs=4M status=progress\n</code></pre> <ul> <li><code>if</code>: Input File \u2014 the device you are copying from (your microSD card).</li> <li><code>of</code>: Output File \u2014 the image file you are creating.</li> <li><code>bs</code>: Block Size \u2014 specifies how much data should be read at each iteration. <code>4M</code> is usually a good size.</li> <li><code>status=progress</code>: shows the progress during the copy.</li> </ul> </li> </ol>"},{"location":"setup/camera-testing/#copy-the-backup-image-to-new-microsd-cards","title":"Copy the Backup Image to New microSD Cards","text":"<ol> <li> <p>To clone the image onto a new microSD card, insert the new card and identify it just like you did in the first step.</p> <pre><code>sudo dd if=raspberrypi_backup.img of=/dev/sdY bs=4M status=progress\n</code></pre> <p>Replace <code>/dev/sdY</code> with the device name of your new microSD card.</p> <p>\u26a0\ufe0f Caution: Again, be very careful to identify the correct device to avoid data loss.</p> </li> </ol>"},{"location":"setup/camera-testing/#expand-the-filesystem-optional","title":"Expand the Filesystem (Optional)","text":"<ol> <li> <p>If your new microSD card is larger than the original, you might need to expand the filesystem to use the additional space. You can do this using <code>raspi-config</code> on the Raspberry Pi.</p> <pre><code>sudo raspi-config\n</code></pre> <p>Navigate to <code>Advanced Options</code> &gt; <code>Expand Filesystem</code>.</p> </li> </ol> <p>After following these steps, you should have successfully cloned your Raspberry Pi's microSD card.</p> <p>Note: The <code>dd</code> command can be very dangerous if misused. Always double-check your device names and ensure you understand the commands you're running. ```</p> <p>Here's a breakdown of the command:</p> <ol> <li> <p><code>nvarguscamerasrc sensor_mode=0</code>: This is a GStreamer plugin specific to the NVIDIA platform that provides support for the Camera Serial Interface (CSI) cameras. <code>sensor_mode=0</code> indicates that the command should use the first sensor mode of the camera. The sensor mode usually defines properties such as the resolution and frame rate that the camera supports.</p> </li> <li> <p><code>'video/x-raw(memory:NVMM),width=3820, height=2464, framerate=21/1, format=NV12'</code>: This part of the command specifies the desired output from the camera source. The properties indicate that the video should be in NV12 format, with a resolution of 3820x2464 pixels and a frame rate of 21 frames per second. NVMM refers to NVIDIA's proprietary multimedia memory.</p> </li> <li> <p><code>nvvidconv flip-method=0</code>: This is another NVIDIA specific GStreamer plugin that converts video from one format to another. The <code>flip-method=0</code> option means that no flipping operation should be performed on the frames. </p> </li> <li> <p><code>'video/x-raw,width=960, height=616'</code>: This specifies the desired output format and resolution after the conversion. The resolution is downscaled to 960x616 pixels.</p> </li> <li> <p><code>nvvidconv ! nvegltransform ! nveglglessink -e</code>: This part of the pipeline takes the video from the conversion, applies an EGLStream transformation (<code>nvegltransform</code>) and then sends it to a EGL/GLES-based render sink (<code>nveglglessink</code>). This sink displays the video on the device's screen. The <code>-e</code> flag at the end of the command tells GStreamer to send an end-of-stream signal when the source stops, which will properly close down the pipeline.</p> </li> </ol>"},{"location":"setup/camera-testing/#why-and-not","title":"Why \"!\" and not \"|\"?","text":"<p>In the context of a GStreamer command, the \"!\" (aka bang) character is used to connect different elements of a GStreamer pipeline together. It serves a similar role to the UNIX \"|\" (pipe) character in a regular UNIX shell command, where it's used to pipe the output from one command into another.</p> <p>However, there's an important difference between the two. In a UNIX shell command, the | character sends the standard output (stdout) of one command to the standard input (stdin) of another. In a GStreamer pipeline, the ! character doesn't simply pipe data from one element to the next. Instead, it establishes a connection between two GStreamer elements, allowing them to negotiate formats, buffer management, and other details. This negotiation process can involve more complex operations like format conversion, and it happens before any data is actually transferred.</p> <p>So, in summary, while | and ! might seem similar, the latter is used in GStreamer to create more complex, negotiated connections between different multimedia processing elements.</p>"},{"location":"setup/camera-testing/#flip-modes","title":"Flip Modes","text":"<p>The <code>flip-method</code> property of the <code>nvvidconv</code> (NVIDIA Video Converter) plugin controls the orientation of the output video in the NVIDIA Jetson platform. This is useful for handling scenarios where the camera could be mounted in various orientations.</p> <p>Here are the possible values for the <code>flip-method</code> parameter:</p> <ul> <li><code>0</code> (Identity) - No rotation, no vertical flip.</li> <li><code>1</code> (Counterclockwise) - Rotate counter-clockwise 90 degrees.</li> <li><code>2</code> (Rotate 180) - Rotate 180 degrees.</li> <li><code>3</code> (Clockwise) - Rotate clockwise 90 degrees.</li> <li><code>4</code> (Horizontal Flip) - Flip horizontally.</li> <li><code>5</code> (Upper Right Diagonal) - Flip across upper right/lower left diagonal.</li> <li><code>6</code> (Vertical Flip) - Flip vertically.</li> <li><code>7</code> (Upper Left Diagonal) - Flip across upper left/lower right diagonal.</li> </ul> <p>Each number corresponds to a specific operation on the video frames. The specific operation will be applied to each frame of the video before it's sent to the next element in the GStreamer pipeline.</p>"},{"location":"setup/camera-testing/#resources","title":"Resources","text":""},{"location":"setup/camera-testing/#dans-blog","title":"Dan's Blog","text":"<p>NVIDIA CSI Camera GitHub Repo</p>"},{"location":"setup/camera-testing/#jetson-hacks-blog","title":"Jetson Hacks Blog","text":"<p>https://jetsonhacks.com/2019/04/02/jetson-nano-raspberry-pi-camera/</p>"},{"location":"setup/clone-sd/","title":"Cloning a microSD Card using UNIX <code>dd</code> Command","text":"<p>This guide explains how to clone a microSD card using the <code>dd</code> command on UNIX and UNIX-like systems.</p>"},{"location":"setup/clone-sd/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Cloning a microSD Card using UNIX <code>dd</code> Command</li> <li>Table of Contents</li> <li>Identify the microSD Card Device</li> <li>Unmount the microSD Card</li> <li>Create a Backup Image</li> <li>Copy the Backup Image to New microSD Cards</li> <li>Expand the Filesystem (Optional)</li> </ul>"},{"location":"setup/clone-sd/#identify-the-microsd-card-device","title":"Identify the microSD Card Device","text":"<ol> <li> <p>First, identify the device name for your microSD card. Use either <code>lsblk</code> or <code>df</code> to list devices and their mount points.</p> <pre><code>lsblk\n</code></pre> <p>Or:</p> <pre><code>df -h\n</code></pre> </li> <li> <p>Look for the device corresponding to your microSD card. It's generally something like <code>/dev/sdX</code> or <code>/dev/mmcblkX</code>, where <code>X</code> is a letter.</p> <p>\u26a0\ufe0f Caution: Be very careful to identify the correct device, as choosing the wrong one could result in data loss.</p> </li> </ol>"},{"location":"setup/clone-sd/#unmount-the-microsd-card","title":"Unmount the microSD Card","text":"<ol> <li> <p>Before copying data, unmount the partitions of the microSD card to ensure that no data is being read or written during the cloning process.</p> <pre><code>sudo umount /dev/sdX*\n</code></pre> </li> </ol>"},{"location":"setup/clone-sd/#create-a-backup-image","title":"Create a Backup Image","text":"<ol> <li> <p>Use the <code>dd</code> command to create an image file of the microSD card.</p> <pre><code>sudo dd if=/dev/sdX of=raspberrypi_backup.img bs=4M status=progress\n</code></pre> <ul> <li><code>if</code>: Input File \u2014 the device you are copying from (your microSD card).</li> <li><code>of</code>: Output File \u2014 the image file you are creating.</li> <li><code>bs</code>: Block Size \u2014 specifies how much data should be read at each iteration. <code>4M</code> is usually a good size.</li> <li><code>status=progress</code>: shows the progress during the copy.</li> </ul> </li> </ol>"},{"location":"setup/clone-sd/#copy-the-backup-image-to-new-microsd-cards","title":"Copy the Backup Image to New microSD Cards","text":"<ol> <li> <p>To clone the image onto a new microSD card, insert the new card and identify it just like you did in the first step.</p> <pre><code>sudo dd if=raspberrypi_backup.img of=/dev/sdY bs=4M status=progress\n</code></pre> <p>Replace <code>/dev/sdY</code> with the device name of your new microSD card.</p> <p>\u26a0\ufe0f Caution: Again, be very careful to identify the correct device to avoid data loss.</p> </li> </ol>"},{"location":"setup/clone-sd/#expand-the-filesystem-optional","title":"Expand the Filesystem (Optional)","text":"<ol> <li> <p>If your new microSD card is larger than the original, you might need to expand the filesystem to use the additional space. You can do this using <code>raspi-config</code> on the Raspberry Pi.</p> <pre><code>sudo raspi-config\n</code></pre> <p>Navigate to <code>Advanced Options</code> &gt; <code>Expand Filesystem</code>.</p> </li> </ol> <p>After following these steps, you should have successfully cloned your Raspberry Pi's microSD card.</p> <p>Note: The <code>dd</code> command can be very dangerous if misused. Always double-check your device names and ensure you understand the commands you're running.</p>"},{"location":"setup/conda-pi-setup/","title":"Raspberry Pi Setup","text":""},{"location":"setup/conda-pi-setup/#install-conda-for-the-arm-processor","title":"Install Conda for the ARM Processor","text":"<p>When asked:</p> <p>Do you wish the installer to prepend the Miniconda3 install location to PATH in your /root/.bashrc?</p> <p>Answer: yes</p> <pre><code>cd /tmp\nwget http://repo.continuum.io/miniconda/Miniconda3-latest-Linux-armv7l.sh\nchmod 755 Miniconda3-latest-Linux-armv7l.sh\n./Miniconda3-latest-Linux-armv7l.sh\n</code></pre>"},{"location":"setup/conda-pi-setup/#test-conda-in-your-path","title":"Test Conda In Your PATH","text":"<p><pre><code>which conda\n</code></pre> Should return:</p> <pre><code>/home/pi/miniconda3/bin/conda\n</code></pre>"},{"location":"setup/conda-pi-setup/#add-the-raspberry-pi-channel-to-conda","title":"Add the Raspberry Pi Channel to Conda","text":"<pre><code>conda config --add channels rpi\nconda install python=3.6\n</code></pre>"},{"location":"setup/conda-pi-setup/#test-python","title":"Test Python","text":"<pre><code>python --version\n</code></pre> <pre><code>Python 3.6.6\n</code></pre>"},{"location":"setup/conda-pi-setup/#create-a-donkeycar-conda-environment","title":"Create a DonkeyCar Conda Environment","text":"<pre><code>conda create --name donkey python=3\n</code></pre> <pre><code>The following NEW packages will be INSTALLED:\n\n    ca-certificates: 2018.8.24-0          rpi\n    certifi:         2018.8.24-py36_1     rpi\n    ncurses:         6.1-h4f752ac_1       rpi\n    openssl:         1.0.2r-hdff2a78_0    rpi\n    pip:             18.0-py36_1          rpi\n    python:          3.6.6-hd0568c0_1     rpi\n    readline:        7.0-hcb560eb_1       rpi\n    setuptools:      40.2.0-py36_0        rpi\n    sqlite:          3.24.0-hfcb1bcf_1    rpi\n    tk:              8.6.8-h849d6a0_0     rpi\n    wheel:           0.31.1-py36_1        rpi\n    xz:              5.2.4-hdff2a78_1     rpi\n    zlib:            1.2.11-hdff2a78_1003 rpi\n\nProceed ([y]/n)? y\n</code></pre>"},{"location":"setup/conda-pi-setup/#add-the-conda-shell-to-the-end-of-our-bashrc-file","title":"Add the conda shell to the end of our .bashrc file","text":"<pre><code>echo \". /home/pi/miniconda3/etc/profile.d/conda.sh\" &gt;&gt; ~/.bashrc\n</code></pre> <p><pre><code>conda activate\n</code></pre> The shell prompt should now be \"base\"</p>"},{"location":"setup/conda-pi-setup/#activate-your-donkey-python-environment","title":"Activate Your Donkey Python Environment","text":"<pre><code>source activate donkey\n</code></pre> <p>You should see the prompt:</p> <pre><code>(donkey) pi@myhost:~ $\n</code></pre>"},{"location":"setup/conda-pi-setup/#verify-git-is-installed","title":"Verify Git Is installed","text":"<pre><code>git --version\n</code></pre> <p>git version 2.20.1</p>"},{"location":"setup/conda-pi-setup/#clone-the-donkeycar-repository","title":"Clone the DonkeyCar repository","text":"<pre><code>git clone https://github.com/autorope/donkeycar\ncd donkeycar\ngit checkout master\n</code></pre> <pre><code>sudo apt-get install build-essential python3 python3-dev python3-pip python3-virtualenv python3-numpy python3-picamera python3-pandas python3-rpi.gpio i2c-tools avahi-utils joystick libopenjp2-7-dev libtiff5-dev gfortran libatlas-base-dev libopenblas-dev libhdf5-serial-dev libgeos-dev git ntp\n</code></pre> <pre><code>sudo apt-get install libilmbase-dev libopenexr-dev libgstreamer1.0-dev libjasper-dev libwebp-dev libatlas-base-dev libavcodec-dev libavformat-dev libswscale-dev libqtgui4 libqt4-test\n</code></pre>"},{"location":"setup/conda-pi-setup/#clone-donkeycar-repo","title":"Clone DonkeyCar Repo","text":"<pre><code>pip freeze\n</code></pre> <p>certifi==2018.8.24</p> <pre><code>git clone https://github.com/autorope/donkeycar\ncd donkeycar\npip install -e .[pi]\n</code></pre>"},{"location":"setup/donkey-cmd/","title":"The Donkey Command","text":"<p>In normal configuration, the <code>donkey</code> is a UNIX shell script that is located in the bin directory of your local environment.  For example here is a sample path to the <code>donkey</code> command if you are logged into the standard AI Racing account called arl</p> <pre><code>/home/arl/env/bin\n</code></pre> <pre><code>#!/home/dan/env/bin/python3\n# EASY-INSTALL-ENTRY-SCRIPT: 'donkeycar','console_scripts','donkey'\n__requires__ = 'donkeycar'\nimport re\nimport sys\nfrom pkg_resources import load_entry_point\n\nif __name__ == '__main__':\n    sys.argv[0] = re.sub(r'(-script\\.pyw?|\\.exe)?$', '', sys.argv[0])\nsys.exit(\nload_entry_point('donkeycar', 'console_scripts', 'donkey')()\n)\n</code></pre> <p>This file is automatically generated</p> <p>This UNIX script is automatically generated and serves as an entry point for the donkeycar package. The console_scripts and donkey parameters provide more information about the entry point, usually used by package management tools like setuptools.</p>"},{"location":"setup/donkey-cmd/#imports","title":"Imports","text":"<pre><code>import re\nimport sys\nfrom pkg_resources import load_entry_point\n</code></pre> <ol> <li>re: imports Python's regular expression module.</li> <li>sys: imports Python's system module to interact with the Python runtime environment.</li> <li>load_entry_point: imports a function that is responsible for loading the entry point specified.</li> </ol> <p>On the installer for the 4.5.1, this file is not generated.</p>"},{"location":"setup/gpu-options/","title":"GPU Options","text":"<p>PCPartPicker Part List: https://pcpartpicker.com/list/mrFYPX</p> <p>CPU: AMD Ryzen 5 3600 3.6 GHz 6-Core Processor  ($95.00 @ Amazon) Motherboard: MSI A520M-A PRO Micro ATX AM4 Motherboard  ($101.11 @ Amazon) Memory: Silicon Power SP016GBLFU320X02 16 GB (1 x 16 GB) DDR4-3200 CL22 Memory  ($23.99 @ Amazon) Storage: TEAMGROUP MP33 512 GB M.2-2280 PCIe 3.0 X4 NVME Solid State Drive  ($22.49 @ Amazon) Video Card: Asus Dual GeForce RTX 3060 V2 OC Edition GeForce RTX 3060 12GB 12 GB Video Card  ($299.99 @ Amazon) Case: Thermaltake Versa H18 MicroATX Mini Tower Case  ($49.99 @ Amazon) Power Supply: be quiet! Pure Power 11 400 W 80+ Gold Certified ATX Power Supply  ($89.69 @ Amazon) Monitor: *Acer V227Q Abmix 21.5\" 1920 x 1080 75 Hz Monitor  ($87.29 @ Amazon) Total: $769.55</p>"},{"location":"setup/installation-tips/","title":"Installation Tips","text":"<p>Installing the Donkey Car software is a complex process.  It typically takes about six hours to add the required tools and Python libraries to a raw NVIDIA or Raspberry Pi OS image.</p> <p>Here are a few tips.</p>"},{"location":"setup/installation-tips/#use-the-h-mode-when-installing-software","title":"Use the -H mode when installing software","text":"<p>We should always use the HOME environment variables when using sudo:</p> <pre><code>sudo -H pip install package_name\n</code></pre> <p>In this command, sudo -H is used to ensure that the home environment variable is set to the home directory of the target user (root in this case), which can sometimes avoid permissions issues that arise when sudo retains your normal user's home directory environment variable. Essentially, the -H option makes sure that the operation is as clean as possible from an environment perspective, which can be important when installing software that might write configuration files or data into the user's home directory.</p>"},{"location":"setup/installation-tips/#remember-to-use-python3-and-pip3","title":"Remember to Use Python3 and Pip3","text":"<p>Sometimes older Python2 tools get mixed up with the current Python 3 tools.  By adding the \"3\" suffix to your commands you can guarantee that that your path will pick up the right version of Python tools.</p>"},{"location":"setup/installation-tips/#when-to-use-python3-and-pip3-over-python-and-pip-in-unix-shell","title":"When to Use <code>python3</code> and <code>pip3</code> Over <code>python</code> and <code>pip</code> in UNIX Shell","text":""},{"location":"setup/installation-tips/#python-version","title":"Python Version","text":"<ol> <li>Python 2 vs Python 3: Python 2 and Python 3 are two different versions of the Python programming language. Python 2 is no longer maintained as of January 1, 2020, but it still exists on some systems for legacy reasons. <code>python3</code> explicitly runs Python 3.x, whereas <code>python</code> might run either Python 2.x or Python 3.x depending on the system configuration.</li> </ol>"},{"location":"setup/installation-tips/#system-configuration","title":"System Configuration","text":"<ol> <li> <p>Multiple Python Installations: On some systems, you may have both Python 2 and Python 3 installed. In such cases, <code>python</code> usually refers to Python 2 and <code>python3</code> to Python 3. Similarly, <code>pip</code> might point to the package manager for Python 2, and <code>pip3</code> will point to Python 3. Always use <code>python3</code> and <code>pip3</code> to ensure that you're working with Python 3.x.</p> </li> <li> <p>Aliases: Some systems alias <code>python</code> to <code>python3</code>. This is common in more recent Linux distributions. On such systems, it may not matter if you use <code>python</code> or <code>python3</code>. However, using <code>python3</code> is more explicit and can avoid ambiguity.</p> </li> </ol>"},{"location":"setup/installation-tips/#script-compatibility","title":"Script Compatibility","text":"<ol> <li> <p>Version-Specific Code: If you're running or writing code that is specific to Python 3, use <code>python3</code>. Similarly, if you're installing packages that are intended for use with Python 3, use <code>pip3</code>.</p> </li> <li> <p>Portability: If you're writing a script that you plan to share with others, it's safer to specify <code>python3</code> if your code is not compatible with Python 2.</p> </li> </ol>"},{"location":"setup/installation-tips/#virtual-environments","title":"Virtual Environments","text":"<ol> <li>Virtualenv: If you're using a Python virtual environment, the <code>python</code> and <code>pip</code> commands will point to the versions associated with the active environment, regardless of whether it's Python 2 or 3. So, within a Python 3 virtual environment, <code>python</code> and <code>pip</code> will be equivalent to <code>python3</code> and <code>pip3</code>.</li> </ol>"},{"location":"setup/installation-tips/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Explicit is Better: If you're in doubt, being explicit is usually better. Using <code>python3</code> and <code>pip3</code> makes it clear that you're using Python 3.</p> </li> <li> <p>Check Version: If ever in doubt, you can always check which version you're running by using <code>python --version</code> or <code>python3 --version</code> and <code>pip --version</code> or <code>pip3 --version</code>.</p> </li> </ol> <p>In summary, if you want to make sure you are using Python 3 and its associated package manager, use <code>python3</code> and <code>pip3</code>.</p>"},{"location":"setup/microSD-write-speed/","title":"Selecting the Right MicroSD Card","text":""},{"location":"setup/microSD-write-speed/#size","title":"Size","text":"<p>Although you can boot the Raspberry Pi and NVIDIA Nano on a 16GB microSD card, it will quickly run out of room after you download the 36K Python libraries.  Our suggestion is to use a minimum of a 64 GB card and ideally a 128 GB microSD card.</p> <p></p> <p>In the item above, the \"Class 10\" indicates it will write above 10 MB/sec.</p>"},{"location":"setup/microSD-write-speed/#bandwidth-calculation-for-224x224-color-images","title":"Bandwidth Calculation for 224x224 Color Images","text":"<p>What speed microSD card should you purchase?  Low-cost cards write at 10 MB/sec and more expensive cards can run up to 100 MB/sec.  What one should we purchase?</p> <p>The default image for the Donkey Car is 224x224 color images.</p>"},{"location":"setup/microSD-write-speed/#analysis","title":"Analysis","text":"<p>Calculate the total microSD card write bandwidth needed for a camera that reads 224x224 color images and needs to write 20 images per second to the disk.</p>"},{"location":"setup/microSD-write-speed/#step-by-step-calculation","title":"Step-by-Step Calculation","text":""},{"location":"setup/microSD-write-speed/#step-1-calculate-the-size-of-a-single-image","title":"Step 1: Calculate the Size of a Single Image","text":"<p>A 224x224 color image would have 224 * 224 = 50,176 pixels.</p> <p>Assuming the image uses a standard RGB color scheme, each pixel would have three color channels: Red, Green, and Blue. Each channel typically requires 1 byte (or 8 bits) to store its intensity value. So, each pixel would need 3 bytes.  The size of a single image would be:</p> <pre><code>Size of a single image = Number of pixels * Bytes per pixel\n\n= 50,176 pixels * 3 bytes/pixel\n= 150,528 bytes/image\n= 150 KB\napprox 0.147 MB\n</code></pre>"},{"location":"setup/microSD-write-speed/#step-2-calculate-the-total-bandwidth-needed-per-second","title":"Step 2: Calculate the Total Bandwidth Needed Per Second","text":"<p>We are writing 20 images a second to the disk, so the total bandwidth needed per second would be:</p> <pre><code>Total bandwidth per second = Size of a single image * Number of images per second\n\n= 0.147 MB * 20\n= approx 2.94 megabytes/s\n</code></pre>"},{"location":"setup/microSD-write-speed/#conclusion","title":"Conclusion","text":"<p>The microSD would need a total bandwidth of approximately 3 MB/s to write 20 224x224 color images to the disk each second.  Since most microSD cards today write from 10 MB/s to 100 MB/s there is no worry about having enough write speed to the microSD card.</p>"},{"location":"setup/nano-docker/","title":"Docker on Nano","text":"<p>Note</p> <p>These instructions were copied from a message on the Discord Server that were posed by user naisy on at 08/31/2023 5:48 AM. They are not supported by the DonkeyCar project (yet).</p> <p>Due to the complexities of building the correct environments on the Nano, one alternative is to use the Docker container system.  Although this adds some additional overhead, it means that you can download a virtual machine with all the correct software installed.</p> <p>You must have a minumum of a 64GB microSD card to run Docker on the NVIDIA Nano.</p>"},{"location":"setup/nano-docker/#step-1-download-docker-github-repo","title":"Step 1: Download Docker GitHub Repo","text":"<p>In this step we will clone a repository that has some very small UNIX shell scripts.</p> <pre><code>mkdir ~/projects\ncd ~/projects\ngit clone https://github.com/naisy/Docker\ncd Docker\n</code></pre>"},{"location":"setup/nano-docker/#step-2-run-the-permissions-shell-script","title":"Step 2: Run the Permissions Shell Script","text":"<pre><code>mkdir ~/docker\ncp run-jetson-jp461-donkeycar??.sh ~/docker\ncp run-jetson-jp461-gpio-permission.sh ~/docker\n\ncd ~/docker\n./run-jetson-jp461-gpio-permission.sh\n</code></pre>"},{"location":"setup/nano-docker/#step-3-run-the-main-shell-script","title":"Step 3: Run the Main Shell Script","text":"<p>Due to memory limitations, only one of either DonkeyCar 4.5 or DonkeyCar 5.0 should be activated.</p>"},{"location":"setup/nano-docker/#donkeycar-45","title":"DonkeyCar 4.5","text":"<p>./run-jetson-jp461-donkeycar45.sh</p>"},{"location":"setup/nano-docker/#or-donkeycar-50","title":"or DonkeyCar 5.0","text":"<p>./run-jetson-jp461-donkeycar50.sh</p>"},{"location":"setup/nano-docker/#login-into-the-virtual-machine","title":"Login into the Virtual Machine","text":"<p>Log in to the jupyterlab terminal: PC Web browser for DonkeyCar 4.5</p> <p>`` http://your_jetson_ip:8890/ <pre><code>for DonkeyCar 5.0 (dev3)\n</code></pre> http://your_jetson_ip:8891/ Password: jupyterlab Launch JupyterLab Terminal ```</p> <p>DonkeyCar: From this point on, it is exactly the same as a official donkeycar.</p>"},{"location":"setup/nano-docker/#create-mycar","title":"create mycar","text":"<p>donkey createcar --path ~/data/mycar45 cd ~/data/mycar45 ls</p>"},{"location":"setup/nano-python-upgrade/","title":"How to Upgrade Python on the NVIDIA Nano","text":"<p>The current Jetson Nano image is version 4.6.1. The Donkey Car software only runs on 4.5.1</p> <p>There are several bugs with the Donkey Car system that do not run on Jetson Nano 4.5.2. So how do we get the latest good stuff without allowing the system to automatically update to 4.5.2?</p> <p>Warning</p> <p>Do not run these commands!</p> <pre><code>sudo apt-get update -y\nsudo apt-get upgrade -y\n</code></pre> <p>When the command <code>sudo apt-get upgrade -y</code> is sent, the JetPack dependency tree tries to upgrade to Jetpack version 4.5.2 which will cause the JetPack to lock up when it tries to boot into the Ubuntu Desktop.  Note, this is a bug in the NVIDIA code, not the Donkey Car code.</p> <p>We should never run <code>sudo apt-get update -y</code> or <code>sudo apt-get upgrade -y</code> when running JP4.5.1.</p> <p>The following is how we upgrade Ubuntu, but keep JP from attempting to upgrade to JP4.5.2:</p> <ol> <li>Once the Desktop has opened, select \"System Settings\" (the Gear with the wrench on top), then select \"System &amp; Updates\".</li> <li>When \"System &amp; Updates\" has opened, select the \"Other Software\" Tab.</li> <li>When the \"Other Software\" Tab opens, starting from the top, uncheck boxes that reference the Jetpack and CUDA libraries - 3, 6, and 7, then select \"close\".</li> <li>Unchecking those boxes prevents the \"Software Updater\" from updating JP to 4.5.2, but allows Ubuntu and other software to update.</li> </ol> <p></p> <p>Warning</p> <p>Always use the \"Software Updater GUI\" to update your Ubuntu and never run <code>sudo apt-get upgrade -y</code>.</p> <p>After completing the above process, let the Desktop stay open for a while and the \"Software Updater\" will appear.</p> <p>Let the \"Software Updater\" update Ubuntu whenever you use the Desktop. </p>"},{"location":"setup/nano-python-upgrade/#references","title":"References","text":"<ul> <li>Jetson Hacks</li> </ul>"},{"location":"setup/nano-sd-image-checklist/","title":"Nano SD Image Checklist","text":"<p>This is a checklist that is genralized for all our events.  We can't assume any network connectivity at these events.</p>"},{"location":"setup/nano-sd-image-checklist/#required","title":"Required","text":"<ol> <li>Image is based on the Nvidia Jetson image</li> <li>There is a user \"donkey\" with a password \"car\"</li> <li>The desktop has Chromium and Terminl locked at the top</li> <li>The DonkeyCar Bookmarks are in place</li> <li>The default WiFi is setup and working for the event (use a guest account if we are at company site?)</li> <li>A virtual envinroment is setup and the user is set to that automatically at the end of the .basrc script</li> <li>The Latest DonkeyCar software installed consistently and tested</li> </ol>"},{"location":"setup/nano-sd-image-checklist/#optional","title":"Optional","text":"<ol> <li>Swap file setup (at least 6 gig) for compiling OpenCV</li> <li>The CSI Camera demo is installed from the Jetson Hacks site to test the camera and do face recognition demos</li> <li>The evtest program in installed to test the Logitech F710 joystick</li> <li>The default config.py and myconfig.py are setup and customized for the CSI camera</li> <li>The default image in the config.py file is 224X224</li> <li>The \"desktop\" apps (word processing, spreadsheets, presentations) have been removed from the default dock</li> <li>The Chrome browser and the Terminal are on the dock</li> <li>The Chome bookmark bar is enabled (go to the Chrome Settings)</li> <li>The latest version of OpenCV (cv2) is installed, compiled and tested</li> <li>Decent python editor?</li> <li>Jupyter notebook support (arm version!)</li> <li>Sample Jupyter notebooks installed for viewing tub data and cleaning up the tub files (removing data with no speed)</li> </ol>"},{"location":"setup/nano-sd-image-checklist/#unknowns","title":"Unknowns","text":"<ol> <li>Can we write a menu-driven UNIX script that will automatically copy tubs to a GPU server and get a model back?</li> <li>Can we assign static IP addresses and names to each car (dk1, dk2, dk3)</li> <li>Can we assume that ALL cars use the same default calibration?</li> <li>Can we write a short test script to verify that all the components are installed and working?</li> <li>What standards should we have for the GPU servers (Ubuntu, not RedHat)</li> <li>What other </li> </ol>"},{"location":"setup/pre-drive-checklist/","title":"DonkeyCar Pre-Drive Checklist","text":"<ol> <li>Do you have all the software installed correctly?</li> <li>dependencies installed</li> <li>donkey command workings</li> <li>Do you have your PWM Board working?</li> <li>LED light on the PWM card</li> <li>i2cdetect working</li> <li>Battery checks</li> <li>Voltage in the motor battery is 7.2 volts</li> <li>Power level in SBC battery is 100%</li> <li>Configuration File</li> <li>Streering and Throttle calibrated</li> </ol>"},{"location":"setup/pwm-board/","title":"PWM Board Deep Dive","text":"<p>The DonkeyCar uses the low cost PCA9685 PWM board.</p>"},{"location":"setup/pwm-board/#pca9685-pwm-board","title":"PCA9685 PWM Board","text":""},{"location":"setup/pwm-board/#pi-40-pin-header","title":"Pi 40 Pin Header","text":""},{"location":"setup/pwm-board/#connections","title":"Connections","text":""},{"location":"setup/pwm-board/#references","title":"References","text":"<ul> <li>Connections Handown (PowerPoint)</li> <li>Using a PCA9685 module with Raspberry Pi</li> </ul>"},{"location":"setup/raspberry-pi-parts/","title":"Raspberry Pi Parts","text":""},{"location":"setup/raspberry-pi-parts/#single-board-computer","title":"Single Board Computer","text":"<p>We only use the 8GB version:</p> <p>Raspberry Pi 4</p>"},{"location":"setup/raspberry-pi-parts/#power-supply","title":"Power Supply","text":"<p>The Raspberry Pi 4 needs a 5V.1, 3A (15W) USB-C Power Supply.  It is unclear to us if you can connect existing barrel connectors power directly to the 40-pin 5V/GND pins.</p> <p>Offical Raspberry Pi Page</p> <p>Digi-Key Part Number 2648-SC0445-ND - AC/DC WALL MNT ADAPTER 5.1V 15W</p> <p>Amason Raspberry Pi 15W USB-C Power Supply US - White $15 - Official Raspberry Pi 4 type-c power supply Input:100-240V 50/60Hz 0.5A,Output:5.1-3.0A</p>"},{"location":"setup/raspberry-pi-parts/#40-pin-power-option","title":"40-Pin Power Option","text":"<ul> <li>Back Powering the Pi via the GPIO Header</li> <li>Can Pi 4 be powered through GPIO pins?</li> <li>power rpi-4 via the 40 pin header</li> </ul>"},{"location":"setup/software-install-notes/","title":"AI Racing League Software Installation","text":""},{"location":"setup/software-install-notes/#apt-get","title":"Apt-get","text":"<p>Apt-get is the software package installed on the Raspberry Pi OS that allows you to install application libraries.</p>"},{"location":"setup/software-install-notes/#donkeycar-libraries-required","title":"DonkeyCar Libraries (required)","text":"<pre><code>sudo apt-get install build-essential python3 python3-dev python3-pip python3-virtualenv python3-numpy python3-picamera python3-pandas python3-rpi.gpio i2c-tools avahi-utils joystick libopenjp2-7-dev libtiff5-dev gfortran libatlas-base-dev libopenblas-dev libhdf5-serial-dev libgeos-dev git ntp\n</code></pre> <ol> <li>build-essential is the library that tracks software library dependency lists.</li> <li>python3 is the library that runs Python 3.  Note that the Raspberry Pi only has Python 2.7 as the default version.  All our DonkeyCar software requires Python 3.7.</li> <li>python3-dev includes software development tools to manage python 3.</li> <li>python3-pip is the pip tool that allows you to install a specific version of a python library</li> <li>python-virtualenv is the tool that allows you to setup a virtual environment for the DonkeyCar specific libraries.  We use this tool instead of the conda tools.</li> <li>python3-numpy are the numerical procssing python libraries.</li> <li>python3-picamera are the libaries to work with the camera on the Raspberry pi.</li> <li>python3-pandas are tools that allow data access for example reading CSV and JSON files.</li> <li>python2-rpi are python libraries that work with the Raspberry Pi.</li> <li>i2ctools are tools that allow you to work with the I2C communications bus.  This is the bus that is used to control the PWM board and which controls the throttle and steering.</li> </ol> <p>The other libraries are mostly small support libraries used for supporting debugging.</p>"},{"location":"setup/software-install-notes/#opencv-optional","title":"OpenCV (optional)","text":"<pre><code>sudo apt-get install libilmbase-dev libopenexr-dev libgstreamer1.0-dev libjasper-dev libwebp-dev libatlas-base-dev libavcodec-dev libavformat-dev libswscale-dev libqtgui4 libqt4-test\n</code></pre>"},{"location":"setup/track-options/","title":"Track Options","text":"<p>Although you can just put tape down on a floor, that is time-consuming and is often a low-quality track.   There are several other options and the prices vary from under $100 to $1,300.</p>"},{"location":"setup/track-options/#vinyl-tarps","title":"Vinyl Tarps","text":"<p>In Minnesota, Billboard Tarps sells used vinyl sign material.  For around $70 you can get a 16' X 25' used black billboard 9oz black vinyl sign that is ideal for creating your own track.</p> <p>Here is a full-size track that is 25' X 35' for about $170:</p> <p></p> <p>This is what 1/2 of the 16'X'25 foot tarp looks like after delivery from Billboard Tarps and Vinyl.</p> <p></p> <p>Billboard Tarps and Vinyl - We suggest you get a dark color (black or dark blue) and then tape down white edges and a yellow dashed line in the center.  You can also buy smaller scraps and use HH-66 vinyl cement as an adhesive to glue smaller tarps together.  You can also glue a black track on a white background to make the road.</p>"},{"location":"setup/track-options/#picking-the-right-size","title":"Picking the Right Size","text":"<p>The typical dimensions of a full-event track is 22 x 34 feet. These dimensions are based on the DIYRobocars Standard Track, which is a popular track for donkey car racing.</p> <p>The smaller track is a good option for beginners, as it is easier to navigate and control. The larger track is a better option for experienced drivers, as it offers more challenges and opportunities for speed.</p> <p>Of course, the dimensions of a donkey car track can vary depending on the specific design. However, the dimensions listed above are a good starting point for anyone who is planning to build or race a donkey car.</p>"},{"location":"setup/track-options/#keeping-a-standard-width","title":"Keeping A Standard Width","text":"<p>The standard width of all the \"road\" tracks is two feet or 24 inches.  This is the distance to the centerline of the white edges.  The roads are typically black with a white edge and a dashed yellow line down the middle of the track.  The key is to have a high contrast between the black road and the white edges.</p> <p>Many people use 2\" (or 1 and 7/8\") inch wide duct tape or Gaffers tape. Gaffer's tape is often preferred for temporary events on carpet. Gaffer's tape doesn't harm the surface to which it adhered.</p>"},{"location":"setup/track-options/#minnesota-stem-partners-tracks","title":"Minnesota STEM Partners Tracks","text":"<p>Below is a sample of a tarp purchased from Billboard Tarps.  Note the actual track is twice this size since it is still folded in half in this photo.</p> <p></p> <p>Track setup in the driver training room:</p> <p></p> <p>Note that this track does not adhere to the 2-foot wide rule.  This is sometimes done when you have many students doing practice driving on the same track.</p> <p></p>"},{"location":"setup/track-options/#optum-track","title":"Optum Track","text":"<p>Optum printed their own track on a local printer that specialized in printing large format signage.  The custom printing cost was about $1,300.00</p>"},{"location":"setup/track-options/#best-buy-track","title":"Best Buy Track","text":"<p>Best Buy also printed its own track for their events.  This photo only shows about 1/3 of the track.</p> <p></p>"},{"location":"setup/track-options/#dan-mccrearys-basement-track","title":"Dan McCreary's Basement Track","text":"<p>This track is just a single piece of white electrical tape.</p> <p></p>"},{"location":"setup/track-options/#interlocking-foam-mats","title":"Interlocking Foam Mats","text":"<p>You can also purchase interlocking foam mats. These are typically two feet by two feet and cost about $30 for a package of 6.  Since each package covers 24 square feet and a full track is about 24x36 feet (758 square feet) we can see the cost of 32 packages is around $960.00.</p> <p>Interlocking Foam Mats From WalMart</p> <p>Amazon Foam Mats</p>"},{"location":"setup/track-options/#references","title":"References","text":"<p>DIYRobocars Standard Track</p>"},{"location":"training-logs/dans-basement/","title":"Dans Basement Training Log","text":"<p>I have a very small track in my basement.  I put down a single white line about 3/4 inch wide using white electrical tape.  The background was a marble blue expoy floor with a lot of color variation.  The surface was very reflective and there were lights in the ceiling with lots of glare.  I drove the car around 10 times in each direction and collected around 4,500 images.</p>"},{"location":"training-logs/dans-basement/#catalogs","title":"Catalogs","text":"<p>I manually edited the catlog files and then edited the manifest.json file to modify the paths:</p> <pre><code>{\"paths\": [\"catalog_3.catalog\", \"catalog_4.catalog\", \"catalog_5.catalog\", \"catalog_6.catalog\", \"catalog_7.catalog\"]\n</code></pre> <pre><code>wc -l data/dans-basement/*.catalog\n</code></pre> <pre><code>   781 data/dans-basement/catalog_3.catalog\n  1000 data/dans-basement/catalog_4.catalog\n  1000 data/dans-basement/catalog_5.catalog\n  1000 data/dans-basement/catalog_6.catalog\n   750 data/dans-basement/catalog_7.catalog\n  4531 total\n</code></pre> <p>This matched the <code>ls -1 ~/mycar/data/dans-basement/images | wc -l</code> command that counted the number of images.</p> <p>I time the training time on the NIVID RTX 2080 and got the model trained in about 1.5 minutes. <pre><code>$ time donkey train --tub=./data/dans-basement --model=./models/dans-basement.h5\n________             ______                   _________              \n___  __ \\_______________  /___________  __    __  ____/_____ ________\n__  / / /  __ \\_  __ \\_  //_/  _ \\_  / / /    _  /    _  __ `/_  ___/\n_  /_/ // /_/ /  / / /  ,&lt;  /  __/  /_/ /     / /___  / /_/ /_  /    \n/_____/ \\____//_/ /_//_/|_| \\___/_\\__, /      \\____/  \\__,_/ /_/     \n                                 /____/                              \n\nusing donkey v4.2.1 ...\nloading config file: ./config.py\nloading personal config over-rides from myconfig.py\n\"get_model_by_type\" model Type is: linear\nCreated KerasLinear\n2021-07-26 21:05:34.259364: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n2021-07-26 21:05:34.278301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-07-26 21:05:34.278898: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \npciBusID: 0000:09:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\ncoreClock: 1.635GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\n2021-07-26 21:05:34.279098: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n2021-07-26 21:05:34.280320: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n2021-07-26 21:05:34.281822: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n2021-07-26 21:05:34.282037: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n2021-07-26 21:05:34.283140: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n2021-07-26 21:05:34.283726: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n2021-07-26 21:05:34.285524: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n2021-07-26 21:05:34.285676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-07-26 21:05:34.286176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-07-26 21:05:34.286568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n2021-07-26 21:05:34.286793: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n2021-07-26 21:05:34.290920: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3592950000 Hz\n2021-07-26 21:05:34.291228: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557d8a05bbb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n2021-07-26 21:05:34.291241: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n2021-07-26 21:05:34.291374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-07-26 21:05:34.291795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \npciBusID: 0000:09:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\ncoreClock: 1.635GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\n2021-07-26 21:05:34.291830: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n2021-07-26 21:05:34.291842: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n2021-07-26 21:05:34.291852: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n2021-07-26 21:05:34.291862: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n2021-07-26 21:05:34.291872: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n2021-07-26 21:05:34.291881: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n2021-07-26 21:05:34.291891: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n2021-07-26 21:05:34.291955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-07-26 21:05:34.292398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-07-26 21:05:34.292782: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n2021-07-26 21:05:34.292805: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n2021-07-26 21:05:34.366898: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n2021-07-26 21:05:34.366930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n2021-07-26 21:05:34.366937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n2021-07-26 21:05:34.367194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-07-26 21:05:34.367855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-07-26 21:05:34.368446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-07-26 21:05:34.368971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9911 MB memory) -&gt; physical GPU (device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:09:00.0, compute capability: 7.5)\n2021-07-26 21:05:34.370680: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557d8bec8fa0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n2021-07-26 21:05:34.370693: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\nModel: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nimg_in (InputLayer)             [(None, 224, 224, 3) 0                                            \n__________________________________________________________________________________________________\nconv2d_1 (Conv2D)               (None, 110, 110, 24) 1824        img_in[0][0]                     \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, 110, 110, 24) 0           conv2d_1[0][0]                   \n__________________________________________________________________________________________________\nconv2d_2 (Conv2D)               (None, 53, 53, 32)   19232       dropout[0][0]                    \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 53, 53, 32)   0           conv2d_2[0][0]                   \n__________________________________________________________________________________________________\nconv2d_3 (Conv2D)               (None, 25, 25, 64)   51264       dropout_1[0][0]                  \n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, 25, 25, 64)   0           conv2d_3[0][0]                   \n__________________________________________________________________________________________________\nconv2d_4 (Conv2D)               (None, 23, 23, 64)   36928       dropout_2[0][0]                  \n__________________________________________________________________________________________________\ndropout_3 (Dropout)             (None, 23, 23, 64)   0           conv2d_4[0][0]                   \n__________________________________________________________________________________________________\nconv2d_5 (Conv2D)               (None, 21, 21, 64)   36928       dropout_3[0][0]                  \n__________________________________________________________________________________________________\ndropout_4 (Dropout)             (None, 21, 21, 64)   0           conv2d_5[0][0]                   \n__________________________________________________________________________________________________\nflattened (Flatten)             (None, 28224)        0           dropout_4[0][0]                  \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 100)          2822500     flattened[0][0]                  \n__________________________________________________________________________________________________\ndropout_5 (Dropout)             (None, 100)          0           dense_1[0][0]                    \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 50)           5050        dropout_5[0][0]                  \n__________________________________________________________________________________________________\ndropout_6 (Dropout)             (None, 50)           0           dense_2[0][0]                    \n__________________________________________________________________________________________________\nn_outputs0 (Dense)              (None, 1)            51          dropout_6[0][0]                  \n__________________________________________________________________________________________________\nn_outputs1 (Dense)              (None, 1)            51          dropout_6[0][0]                  \n==================================================================================================\nTotal params: 2,973,828\nTrainable params: 2,973,828\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\nUsing catalog /home/arl/mycar/data/dans-basement/catalog_7.catalog\n\nRecords # Training 3364\nRecords # Validation 842\nEpoch 1/100\n2021-07-26 21:05:35.291438: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n2021-07-26 21:05:35.613762: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n2021-07-26 21:05:36.322576: W tensorflow/stream_executor/gpu/asm_compiler.cc:116] *** WARNING *** You are using ptxas 9.1.108, which is older than 9.2.88. ptxas 9.x before 9.2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n\nYou do not need to update to CUDA 9.2.88; cherry-picking the ptxas binary is sufficient.\n2021-07-26 21:05:36.376195: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 65280, output: ptxas fatal   : Value 'sm_75' is not defined for option 'gpu-name'\n\nRelying on driver to perform ptx compilation. \nModify $PATH to customize ptxas location.\nThis message will be only logged once.\n27/27 [==============================] - ETA: 0s - loss: 0.2495 - n_outputs0_loss: 0.1717 - n_outputs1_loss: 0.0778\nEpoch 00001: val_loss improved from inf to 0.14744, saving model to ./models/dans-basement.h5\n27/27 [==============================] - 8s 301ms/step - loss: 0.2495 - n_outputs0_loss: 0.1717 - n_outputs1_loss: 0.0778 - val_loss: 0.1474 - val_n_outputs0_loss: 0.1291 - val_n_outputs1_loss: 0.0183\nEpoch 2/100\n27/27 [==============================] - ETA: 0s - loss: 0.1487 - n_outputs0_loss: 0.1265 - n_outputs1_loss: 0.0223\nEpoch 00002: val_loss improved from 0.14744 to 0.09815, saving model to ./models/dans-basement.h5\n27/27 [==============================] - 3s 120ms/step - loss: 0.1487 - n_outputs0_loss: 0.1265 - n_outputs1_loss: 0.0223 - val_loss: 0.0981 - val_n_outputs0_loss: 0.0777 - val_n_outputs1_loss: 0.0205\nEpoch 3/100\n27/27 [==============================] - ETA: 0s - loss: 0.1075 - n_outputs0_loss: 0.0893 - n_outputs1_loss: 0.0182\nEpoch 00003: val_loss improved from 0.09815 to 0.07897, saving model to ./models/dans-basement.h5\n27/27 [==============================] - 3s 117ms/step - loss: 0.1075 - n_outputs0_loss: 0.0893 - n_outputs1_loss: 0.0182 - val_loss: 0.0790 - val_n_outputs0_loss: 0.0687 - val_n_outputs1_loss: 0.0102\nEpoch 4/100\n27/27 [==============================] - ETA: 0s - loss: 0.0917 - n_outputs0_loss: 0.0759 - n_outputs1_loss: 0.0158\nEpoch 00004: val_loss improved from 0.07897 to 0.07055, saving model to ./models/dans-basement.h5\n27/27 [==============================] - 3s 110ms/step - loss: 0.0917 - n_outputs0_loss: 0.0759 - n_outputs1_loss: 0.0158 - val_loss: 0.0705 - val_n_outputs0_loss: 0.0610 - val_n_outputs1_loss: 0.0096\nEpoch 5/100\n27/27 [==============================] - ETA: 0s - loss: 0.0880 - n_outputs0_loss: 0.0734 - n_outputs1_loss: 0.0146\nEpoch 00005: val_loss did not improve from 0.07055\n27/27 [==============================] - 3s 105ms/step - loss: 0.0880 - n_outputs0_loss: 0.0734 - n_outputs1_loss: 0.0146 - val_loss: 0.0751 - val_n_outputs0_loss: 0.0553 - val_n_outputs1_loss: 0.0198\nEpoch 6/100\n27/27 [==============================] - ETA: 0s - loss: 0.0757 - n_outputs0_loss: 0.0629 - n_outputs1_loss: 0.0127\nEpoch 00006: val_loss improved from 0.07055 to 0.05840, saving model to ./models/dans-basement.h5\n27/27 [==============================] - 3s 111ms/step - loss: 0.0757 - n_outputs0_loss: 0.0629 - n_outputs1_loss: 0.0127 - val_loss: 0.0584 - val_n_outputs0_loss: 0.0485 - val_n_outputs1_loss: 0.0099\nEpoch 7/100\n27/27 [==============================] - ETA: 0s - loss: 0.0672 - n_outputs0_loss: 0.0551 - n_outputs1_loss: 0.0120\nEpoch 00007: val_loss improved from 0.05840 to 0.05028, saving model to ./models/dans-basement.h5\n27/27 [==============================] - 3s 110ms/step - loss: 0.0672 - n_outputs0_loss: 0.0551 - n_outputs1_loss: 0.0120 - val_loss: 0.0503 - val_n_outputs0_loss: 0.0450 - val_n_outputs1_loss: 0.0053\nEpoch 8/100\n27/27 [==============================] - ETA: 0s - loss: 0.0621 - n_outputs0_loss: 0.0510 - n_outputs1_loss: 0.0111\nEpoch 00008: val_loss improved from 0.05028 to 0.04540, saving model to ./models/dans-basement.h5\n27/27 [==============================] - 3s 110ms/step - loss: 0.0621 - n_outputs0_loss: 0.0510 - n_outputs1_loss: 0.0111 - val_loss: 0.0454 - val_n_outputs0_loss: 0.0385 - val_n_outputs1_loss: 0.0069\nEpoch 9/100\n27/27 [==============================] - ETA: 0s - loss: 0.0545 - n_outputs0_loss: 0.0441 - n_outputs1_loss: 0.0104\nEpoch 00009: val_loss improved from 0.04540 to 0.04351, saving model to ./models/dans-basement.h5\n27/27 [==============================] - 3s 107ms/step - loss: 0.0545 - n_outputs0_loss: 0.0441 - n_outputs1_loss: 0.0104 - val_loss: 0.0435 - val_n_outputs0_loss: 0.0358 - val_n_outputs1_loss: 0.0077\nEpoch 10/100\n27/27 [==============================] - ETA: 0s - loss: 0.0558 - n_outputs0_loss: 0.0458 - n_outputs1_loss: 0.0099\nEpoch 00010: val_loss improved from 0.04351 to 0.04070, saving model to ./models/dans-basement.h5\n27/27 [==============================] - 3s 110ms/step - loss: 0.0558 - n_outputs0_loss: 0.0458 - n_outputs1_loss: 0.0099 - val_loss: 0.0407 - val_n_outputs0_loss: 0.0357 - val_n_outputs1_loss: 0.0050\nEpoch 11/100\n27/27 [==============================] - ETA: 0s - loss: 0.0505 - n_outputs0_loss: 0.0415 - n_outputs1_loss: 0.0090\nEpoch 00011: val_loss improved from 0.04070 to 0.03935, saving model to ./models/dans-basement.h5\n27/27 [==============================] - 3s 109ms/step - loss: 0.0505 - n_outputs0_loss: 0.0415 - n_outputs1_loss: 0.0090 - val_loss: 0.0393 - val_n_outputs0_loss: 0.0340 - val_n_outputs1_loss: 0.0054\nEpoch 12/100\n27/27 [==============================] - ETA: 0s - loss: 0.0476 - n_outputs0_loss: 0.0388 - n_outputs1_loss: 0.0088\nEpoch 00012: val_loss improved from 0.03935 to 0.03624, saving model to ./models/dans-basement.h5\n27/27 [==============================] - 3s 110ms/step - loss: 0.0476 - n_outputs0_loss: 0.0388 - n_outputs1_loss: 0.0088 - val_loss: 0.0362 - val_n_outputs0_loss: 0.0298 - val_n_outputs1_loss: 0.0065\nEpoch 13/100\n27/27 [==============================] - ETA: 0s - loss: 0.0453 - n_outputs0_loss: 0.0373 - n_outputs1_loss: 0.0080\nEpoch 00013: val_loss improved from 0.03624 to 0.03507, saving model to ./models/dans-basement.h5\n27/27 [==============================] - 3s 108ms/step - loss: 0.0453 - n_outputs0_loss: 0.0373 - n_outputs1_loss: 0.0080 - val_loss: 0.0351 - val_n_outputs0_loss: 0.0294 - val_n_outputs1_loss: 0.0057\nEpoch 14/100\n27/27 [==============================] - ETA: 0s - loss: 0.0430 - n_outputs0_loss: 0.0352 - n_outputs1_loss: 0.0079\nEpoch 00014: val_loss improved from 0.03507 to 0.03211, saving model to ./models/dans-basement.h5\n27/27 [==============================] - 3s 111ms/step - loss: 0.0430 - n_outputs0_loss: 0.0352 - n_outputs1_loss: 0.0079 - val_loss: 0.0321 - val_n_outputs0_loss: 0.0265 - val_n_outputs1_loss: 0.0056\nEpoch 15/100\n27/27 [==============================] - ETA: 0s - loss: 0.0397 - n_outputs0_loss: 0.0327 - n_outputs1_loss: 0.0070\nEpoch 00015: val_loss improved from 0.03211 to 0.03208, saving model to ./models/dans-basement.h5\n27/27 [==============================] - 3s 110ms/step - loss: 0.0397 - n_outputs0_loss: 0.0327 - n_outputs1_loss: 0.0070 - val_loss: 0.0321 - val_n_outputs0_loss: 0.0279 - val_n_outputs1_loss: 0.0042\nEpoch 16/100\n27/27 [==============================] - ETA: 0s - loss: 0.0382 - n_outputs0_loss: 0.0316 - n_outputs1_loss: 0.0065\nEpoch 00016: val_loss improved from 0.03208 to 0.02880, saving model to ./models/dans-basement.h5\n27/27 [==============================] - 3s 108ms/step - loss: 0.0382 - n_outputs0_loss: 0.0316 - n_outputs1_loss: 0.0065 - val_loss: 0.0288 - val_n_outputs0_loss: 0.0243 - val_n_outputs1_loss: 0.0046\nEpoch 17/100\n27/27 [==============================] - ETA: 0s - loss: 0.0381 - n_outputs0_loss: 0.0313 - n_outputs1_loss: 0.0069\nEpoch 00017: val_loss did not improve from 0.02880\n27/27 [==============================] - 3s 104ms/step - loss: 0.0381 - n_outputs0_loss: 0.0313 - n_outputs1_loss: 0.0069 - val_loss: 0.0322 - val_n_outputs0_loss: 0.0281 - val_n_outputs1_loss: 0.0041\nEpoch 18/100\n27/27 [==============================] - ETA: 0s - loss: 0.0375 - n_outputs0_loss: 0.0310 - n_outputs1_loss: 0.0065\nEpoch 00018: val_loss did not improve from 0.02880\n27/27 [==============================] - 3s 107ms/step - loss: 0.0375 - n_outputs0_loss: 0.0310 - n_outputs1_loss: 0.0065 - val_loss: 0.0293 - val_n_outputs0_loss: 0.0257 - val_n_outputs1_loss: 0.0036\nEpoch 19/100\n27/27 [==============================] - ETA: 0s - loss: 0.0372 - n_outputs0_loss: 0.0308 - n_outputs1_loss: 0.0064\nEpoch 00019: val_loss did not improve from 0.02880\n27/27 [==============================] - 3s 108ms/step - loss: 0.0372 - n_outputs0_loss: 0.0308 - n_outputs1_loss: 0.0064 - val_loss: 0.0307 - val_n_outputs0_loss: 0.0275 - val_n_outputs1_loss: 0.0032\nEpoch 20/100\n27/27 [==============================] - ETA: 0s - loss: 0.0347 - n_outputs0_loss: 0.0285 - n_outputs1_loss: 0.0062\nEpoch 00020: val_loss did not improve from 0.02880\n27/27 [==============================] - 3s 104ms/step - loss: 0.0347 - n_outputs0_loss: 0.0285 - n_outputs1_loss: 0.0062 - val_loss: 0.0325 - val_n_outputs0_loss: 0.0283 - val_n_outputs1_loss: 0.0042\nEpoch 21/100\n27/27 [==============================] - ETA: 0s - loss: 0.0349 - n_outputs0_loss: 0.0290 - n_outputs1_loss: 0.0058\nEpoch 00021: val_loss did not improve from 0.02880\n27/27 [==============================] - 3s 107ms/step - loss: 0.0349 - n_outputs0_loss: 0.0290 - n_outputs1_loss: 0.0058 - val_loss: 0.0293 - val_n_outputs0_loss: 0.0258 - val_n_outputs1_loss: 0.0035\nWARNING: CPU random generator seem to be failing, disable hardware random number generation\nWARNING: RDRND generated: 0xffffffff 0xffffffff 0xffffffff 0xffffffff\n\nreal    1m26.930s\nuser    1m30.911s\nsys 0m42.818s\n</code></pre></p>"},{"location":"training-logs/msp-1-cpu/","title":"Training Log for MSP Car #1","text":"<p>After cleanup we only got about 1,500 records.  But here is a log of the training.  It took about 1.5 minutes.</p> <p><pre><code>$ donkey train --tub=./data/msp-car-1 --model=./models/msp-car-1.f5\n</code></pre> _             ______                   _________             ___  __ ___  /__  __    __  _/_ ______ __  / / /  __ _  __ _  ///  _ _  / / /    _  /    _  __ `/  ___/ _  // // // /  / / /  ,&lt;  /  __/  // /     / /  / // /_  /   // _/// ////|| _/__, /      _/  _,/ /_/                                     /____/                              </p> <p>using donkey v4.2.1 ... loading config file: ./config.py loading personal config over-rides from myconfig.py \"get_model_by_type\" model Type is: linear Created KerasLinear 2021-07-26 19:50:45.562205: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA 2021-07-26 19:50:45.565106: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3592950000 Hz 2021-07-26 19:50:45.565470: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d85e9d19f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices: 2021-07-26 19:50:45.565492: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version 2021-07-26 19:50:45.565578: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. Model: \"model\"</p>"},{"location":"training-logs/msp-1-cpu/#layer-type-output-shape-param-connected-to","title":"Layer (type)                    Output Shape         Param #     Connected to","text":"<p>img_in (InputLayer)             [(None, 224, 224, 3) 0                                            </p> <p>conv2d_1 (Conv2D)               (None, 110, 110, 24) 1824        img_in[0][0]                     </p> <p>dropout (Dropout)               (None, 110, 110, 24) 0           conv2d_1[0][0]                   </p> <p>conv2d_2 (Conv2D)               (None, 53, 53, 32)   19232       dropout[0][0]                    </p> <p>dropout_1 (Dropout)             (None, 53, 53, 32)   0           conv2d_2[0][0]                   </p> <p>conv2d_3 (Conv2D)               (None, 25, 25, 64)   51264       dropout_1[0][0]                  </p> <p>dropout_2 (Dropout)             (None, 25, 25, 64)   0           conv2d_3[0][0]                   </p> <p>conv2d_4 (Conv2D)               (None, 23, 23, 64)   36928       dropout_2[0][0]                  </p> <p>dropout_3 (Dropout)             (None, 23, 23, 64)   0           conv2d_4[0][0]                   </p> <p>conv2d_5 (Conv2D)               (None, 21, 21, 64)   36928       dropout_3[0][0]                  </p> <p>dropout_4 (Dropout)             (None, 21, 21, 64)   0           conv2d_5[0][0]                   </p> <p>flattened (Flatten)             (None, 28224)        0           dropout_4[0][0]                  </p> <p>dense_1 (Dense)                 (None, 100)          2822500     flattened[0][0]                  </p> <p>dropout_5 (Dropout)             (None, 100)          0           dense_1[0][0]                    </p> <p>dense_2 (Dense)                 (None, 50)           5050        dropout_5[0][0]                  </p> <p>dropout_6 (Dropout)             (None, 50)           0           dense_2[0][0]                    </p> <p>n_outputs0 (Dense)              (None, 1)            51          dropout_6[0][0]                  </p>"},{"location":"training-logs/msp-1-cpu/#n_outputs1-dense-none-1-51-dropout_600","title":"n_outputs1 (Dense)              (None, 1)            51          dropout_6[0][0]","text":"<p>Total params: 2,973,828 Trainable params: 2,973,828 Non-trainable params: 0</p> <p>None Using catalog /home/arl/mycar/data/msp-car-1/catalog_17.catalog</p> <p>Records # Training 1265 Records # Validation 317 Epoch 1/100 10/10 [==============================] - ETA: 0s - loss: 1.0885 - n_outputs0_loss: 0.5975 - n_outputs1_loss: 0.4909 Epoch 00001: val_loss improved from inf to 0.54341, saving model to ./models/msp-car-1.f5 2021-07-26 19:50:57.881390: W tensorflow/python/util/util.cc:329] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them. WARNING:tensorflow:From /home/arl/miniconda3/envs/donkey/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.init (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating: If using Keras pass *_constraint arguments to layers. 10/10 [==============================] - 11s 1s/step - loss: 1.0885 - n_outputs0_loss: 0.5975 - n_outputs1_loss: 0.4909 - val_loss: 0.5434 - val_n_outputs0_loss: 0.4668 - val_n_outputs1_loss: 0.0767 Epoch 2/100 10/10 [==============================] - ETA: 0s - loss: 0.5522 - n_outputs0_loss: 0.4640 - n_outputs1_loss: 0.0882 Epoch 00002: val_loss improved from 0.54341 to 0.53272, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 999ms/step - loss: 0.5522 - n_outputs0_loss: 0.4640 - n_outputs1_loss: 0.0882 - val_loss: 0.5327 - val_n_outputs0_loss: 0.4605 - val_n_outputs1_loss: 0.0722 Epoch 3/100 10/10 [==============================] - ETA: 0s - loss: 0.5392 - n_outputs0_loss: 0.4638 - n_outputs1_loss: 0.0754 Epoch 00003: val_loss improved from 0.53272 to 0.50775, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.5392 - n_outputs0_loss: 0.4638 - n_outputs1_loss: 0.0754 - val_loss: 0.5077 - val_n_outputs0_loss: 0.4551 - val_n_outputs1_loss: 0.0527 Epoch 4/100 10/10 [==============================] - ETA: 0s - loss: 0.5318 - n_outputs0_loss: 0.4605 - n_outputs1_loss: 0.0713 Epoch 00004: val_loss improved from 0.50775 to 0.49783, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 999ms/step - loss: 0.5318 - n_outputs0_loss: 0.4605 - n_outputs1_loss: 0.0713 - val_loss: 0.4978 - val_n_outputs0_loss: 0.4455 - val_n_outputs1_loss: 0.0523 Epoch 5/100 10/10 [==============================] - ETA: 0s - loss: 0.5333 - n_outputs0_loss: 0.4608 - n_outputs1_loss: 0.0725 Epoch 00005: val_loss improved from 0.49783 to 0.49721, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.5333 - n_outputs0_loss: 0.4608 - n_outputs1_loss: 0.0725 - val_loss: 0.4972 - val_n_outputs0_loss: 0.4451 - val_n_outputs1_loss: 0.0521 Epoch 6/100 10/10 [==============================] - ETA: 0s - loss: 0.5277 - n_outputs0_loss: 0.4619 - n_outputs1_loss: 0.0658 Epoch 00006: val_loss did not improve from 0.49721 10/10 [==============================] - 9s 934ms/step - loss: 0.5277 - n_outputs0_loss: 0.4619 - n_outputs1_loss: 0.0658 - val_loss: 0.4981 - val_n_outputs0_loss: 0.4461 - val_n_outputs1_loss: 0.0520 Epoch 7/100 10/10 [==============================] - ETA: 0s - loss: 0.5265 - n_outputs0_loss: 0.4577 - n_outputs1_loss: 0.0688 Epoch 00007: val_loss improved from 0.49721 to 0.49668, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.5265 - n_outputs0_loss: 0.4577 - n_outputs1_loss: 0.0688 - val_loss: 0.4967 - val_n_outputs0_loss: 0.4442 - val_n_outputs1_loss: 0.0525 Epoch 8/100 10/10 [==============================] - ETA: 0s - loss: 0.5138 - n_outputs0_loss: 0.4467 - n_outputs1_loss: 0.0671 Epoch 00008: val_loss improved from 0.49668 to 0.49536, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.5138 - n_outputs0_loss: 0.4467 - n_outputs1_loss: 0.0671 - val_loss: 0.4954 - val_n_outputs0_loss: 0.4408 - val_n_outputs1_loss: 0.0546 Epoch 9/100 10/10 [==============================] - ETA: 0s - loss: 0.5109 - n_outputs0_loss: 0.4468 - n_outputs1_loss: 0.0642 Epoch 00009: val_loss improved from 0.49536 to 0.48741, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.5109 - n_outputs0_loss: 0.4468 - n_outputs1_loss: 0.0642 - val_loss: 0.4874 - val_n_outputs0_loss: 0.4353 - val_n_outputs1_loss: 0.0521 Epoch 10/100 10/10 [==============================] - ETA: 0s - loss: 0.5030 - n_outputs0_loss: 0.4405 - n_outputs1_loss: 0.0625 Epoch 00010: val_loss did not improve from 0.48741 10/10 [==============================] - 9s 930ms/step - loss: 0.5030 - n_outputs0_loss: 0.4405 - n_outputs1_loss: 0.0625 - val_loss: 0.4936 - val_n_outputs0_loss: 0.4351 - val_n_outputs1_loss: 0.0585 Epoch 11/100 10/10 [==============================] - ETA: 0s - loss: 0.4974 - n_outputs0_loss: 0.4310 - n_outputs1_loss: 0.0664 Epoch 00011: val_loss improved from 0.48741 to 0.47748, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 11s 1s/step - loss: 0.4974 - n_outputs0_loss: 0.4310 - n_outputs1_loss: 0.0664 - val_loss: 0.4775 - val_n_outputs0_loss: 0.4238 - val_n_outputs1_loss: 0.0536 Epoch 12/100 10/10 [==============================] - ETA: 0s - loss: 0.4887 - n_outputs0_loss: 0.4208 - n_outputs1_loss: 0.0679 Epoch 00012: val_loss did not improve from 0.47748 10/10 [==============================] - 9s 925ms/step - loss: 0.4887 - n_outputs0_loss: 0.4208 - n_outputs1_loss: 0.0679 - val_loss: 0.4836 - val_n_outputs0_loss: 0.4148 - val_n_outputs1_loss: 0.0687 Epoch 13/100 10/10 [==============================] - ETA: 0s - loss: 0.4591 - n_outputs0_loss: 0.3927 - n_outputs1_loss: 0.0664 Epoch 00013: val_loss improved from 0.47748 to 0.40567, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.4591 - n_outputs0_loss: 0.3927 - n_outputs1_loss: 0.0664 - val_loss: 0.4057 - val_n_outputs0_loss: 0.3540 - val_n_outputs1_loss: 0.0516 Epoch 14/100 10/10 [==============================] - ETA: 0s - loss: 0.4323 - n_outputs0_loss: 0.3665 - n_outputs1_loss: 0.0658 Epoch 00014: val_loss improved from 0.40567 to 0.37099, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.4323 - n_outputs0_loss: 0.3665 - n_outputs1_loss: 0.0658 - val_loss: 0.3710 - val_n_outputs0_loss: 0.3153 - val_n_outputs1_loss: 0.0556 Epoch 15/100 10/10 [==============================] - ETA: 0s - loss: 0.3754 - n_outputs0_loss: 0.3063 - n_outputs1_loss: 0.0691 Epoch 00015: val_loss improved from 0.37099 to 0.33956, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.3754 - n_outputs0_loss: 0.3063 - n_outputs1_loss: 0.0691 - val_loss: 0.3396 - val_n_outputs0_loss: 0.2853 - val_n_outputs1_loss: 0.0542 Epoch 16/100 10/10 [==============================] - ETA: 0s - loss: 0.3314 - n_outputs0_loss: 0.2723 - n_outputs1_loss: 0.0591 Epoch 00016: val_loss improved from 0.33956 to 0.30289, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.3314 - n_outputs0_loss: 0.2723 - n_outputs1_loss: 0.0591 - val_loss: 0.3029 - val_n_outputs0_loss: 0.2524 - val_n_outputs1_loss: 0.0505 Epoch 17/100 10/10 [==============================] - ETA: 0s - loss: 0.3168 - n_outputs0_loss: 0.2591 - n_outputs1_loss: 0.0576 Epoch 00017: val_loss improved from 0.30289 to 0.28694, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.3168 - n_outputs0_loss: 0.2591 - n_outputs1_loss: 0.0576 - val_loss: 0.2869 - val_n_outputs0_loss: 0.2390 - val_n_outputs1_loss: 0.0479 Epoch 18/100 10/10 [==============================] - ETA: 0s - loss: 0.2990 - n_outputs0_loss: 0.2446 - n_outputs1_loss: 0.0544 Epoch 00018: val_loss improved from 0.28694 to 0.27270, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.2990 - n_outputs0_loss: 0.2446 - n_outputs1_loss: 0.0544 - val_loss: 0.2727 - val_n_outputs0_loss: 0.2257 - val_n_outputs1_loss: 0.0470 Epoch 19/100 10/10 [==============================] - ETA: 0s - loss: 0.2706 - n_outputs0_loss: 0.2185 - n_outputs1_loss: 0.0521 Epoch 00019: val_loss improved from 0.27270 to 0.25193, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.2706 - n_outputs0_loss: 0.2185 - n_outputs1_loss: 0.0521 - val_loss: 0.2519 - val_n_outputs0_loss: 0.2099 - val_n_outputs1_loss: 0.0421 Epoch 20/100 10/10 [==============================] - ETA: 0s - loss: 0.2602 - n_outputs0_loss: 0.2112 - n_outputs1_loss: 0.0490 Epoch 00020: val_loss improved from 0.25193 to 0.23899, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.2602 - n_outputs0_loss: 0.2112 - n_outputs1_loss: 0.0490 - val_loss: 0.2390 - val_n_outputs0_loss: 0.1974 - val_n_outputs1_loss: 0.0416 Epoch 21/100 10/10 [==============================] - ETA: 0s - loss: 0.2345 - n_outputs0_loss: 0.1866 - n_outputs1_loss: 0.0479 Epoch 00021: val_loss improved from 0.23899 to 0.23396, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.2345 - n_outputs0_loss: 0.1866 - n_outputs1_loss: 0.0479 - val_loss: 0.2340 - val_n_outputs0_loss: 0.1911 - val_n_outputs1_loss: 0.0428 Epoch 22/100 10/10 [==============================] - ETA: 0s - loss: 0.2229 - n_outputs0_loss: 0.1758 - n_outputs1_loss: 0.0471 Epoch 00022: val_loss improved from 0.23396 to 0.22651, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.2229 - n_outputs0_loss: 0.1758 - n_outputs1_loss: 0.0471 - val_loss: 0.2265 - val_n_outputs0_loss: 0.1858 - val_n_outputs1_loss: 0.0407 Epoch 23/100 10/10 [==============================] - ETA: 0s - loss: 0.2175 - n_outputs0_loss: 0.1730 - n_outputs1_loss: 0.0445 Epoch 00023: val_loss improved from 0.22651 to 0.22245, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.2175 - n_outputs0_loss: 0.1730 - n_outputs1_loss: 0.0445 - val_loss: 0.2225 - val_n_outputs0_loss: 0.1806 - val_n_outputs1_loss: 0.0419 Epoch 24/100 10/10 [==============================] - ETA: 0s - loss: 0.2084 - n_outputs0_loss: 0.1624 - n_outputs1_loss: 0.0460 Epoch 00024: val_loss improved from 0.22245 to 0.20674, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.2084 - n_outputs0_loss: 0.1624 - n_outputs1_loss: 0.0460 - val_loss: 0.2067 - val_n_outputs0_loss: 0.1694 - val_n_outputs1_loss: 0.0374 Epoch 25/100 10/10 [==============================] - ETA: 0s - loss: 0.1889 - n_outputs0_loss: 0.1457 - n_outputs1_loss: 0.0432 Epoch 00025: val_loss improved from 0.20674 to 0.20416, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.1889 - n_outputs0_loss: 0.1457 - n_outputs1_loss: 0.0432 - val_loss: 0.2042 - val_n_outputs0_loss: 0.1638 - val_n_outputs1_loss: 0.0403 Epoch 26/100 10/10 [==============================] - ETA: 0s - loss: 0.1882 - n_outputs0_loss: 0.1467 - n_outputs1_loss: 0.0414 Epoch 00026: val_loss improved from 0.20416 to 0.19422, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.1882 - n_outputs0_loss: 0.1467 - n_outputs1_loss: 0.0414 - val_loss: 0.1942 - val_n_outputs0_loss: 0.1557 - val_n_outputs1_loss: 0.0385 Epoch 27/100 10/10 [==============================] - ETA: 0s - loss: 0.1706 - n_outputs0_loss: 0.1328 - n_outputs1_loss: 0.0378 Epoch 00027: val_loss did not improve from 0.19422 10/10 [==============================] - 9s 930ms/step - loss: 0.1706 - n_outputs0_loss: 0.1328 - n_outputs1_loss: 0.0378 - val_loss: 0.2016 - val_n_outputs0_loss: 0.1615 - val_n_outputs1_loss: 0.0401 Epoch 28/100 10/10 [==============================] - ETA: 0s - loss: 0.1630 - n_outputs0_loss: 0.1248 - n_outputs1_loss: 0.0382 Epoch 00028: val_loss improved from 0.19422 to 0.18035, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.1630 - n_outputs0_loss: 0.1248 - n_outputs1_loss: 0.0382 - val_loss: 0.1803 - val_n_outputs0_loss: 0.1445 - val_n_outputs1_loss: 0.0358 Epoch 29/100 10/10 [==============================] - ETA: 0s - loss: 0.1601 - n_outputs0_loss: 0.1219 - n_outputs1_loss: 0.0382 Epoch 00029: val_loss improved from 0.18035 to 0.17528, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.1601 - n_outputs0_loss: 0.1219 - n_outputs1_loss: 0.0382 - val_loss: 0.1753 - val_n_outputs0_loss: 0.1410 - val_n_outputs1_loss: 0.0343 Epoch 30/100 10/10 [==============================] - ETA: 0s - loss: 0.1483 - n_outputs0_loss: 0.1117 - n_outputs1_loss: 0.0366 Epoch 00030: val_loss improved from 0.17528 to 0.17039, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.1483 - n_outputs0_loss: 0.1117 - n_outputs1_loss: 0.0366 - val_loss: 0.1704 - val_n_outputs0_loss: 0.1372 - val_n_outputs1_loss: 0.0332 Epoch 31/100 10/10 [==============================] - ETA: 0s - loss: 0.1481 - n_outputs0_loss: 0.1114 - n_outputs1_loss: 0.0368 Epoch 00031: val_loss did not improve from 0.17039 10/10 [==============================] - 9s 915ms/step - loss: 0.1481 - n_outputs0_loss: 0.1114 - n_outputs1_loss: 0.0368 - val_loss: 0.1783 - val_n_outputs0_loss: 0.1436 - val_n_outputs1_loss: 0.0347 Epoch 32/100 10/10 [==============================] - ETA: 0s - loss: 0.1470 - n_outputs0_loss: 0.1111 - n_outputs1_loss: 0.0358 Epoch 00032: val_loss improved from 0.17039 to 0.16278, saving model to ./models/msp-car-1.f5 10/10 [==============================] - 10s 1s/step - loss: 0.1470 - n_outputs0_loss: 0.1111 - n_outputs1_loss: 0.0358 - val_loss: 0.1628 - val_n_outputs0_loss: 0.1301 - val_n_outputs1_loss: 0.0327 Epoch 33/100 10/10 [==============================] - ETA: 0s - loss: 0.1368 - n_outputs0_loss: 0.1027 - n_outputs1_loss: 0.0341 Epoch 00033: val_loss did not improve from 0.16278 10/10 [==============================] - 9s 928ms/step - loss: 0.1368 - n_outputs0_loss: 0.1027 - n_outputs1_loss: 0.0341 - val_loss: 0.1666 - val_n_outputs0_loss: 0.1345 - val_n_outputs1_loss: 0.0320 Epoch 34/100 10/10 [==============================] - ETA: 0s - loss: 0.1305 - n_outputs0_loss: 0.0971 - n_outputs1_loss: 0.0334 Epoch 00034: val_loss did not improve from 0.16278 10/10 [==============================] - 9s 929ms/step - loss: 0.1305 - n_outputs0_loss: 0.0971 - n_outputs1_loss: 0.0334 - val_loss: 0.1728 - val_n_outputs0_loss: 0.1413 - val_n_outputs1_loss: 0.0315 Epoch 35/100 10/10 [==============================] - ETA: 0s - loss: 0.1353 - n_outputs0_loss: 0.1027 - n_outputs1_loss: 0.0326 Epoch 00035: val_loss did not improve from 0.16278 10/10 [==============================] - 9s 933ms/step - loss: 0.1353 - n_outputs0_loss: 0.1027 - n_outputs1_loss: 0.0326 - val_loss: 0.1706 - val_n_outputs0_loss: 0.1391 - val_n_outputs1_loss: 0.0315 Epoch 36/100 10/10 [==============================] - ETA: 0s - loss: 0.1319 - n_outputs0_loss: 0.0989 - n_outputs1_loss: 0.0331 Epoch 00036: val_loss did not improve from 0.16278 10/10 [==============================] - 9s 936ms/step - loss: 0.1319 - n_outputs0_loss: 0.0989 - n_outputs1_loss: 0.0331 - val_loss: 0.1729 - val_n_outputs0_loss: 0.1401 - val_n_outputs1_loss: 0.0328 Epoch 37/100 10/10 [==============================] - ETA: 0s - loss: 0.1290 - n_outputs0_loss: 0.0952 - n_outputs1_loss: 0.0338 Epoch 00037: val_loss did not improve from 0.16278 10/10 [==============================] - 9s 929ms/step - loss: 0.1290 - n_outputs0_loss: 0.0952 - n_outputs1_loss: 0.0338 - val_loss: 0.1709 - val_n_outputs0_loss: 0.1381 - val_n_outputs1_loss: 0.0327 WARNING: CPU random generator seem to be failing, disable hardware random number generation WARNING: RDRND generated: 0xffffffff 0xffffffff 0xffffffff 0xffffffff (donkey) arl@arl1: ```</p>"},{"location":"training-logs/msp-car-1/","title":"Minnesota STEM Partners Car 1 Training Log","text":"<pre><code>time donkey train --tub=./data/msp-car-1 --model=./models/msp-car-1.h5\n________             ______                   _________              \n___  __ \\_______________  /___________  __    __  ____/_____ ________\n__  / / /  __ \\_  __ \\_  //_/  _ \\_  / / /    _  /    _  __ `/_  ___/\n_  /_/ // /_/ /  / / /  ,&lt;  /  __/  /_/ /     / /___  / /_/ /_  /    \n/_____/ \\____//_/ /_//_/|_| \\___/_\\__, /      \\____/  \\__,_/ /_/     \n                                 /____/                              \n\nusing donkey v4.2.1 ...\nloading config file: ./config.py\nloading personal config over-rides from myconfig.py\n\"get_model_by_type\" model Type is: linear\nCreated KerasLinear\n2021-07-26 21:18:57.390998: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n2021-07-26 21:18:57.409838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-07-26 21:18:57.410285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \npciBusID: 0000:09:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\ncoreClock: 1.635GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\n2021-07-26 21:18:57.410424: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n2021-07-26 21:18:57.411314: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n2021-07-26 21:18:57.412358: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n2021-07-26 21:18:57.412506: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n2021-07-26 21:18:57.413323: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n2021-07-26 21:18:57.413712: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n2021-07-26 21:18:57.415437: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n2021-07-26 21:18:57.415619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-07-26 21:18:57.416133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-07-26 21:18:57.416523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n2021-07-26 21:18:57.416750: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n2021-07-26 21:18:57.420820: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3592950000 Hz\n2021-07-26 21:18:57.421125: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5629cbee7970 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n2021-07-26 21:18:57.421136: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n2021-07-26 21:18:57.421270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-07-26 21:18:57.421679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \npciBusID: 0000:09:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\ncoreClock: 1.635GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\n2021-07-26 21:18:57.421712: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n2021-07-26 21:18:57.421724: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n2021-07-26 21:18:57.421735: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n2021-07-26 21:18:57.421746: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n2021-07-26 21:18:57.421756: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n2021-07-26 21:18:57.421766: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n2021-07-26 21:18:57.421776: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n2021-07-26 21:18:57.421840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-07-26 21:18:57.422285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-07-26 21:18:57.422675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n2021-07-26 21:18:57.422700: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n2021-07-26 21:18:57.504507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n2021-07-26 21:18:57.504534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n2021-07-26 21:18:57.504541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n2021-07-26 21:18:57.504754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-07-26 21:18:57.505207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-07-26 21:18:57.505632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-07-26 21:18:57.506019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9892 MB memory) -&gt; physical GPU (device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:09:00.0, compute capability: 7.5)\n2021-07-26 21:18:57.507379: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5629cdd66f30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n2021-07-26 21:18:57.507389: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\nModel: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nimg_in (InputLayer)             [(None, 224, 224, 3) 0                                            \n__________________________________________________________________________________________________\nconv2d_1 (Conv2D)               (None, 110, 110, 24) 1824        img_in[0][0]                     \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, 110, 110, 24) 0           conv2d_1[0][0]                   \n__________________________________________________________________________________________________\nconv2d_2 (Conv2D)               (None, 53, 53, 32)   19232       dropout[0][0]                    \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 53, 53, 32)   0           conv2d_2[0][0]                   \n__________________________________________________________________________________________________\nconv2d_3 (Conv2D)               (None, 25, 25, 64)   51264       dropout_1[0][0]                  \n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, 25, 25, 64)   0           conv2d_3[0][0]                   \n__________________________________________________________________________________________________\nconv2d_4 (Conv2D)               (None, 23, 23, 64)   36928       dropout_2[0][0]                  \n__________________________________________________________________________________________________\ndropout_3 (Dropout)             (None, 23, 23, 64)   0           conv2d_4[0][0]                   \n__________________________________________________________________________________________________\nconv2d_5 (Conv2D)               (None, 21, 21, 64)   36928       dropout_3[0][0]                  \n__________________________________________________________________________________________________\ndropout_4 (Dropout)             (None, 21, 21, 64)   0           conv2d_5[0][0]                   \n__________________________________________________________________________________________________\nflattened (Flatten)             (None, 28224)        0           dropout_4[0][0]                  \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 100)          2822500     flattened[0][0]                  \n__________________________________________________________________________________________________\ndropout_5 (Dropout)             (None, 100)          0           dense_1[0][0]                    \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 50)           5050        dropout_5[0][0]                  \n__________________________________________________________________________________________________\ndropout_6 (Dropout)             (None, 50)           0           dense_2[0][0]                    \n__________________________________________________________________________________________________\nn_outputs0 (Dense)              (None, 1)            51          dropout_6[0][0]                  \n__________________________________________________________________________________________________\nn_outputs1 (Dense)              (None, 1)            51          dropout_6[0][0]                  \n==================================================================================================\nTotal params: 2,973,828\nTrainable params: 2,973,828\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\nUsing catalog /home/arl/mycar/data/msp-car-1/catalog_17.catalog\n\nRecords # Training 1265\nRecords # Validation 317\nEpoch 1/100\n2021-07-26 21:18:58.397797: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n2021-07-26 21:18:58.705078: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n2021-07-26 21:18:59.429125: W tensorflow/stream_executor/gpu/asm_compiler.cc:116] *** WARNING *** You are using ptxas 9.1.108, which is older than 9.2.88. ptxas 9.x before 9.2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n\nYou do not need to update to CUDA 9.2.88; cherry-picking the ptxas binary is sufficient.\n2021-07-26 21:18:59.481809: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 65280, output: ptxas fatal   : Value 'sm_75' is not defined for option 'gpu-name'\n\nRelying on driver to perform ptx compilation. \nModify $PATH to customize ptxas location.\nThis message will be only logged once.\n10/10 [==============================] - ETA: 0s - loss: 0.6674 - n_outputs0_loss: 0.5162 - n_outputs1_loss: 0.1512\nEpoch 00001: val_loss improved from inf to 0.60297, saving model to ./models/msp-car-1.h5\n10/10 [==============================] - 3s 288ms/step - loss: 0.6674 - n_outputs0_loss: 0.5162 - n_outputs1_loss: 0.1512 - val_loss: 0.6030 - val_n_outputs0_loss: 0.4514 - val_n_outputs1_loss: 0.1516\nEpoch 2/100\n10/10 [==============================] - ETA: 0s - loss: 0.6050 - n_outputs0_loss: 0.5074 - n_outputs1_loss: 0.0976\nEpoch 00002: val_loss improved from 0.60297 to 0.51595, saving model to ./models/msp-car-1.h5\n10/10 [==============================] - 1s 124ms/step - loss: 0.6050 - n_outputs0_loss: 0.5074 - n_outputs1_loss: 0.0976 - val_loss: 0.5160 - val_n_outputs0_loss: 0.4188 - val_n_outputs1_loss: 0.0972\nEpoch 3/100\n10/10 [==============================] - ETA: 0s - loss: 0.5707 - n_outputs0_loss: 0.4923 - n_outputs1_loss: 0.0784\nEpoch 00003: val_loss improved from 0.51595 to 0.50280, saving model to ./models/msp-car-1.h5\n10/10 [==============================] - 1s 110ms/step - loss: 0.5707 - n_outputs0_loss: 0.4923 - n_outputs1_loss: 0.0784 - val_loss: 0.5028 - val_n_outputs0_loss: 0.4224 - val_n_outputs1_loss: 0.0804\nEpoch 4/100\n10/10 [==============================] - ETA: 0s - loss: 0.5615 - n_outputs0_loss: 0.4917 - n_outputs1_loss: 0.0698\nEpoch 00004: val_loss improved from 0.50280 to 0.49159, saving model to ./models/msp-car-1.h5\n10/10 [==============================] - 1s 110ms/step - loss: 0.5615 - n_outputs0_loss: 0.4917 - n_outputs1_loss: 0.0698 - val_loss: 0.4916 - val_n_outputs0_loss: 0.4203 - val_n_outputs1_loss: 0.0713\nEpoch 5/100\n10/10 [==============================] - ETA: 0s - loss: 0.5541 - n_outputs0_loss: 0.4854 - n_outputs1_loss: 0.0687\nEpoch 00005: val_loss improved from 0.49159 to 0.48784, saving model to ./models/msp-car-1.h5\n10/10 [==============================] - 1s 104ms/step - loss: 0.5541 - n_outputs0_loss: 0.4854 - n_outputs1_loss: 0.0687 - val_loss: 0.4878 - val_n_outputs0_loss: 0.4107 - val_n_outputs1_loss: 0.0772\nEpoch 6/100\n10/10 [==============================] - ETA: 0s - loss: 0.5527 - n_outputs0_loss: 0.4827 - n_outputs1_loss: 0.0701\nEpoch 00006: val_loss improved from 0.48784 to 0.48521, saving model to ./models/msp-car-1.h5\n10/10 [==============================] - 1s 109ms/step - loss: 0.5527 - n_outputs0_loss: 0.4827 - n_outputs1_loss: 0.0701 - val_loss: 0.4852 - val_n_outputs0_loss: 0.4127 - val_n_outputs1_loss: 0.0725\nEpoch 7/100\n10/10 [==============================] - ETA: 0s - loss: 0.5405 - n_outputs0_loss: 0.4764 - n_outputs1_loss: 0.0641\nEpoch 00007: val_loss improved from 0.48521 to 0.48270, saving model to ./models/msp-car-1.h5\n10/10 [==============================] - 1s 110ms/step - loss: 0.5405 - n_outputs0_loss: 0.4764 - n_outputs1_loss: 0.0641 - val_loss: 0.4827 - val_n_outputs0_loss: 0.4097 - val_n_outputs1_loss: 0.0730\nEpoch 8/100\n10/10 [==============================] - ETA: 0s - loss: 0.5383 - n_outputs0_loss: 0.4724 - n_outputs1_loss: 0.0659\nEpoch 00008: val_loss improved from 0.48270 to 0.47415, saving model to ./models/msp-car-1.h5\n10/10 [==============================] - 1s 109ms/step - loss: 0.5383 - n_outputs0_loss: 0.4724 - n_outputs1_loss: 0.0659 - val_loss: 0.4741 - val_n_outputs0_loss: 0.4026 - val_n_outputs1_loss: 0.0715\nEpoch 9/100\n10/10 [==============================] - ETA: 0s - loss: 0.5288 - n_outputs0_loss: 0.4640 - n_outputs1_loss: 0.0648\nEpoch 00009: val_loss did not improve from 0.47415\n10/10 [==============================] - 1s 101ms/step - loss: 0.5288 - n_outputs0_loss: 0.4640 - n_outputs1_loss: 0.0648 - val_loss: 0.4780 - val_n_outputs0_loss: 0.4069 - val_n_outputs1_loss: 0.0711\nEpoch 10/100\n10/10 [==============================] - ETA: 0s - loss: 0.5344 - n_outputs0_loss: 0.4677 - n_outputs1_loss: 0.0667\nEpoch 00010: val_loss improved from 0.47415 to 0.45939, saving model to ./models/msp-car-1.h5\n10/10 [==============================] - 1s 111ms/step - loss: 0.5344 - n_outputs0_loss: 0.4677 - n_outputs1_loss: 0.0667 - val_loss: 0.4594 - val_n_outputs0_loss: 0.3903 - val_n_outputs1_loss: 0.0691\nEpoch 11/100\n10/10 [==============================] - ETA: 0s - loss: 0.5014 - n_outputs0_loss: 0.4349 - n_outputs1_loss: 0.0666\nEpoch 00011: val_loss improved from 0.45939 to 0.44304, saving model to ./models/msp-car-1.h5\n10/10 [==============================] - 1s 112ms/step - loss: 0.5014 - n_outputs0_loss: 0.4349 - n_outputs1_loss: 0.0666 - val_loss: 0.4430 - val_n_outputs0_loss: 0.3672 - val_n_outputs1_loss: 0.0758\nEpoch 12/100\n10/10 [==============================] - ETA: 0s - loss: 0.4585 - n_outputs0_loss: 0.3847 - n_outputs1_loss: 0.0738\nEpoch 00012: val_loss improved from 0.44304 to 0.36563, saving model to ./models/msp-car-1.h5\n10/10 [==============================] - 1s 109ms/step - loss: 0.4585 - n_outputs0_loss: 0.3847 - n_outputs1_loss: 0.0738 - val_loss: 0.3656 - val_n_outputs0_loss: 0.2934 - val_n_outputs1_loss: 0.0723\nEpoch 13/100\n10/10 [==============================] - ETA: 0s - loss: 0.3922 - n_outputs0_loss: 0.3257 - n_outputs1_loss: 0.0664\nEpoch 00013: val_loss improved from 0.36563 to 0.30773, saving model to ./models/msp-car-1.h5\n10/10 [==============================] - 1s 111ms/step - loss: 0.3922 - n_outputs0_loss: 0.3257 - n_outputs1_loss: 0.0664 - val_loss: 0.3077 - val_n_outputs0_loss: 0.2463 - val_n_outputs1_loss: 0.0614\nEpoch 14/100\n10/10 [==============================] - ETA: 0s - loss: 0.3662 - n_outputs0_loss: 0.3052 - n_outputs1_loss: 0.0610\nEpoch 00014: val_loss improved from 0.30773 to 0.27574, saving model to ./models/msp-car-1.h5\n10/10 [==============================] - 1s 107ms/step - loss: 0.3662 - n_outputs0_loss: 0.3052 - n_outputs1_loss: 0.0610 - val_loss: 0.2757 - val_n_outputs0_loss: 0.2294 - val_n_outputs1_loss: 0.0463\nEpoch 15/100\n10/10 [==============================] - ETA: 0s - loss: 0.3233 - n_outputs0_loss: 0.2626 - n_outputs1_loss: 0.0607\nEpoch 00015: val_loss improved from 0.27574 to 0.24205, saving model to ./models/msp-car-1.h5\n10/10 [==============================] - 1s 110ms/step - loss: 0.3233 - n_outputs0_loss: 0.2626 - n_outputs1_loss: 0.0607 - val_loss: 0.2421 - val_n_outputs0_loss: 0.1966 - val_n_outputs1_loss: 0.0454\nEpoch 16/100\n10/10 [==============================] - ETA: 0s - loss: 0.3078 - n_outputs0_loss: 0.2500 - n_outputs1_loss: 0.0577\nEpoch 00016: val_loss did not improve from 0.24205\n10/10 [==============================] - 1s 100ms/step - loss: 0.3078 - n_outputs0_loss: 0.2500 - n_outputs1_loss: 0.0577 - val_loss: 0.2473 - val_n_outputs0_loss: 0.2023 - val_n_outputs1_loss: 0.0450\nEpoch 17/100\n10/10 [==============================] - ETA: 0s - loss: 0.2959 - n_outputs0_loss: 0.2404 - n_outputs1_loss: 0.0555\nEpoch 00017: val_loss improved from 0.24205 to 0.22809, saving model to ./models/msp-car-1.h5\n10/10 [==============================] - 1s 113ms/step - loss: 0.2959 - n_outputs0_loss: 0.2404 - n_outputs1_loss: 0.0555 - val_loss: 0.2281 - val_n_outputs0_loss: 0.1842 - val_n_outputs1_loss: 0.0438\nEpoch 18/100\n10/10 [==============================] - ETA: 0s - loss: 0.2820 - n_outputs0_loss: 0.2280 - n_outputs1_loss: 0.0540\nEpoch 00018: val_loss improved from 0.22809 to 0.21671, saving model to ./models/msp-car-1.h5\n10/10 [==============================] - 1s 107ms/step - loss: 0.2820 - n_outputs0_loss: 0.2280 - n_outputs1_loss: 0.0540 - val_loss: 0.2167 - val_n_outputs0_loss: 0.1768 - val_n_outputs1_loss: 0.0400\nEpoch 19/100\n10/10 [==============================] - ETA: 0s - loss: 0.2568 - n_outputs0_loss: 0.2044 - n_outputs1_loss: 0.0524\nEpoch 00019: val_loss did not improve from 0.21671\n10/10 [==============================] - 1s 99ms/step - loss: 0.2568 - n_outputs0_loss: 0.2044 - n_outputs1_loss: 0.0524 - val_loss: 0.2190 - val_n_outputs0_loss: 0.1788 - val_n_outputs1_loss: 0.0402\nEpoch 20/100\n10/10 [==============================] - ETA: 0s - loss: 0.2621 - n_outputs0_loss: 0.2123 - n_outputs1_loss: 0.0499\nEpoch 00020: val_loss improved from 0.21671 to 0.21046, saving model to ./models/msp-car-1.h5\n10/10 [==============================] - 1s 113ms/step - loss: 0.2621 - n_outputs0_loss: 0.2123 - n_outputs1_loss: 0.0499 - val_loss: 0.2105 - val_n_outputs0_loss: 0.1718 - val_n_outputs1_loss: 0.0386\nEpoch 21/100\n10/10 [==============================] - ETA: 0s - loss: 0.2521 - n_outputs0_loss: 0.2052 - n_outputs1_loss: 0.0469\nEpoch 00021: val_loss improved from 0.21046 to 0.20605, saving model to ./models/msp-car-1.h5\n10/10 [==============================] - 1s 111ms/step - loss: 0.2521 - n_outputs0_loss: 0.2052 - n_outputs1_loss: 0.0469 - val_loss: 0.2060 - val_n_outputs0_loss: 0.1675 - val_n_outputs1_loss: 0.0385\nEpoch 22/100\n10/10 [==============================] - ETA: 0s - loss: 0.2261 - n_outputs0_loss: 0.1781 - n_outputs1_loss: 0.0480\nEpoch 00022: val_loss improved from 0.20605 to 0.20553, saving model to ./models/msp-car-1.h5\n10/10 [==============================] - 1s 106ms/step - loss: 0.2261 - n_outputs0_loss: 0.1781 - n_outputs1_loss: 0.0480 - val_loss: 0.2055 - val_n_outputs0_loss: 0.1711 - val_n_outputs1_loss: 0.0344\nEpoch 23/100\n10/10 [==============================] - ETA: 0s - loss: 0.2222 - n_outputs0_loss: 0.1794 - n_outputs1_loss: 0.0429\nEpoch 00023: val_loss improved from 0.20553 to 0.20273, saving model to ./models/msp-car-1.h5\n10/10 [==============================] - 1s 109ms/step - loss: 0.2222 - n_outputs0_loss: 0.1794 - n_outputs1_loss: 0.0429 - val_loss: 0.2027 - val_n_outputs0_loss: 0.1697 - val_n_outputs1_loss: 0.0331\nEpoch 24/100\n10/10 [==============================] - ETA: 0s - loss: 0.2126 - n_outputs0_loss: 0.1698 - n_outputs1_loss: 0.0428\nEpoch 00024: val_loss improved from 0.20273 to 0.19049, saving model to ./models/msp-car-1.h5\n10/10 [==============================] - 1s 105ms/step - loss: 0.2126 - n_outputs0_loss: 0.1698 - n_outputs1_loss: 0.0428 - val_loss: 0.1905 - val_n_outputs0_loss: 0.1562 - val_n_outputs1_loss: 0.0343\nEpoch 25/100\n10/10 [==============================] - ETA: 0s - loss: 0.2062 - n_outputs0_loss: 0.1658 - n_outputs1_loss: 0.0404\nEpoch 00025: val_loss improved from 0.19049 to 0.18404, saving model to ./models/msp-car-1.h5\n10/10 [==============================] - 1s 109ms/step - loss: 0.2062 - n_outputs0_loss: 0.1658 - n_outputs1_loss: 0.0404 - val_loss: 0.1840 - val_n_outputs0_loss: 0.1488 - val_n_outputs1_loss: 0.0352\nEpoch 26/100\n10/10 [==============================] - ETA: 0s - loss: 0.1928 - n_outputs0_loss: 0.1555 - n_outputs1_loss: 0.0372\nEpoch 00026: val_loss did not improve from 0.18404\n10/10 [==============================] - 1s 102ms/step - loss: 0.1928 - n_outputs0_loss: 0.1555 - n_outputs1_loss: 0.0372 - val_loss: 0.1907 - val_n_outputs0_loss: 0.1563 - val_n_outputs1_loss: 0.0344\nEpoch 27/100\n10/10 [==============================] - ETA: 0s - loss: 0.1834 - n_outputs0_loss: 0.1428 - n_outputs1_loss: 0.0406\nEpoch 00027: val_loss did not improve from 0.18404\n10/10 [==============================] - 1s 103ms/step - loss: 0.1834 - n_outputs0_loss: 0.1428 - n_outputs1_loss: 0.0406 - val_loss: 0.1922 - val_n_outputs0_loss: 0.1527 - val_n_outputs1_loss: 0.0396\nEpoch 28/100\n10/10 [==============================] - ETA: 0s - loss: 0.1668 - n_outputs0_loss: 0.1282 - n_outputs1_loss: 0.0386\nEpoch 00028: val_loss improved from 0.18404 to 0.17462, saving model to ./models/msp-car-1.h5\n10/10 [==============================] - 1s 113ms/step - loss: 0.1668 - n_outputs0_loss: 0.1282 - n_outputs1_loss: 0.0386 - val_loss: 0.1746 - val_n_outputs0_loss: 0.1436 - val_n_outputs1_loss: 0.0311\nEpoch 29/100\n10/10 [==============================] - ETA: 0s - loss: 0.1654 - n_outputs0_loss: 0.1282 - n_outputs1_loss: 0.0372\nEpoch 00029: val_loss improved from 0.17462 to 0.17365, saving model to ./models/msp-car-1.h5\n10/10 [==============================] - 1s 107ms/step - loss: 0.1654 - n_outputs0_loss: 0.1282 - n_outputs1_loss: 0.0372 - val_loss: 0.1736 - val_n_outputs0_loss: 0.1432 - val_n_outputs1_loss: 0.0305\nEpoch 30/100\n10/10 [==============================] - ETA: 0s - loss: 0.1615 - n_outputs0_loss: 0.1250 - n_outputs1_loss: 0.0364\nEpoch 00030: val_loss did not improve from 0.17365\n10/10 [==============================] - 1s 96ms/step - loss: 0.1615 - n_outputs0_loss: 0.1250 - n_outputs1_loss: 0.0364 - val_loss: 0.1799 - val_n_outputs0_loss: 0.1493 - val_n_outputs1_loss: 0.0306\nEpoch 31/100\n10/10 [==============================] - ETA: 0s - loss: 0.1495 - n_outputs0_loss: 0.1162 - n_outputs1_loss: 0.0332\nEpoch 00031: val_loss improved from 0.17365 to 0.17255, saving model to ./models/msp-car-1.h5\n10/10 [==============================] - 1s 112ms/step - loss: 0.1495 - n_outputs0_loss: 0.1162 - n_outputs1_loss: 0.0332 - val_loss: 0.1726 - val_n_outputs0_loss: 0.1383 - val_n_outputs1_loss: 0.0342\nEpoch 32/100\n10/10 [==============================] - ETA: 0s - loss: 0.1453 - n_outputs0_loss: 0.1121 - n_outputs1_loss: 0.0333\nEpoch 00032: val_loss did not improve from 0.17255\n10/10 [==============================] - 1s 104ms/step - loss: 0.1453 - n_outputs0_loss: 0.1121 - n_outputs1_loss: 0.0333 - val_loss: 0.1764 - val_n_outputs0_loss: 0.1456 - val_n_outputs1_loss: 0.0308\nEpoch 33/100\n10/10 [==============================] - ETA: 0s - loss: 0.1346 - n_outputs0_loss: 0.1043 - n_outputs1_loss: 0.0303\nEpoch 00033: val_loss improved from 0.17255 to 0.17092, saving model to ./models/msp-car-1.h5\n10/10 [==============================] - 1s 115ms/step - loss: 0.1346 - n_outputs0_loss: 0.1043 - n_outputs1_loss: 0.0303 - val_loss: 0.1709 - val_n_outputs0_loss: 0.1395 - val_n_outputs1_loss: 0.0315\nEpoch 34/100\n10/10 [==============================] - ETA: 0s - loss: 0.1293 - n_outputs0_loss: 0.0991 - n_outputs1_loss: 0.0302\nEpoch 00034: val_loss improved from 0.17092 to 0.16704, saving model to ./models/msp-car-1.h5\n10/10 [==============================] - 1s 109ms/step - loss: 0.1293 - n_outputs0_loss: 0.0991 - n_outputs1_loss: 0.0302 - val_loss: 0.1670 - val_n_outputs0_loss: 0.1342 - val_n_outputs1_loss: 0.0329\nEpoch 35/100\n10/10 [==============================] - ETA: 0s - loss: 0.1196 - n_outputs0_loss: 0.0890 - n_outputs1_loss: 0.0306\nEpoch 00035: val_loss improved from 0.16704 to 0.15917, saving model to ./models/msp-car-1.h5\n10/10 [==============================] - 1s 110ms/step - loss: 0.1196 - n_outputs0_loss: 0.0890 - n_outputs1_loss: 0.0306 - val_loss: 0.1592 - val_n_outputs0_loss: 0.1280 - val_n_outputs1_loss: 0.0311\nEpoch 36/100\n10/10 [==============================] - ETA: 0s - loss: 0.1086 - n_outputs0_loss: 0.0805 - n_outputs1_loss: 0.0281\nEpoch 00036: val_loss improved from 0.15917 to 0.15774, saving model to ./models/msp-car-1.h5\n10/10 [==============================] - 1s 114ms/step - loss: 0.1086 - n_outputs0_loss: 0.0805 - n_outputs1_loss: 0.0281 - val_loss: 0.1577 - val_n_outputs0_loss: 0.1264 - val_n_outputs1_loss: 0.0313\nEpoch 37/100\n10/10 [==============================] - ETA: 0s - loss: 0.1032 - n_outputs0_loss: 0.0753 - n_outputs1_loss: 0.0279\nEpoch 00037: val_loss did not improve from 0.15774\n10/10 [==============================] - 1s 99ms/step - loss: 0.1032 - n_outputs0_loss: 0.0753 - n_outputs1_loss: 0.0279 - val_loss: 0.1598 - val_n_outputs0_loss: 0.1281 - val_n_outputs1_loss: 0.0317\nEpoch 38/100\n10/10 [==============================] - ETA: 0s - loss: 0.1050 - n_outputs0_loss: 0.0783 - n_outputs1_loss: 0.0266\nEpoch 00038: val_loss did not improve from 0.15774\n10/10 [==============================] - 1s 105ms/step - loss: 0.1050 - n_outputs0_loss: 0.0783 - n_outputs1_loss: 0.0266 - val_loss: 0.1586 - val_n_outputs0_loss: 0.1269 - val_n_outputs1_loss: 0.0317\nEpoch 39/100\n10/10 [==============================] - ETA: 0s - loss: 0.0983 - n_outputs0_loss: 0.0722 - n_outputs1_loss: 0.0261\nEpoch 00039: val_loss improved from 0.15774 to 0.15441, saving model to ./models/msp-car-1.h5\n10/10 [==============================] - 1s 111ms/step - loss: 0.0983 - n_outputs0_loss: 0.0722 - n_outputs1_loss: 0.0261 - val_loss: 0.1544 - val_n_outputs0_loss: 0.1243 - val_n_outputs1_loss: 0.0301\nEpoch 40/100\n10/10 [==============================] - ETA: 0s - loss: 0.0967 - n_outputs0_loss: 0.0703 - n_outputs1_loss: 0.0265\nEpoch 00040: val_loss did not improve from 0.15441\n10/10 [==============================] - 1s 103ms/step - loss: 0.0967 - n_outputs0_loss: 0.0703 - n_outputs1_loss: 0.0265 - val_loss: 0.1588 - val_n_outputs0_loss: 0.1275 - val_n_outputs1_loss: 0.0313\nEpoch 41/100\n10/10 [==============================] - ETA: 0s - loss: 0.0989 - n_outputs0_loss: 0.0736 - n_outputs1_loss: 0.0253\nEpoch 00041: val_loss did not improve from 0.15441\n10/10 [==============================] - 1s 104ms/step - loss: 0.0989 - n_outputs0_loss: 0.0736 - n_outputs1_loss: 0.0253 - val_loss: 0.1580 - val_n_outputs0_loss: 0.1271 - val_n_outputs1_loss: 0.0308\nEpoch 42/100\n10/10 [==============================] - ETA: 0s - loss: 0.1010 - n_outputs0_loss: 0.0758 - n_outputs1_loss: 0.0253\nEpoch 00042: val_loss did not improve from 0.15441\n10/10 [==============================] - 1s 107ms/step - loss: 0.1010 - n_outputs0_loss: 0.0758 - n_outputs1_loss: 0.0253 - val_loss: 0.1614 - val_n_outputs0_loss: 0.1315 - val_n_outputs1_loss: 0.0299\nEpoch 43/100\n10/10 [==============================] - ETA: 0s - loss: 0.0923 - n_outputs0_loss: 0.0680 - n_outputs1_loss: 0.0243\nEpoch 00043: val_loss did not improve from 0.15441\n10/10 [==============================] - 1s 101ms/step - loss: 0.0923 - n_outputs0_loss: 0.0680 - n_outputs1_loss: 0.0243 - val_loss: 0.1587 - val_n_outputs0_loss: 0.1298 - val_n_outputs1_loss: 0.0288\nEpoch 44/100\n10/10 [==============================] - ETA: 0s - loss: 0.0870 - n_outputs0_loss: 0.0629 - n_outputs1_loss: 0.0242\nEpoch 00044: val_loss did not improve from 0.15441\n10/10 [==============================] - 1s 105ms/step - loss: 0.0870 - n_outputs0_loss: 0.0629 - n_outputs1_loss: 0.0242 - val_loss: 0.1601 - val_n_outputs0_loss: 0.1304 - val_n_outputs1_loss: 0.0296\nWARNING: CPU random generator seem to be failing, disable hardware random number generation\nWARNING: RDRND generated: 0xffffffff 0xffffffff 0xffffffff 0xffffffff\n\nreal    1m10.563s\nuser    1m11.485s\nsys 0m39.110s\n</code></pre>"},{"location":"training-logs/msp-car-2/","title":"Training run for Minneapolis STEM Partners","text":"<p>Car #2 had 15045 images</p> <p>wc -l ~/mycar/data/msp-car-2/*.catalog 15045</p>"},{"location":"training-logs/msp-car-2/#ls-1-mycardatamsp-car-2images-wc-l","title":"ls -1 ~/mycar/data/msp-car-2/images | wc -l","text":"<p>15045</p> <pre><code>(donkey) arl@arl1:~/mycar$ donkey train --tub=./data/msp-car-2 --model=./models/msp-car-2.h5\n________             ______                   _________              \n___  __ \\_______________  /___________  __    __  ____/_____ ________\n__  / / /  __ \\_  __ \\_  //_/  _ \\_  / / /    _  /    _  __ `/_  ___/\n_  /_/ // /_/ /  / / /  ,&lt;  /  __/  /_/ /     / /___  / /_/ /_  /    \n/_____/ \\____//_/ /_//_/|_| \\___/_\\__, /      \\____/  \\__,_/ /_/     /____/                              \n\nusing donkey v4.2.1 ...\nloading config file: ./config.py\nloading personal config over-rides from myconfig.py\n\"get_model_by_type\" model Type is: linear\nCreated KerasLinear\n2021-07-26 20:22:54.320076: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n2021-07-26 20:22:54.338339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-07-26 20:22:54.338783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \npciBusID: 0000:09:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\ncoreClock: 1.635GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\n2021-07-26 20:22:54.338925: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n2021-07-26 20:22:54.339823: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n2021-07-26 20:22:54.340834: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n2021-07-26 20:22:54.340981: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n2021-07-26 20:22:54.341775: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n2021-07-26 20:22:54.342170: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n2021-07-26 20:22:54.343898: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n2021-07-26 20:22:54.344043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-07-26 20:22:54.344546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-07-26 20:22:54.344933: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n2021-07-26 20:22:54.345163: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n2021-07-26 20:22:54.349277: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3592950000 Hz\n2021-07-26 20:22:54.349572: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5575e341a9f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n2021-07-26 20:22:54.349585: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n2021-07-26 20:22:54.349717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-07-26 20:22:54.350124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \npciBusID: 0000:09:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\ncoreClock: 1.635GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\n2021-07-26 20:22:54.350160: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n2021-07-26 20:22:54.350171: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n2021-07-26 20:22:54.350180: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n2021-07-26 20:22:54.350191: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n2021-07-26 20:22:54.350200: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n2021-07-26 20:22:54.350210: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n2021-07-26 20:22:54.350220: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n2021-07-26 20:22:54.350282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-07-26 20:22:54.350723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-07-26 20:22:54.351106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n2021-07-26 20:22:54.351127: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n2021-07-26 20:22:54.423106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n2021-07-26 20:22:54.423133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 2021-07-26 20:22:54.423138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N 2021-07-26 20:22:54.423354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-07-26 20:22:54.423819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-07-26 20:22:54.424248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-07-26 20:22:54.424632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9890 MB memory) -&gt; physical GPU (device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:09:00.0, compute capability: 7.5)\n2021-07-26 20:22:54.425999: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5575e52b18b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n2021-07-26 20:22:54.426009: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\nModel: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nimg_in (InputLayer)             [(None, 224, 224, 3) 0                                            \n__________________________________________________________________________________________________\nconv2d_1 (Conv2D)               (None, 110, 110, 24) 1824        img_in[0][0]                     \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, 110, 110, 24) 0           conv2d_1[0][0]                   \n__________________________________________________________________________________________________\nconv2d_2 (Conv2D)               (None, 53, 53, 32)   19232       dropout[0][0]                    \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 53, 53, 32)   0           conv2d_2[0][0]                   \n__________________________________________________________________________________________________\nconv2d_3 (Conv2D)               (None, 25, 25, 64)   51264       dropout_1[0][0]                  \n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, 25, 25, 64)   0           conv2d_3[0][0]                   \n__________________________________________________________________________________________________\nconv2d_4 (Conv2D)               (None, 23, 23, 64)   36928       dropout_2[0][0]                  \n__________________________________________________________________________________________________\ndropout_3 (Dropout)             (None, 23, 23, 64)   0           conv2d_4[0][0]                   \n__________________________________________________________________________________________________\nconv2d_5 (Conv2D)               (None, 21, 21, 64)   36928       dropout_3[0][0]                  \n__________________________________________________________________________________________________\ndropout_4 (Dropout)             (None, 21, 21, 64)   0           conv2d_5[0][0]                   \n__________________________________________________________________________________________________\nflattened (Flatten)             (None, 28224)        0           dropout_4[0][0]                  \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 100)          2822500     flattened[0][0]                  \n__________________________________________________________________________________________________\ndropout_5 (Dropout)             (None, 100)          0           dense_1[0][0]                    \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 50)           5050        dropout_5[0][0]                  \n__________________________________________________________________________________________________\ndropout_6 (Dropout)             (None, 50)           0           dense_2[0][0]                    \n__________________________________________________________________________________________________\nn_outputs0 (Dense)              (None, 1)            51          dropout_6[0][0]                  \n__________________________________________________________________________________________________\nn_outputs1 (Dense)              (None, 1)            51          dropout_6[0][0]                  ==================================================================================================\nTotal params: 2,973,828\nTrainable params: 2,973,828\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\nUsing catalog /home/arl/mycar/data/msp-car-2/catalog_22.catalog\n\nRecords # Training 11696\nRecords # Validation 2924\nEpoch 1/100\n2021-07-26 20:22:55.471623: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n2021-07-26 20:22:55.802565: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n2021-07-26 20:22:56.515413: W tensorflow/stream_executor/gpu/asm_compiler.cc:116] *** WARNING *** You are using ptxas 9.1.108, which is older than 9.2.88. ptxas 9.x before 9.2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n\nYou do not need to update to CUDA 9.2.88; cherry-picking the ptxas binary is sufficient.\n2021-07-26 20:22:56.559204: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 65280, output: ptxas fatal   : Value 'sm_75' is not defined for option 'gpu-name'\n\nRelying on driver to perform ptx compilation. \nModify $PATH to customize ptxas location.\nThis message will be only logged once.\n92/92 [==============================] - ETA: 0s - loss: 0.6304 - n_outputs0_loss: 0.3238 - n_outputs1_loss: 0.3066 \nEpoch 00001: val_loss improved from inf to 0.59133, saving model to ./models/msp-car-2.h5\n92/92 [==============================] - 29s 310ms/step - loss: 0.6304 - n_outputs0_loss: 0.3238 - n_outputs1_loss: 0.3066 - val_loss: 0.5913 - val_n_outputs0_loss: 0.3121 - val_n_outputs1_loss: 0.2793\nEpoch 2/100\n92/92 [==============================] - ETA: 0s - loss: 0.5150 - n_outputs0_loss: 0.2730 - n_outputs1_loss: 0.2419\nEpoch 00002: val_loss improved from 0.59133 to 0.39368, saving model to ./models/msp-car-2.h5\n92/92 [==============================] - 11s 117ms/step - loss: 0.5150 - n_outputs0_loss: 0.2730 - n_outputs1_loss: 0.2419 - val_loss: 0.3937 - val_n_outputs0_loss: 0.2108 - val_n_outputs1_loss: 0.1828\nEpoch 3/100\n92/92 [==============================] - ETA: 0s - loss: 0.3885 - n_outputs0_loss: 0.2088 - n_outputs1_loss: 0.1797\nEpoch 00003: val_loss improved from 0.39368 to 0.34087, saving model to ./models/msp-car-2.h5\n92/92 [==============================] - 11s 117ms/step - loss: 0.3885 - n_outputs0_loss: 0.2088 - n_outputs1_loss: 0.1797 - val_loss: 0.3409 - val_n_outputs0_loss: 0.1923 - val_n_outputs1_loss: 0.1486\nEpoch 4/100\n92/92 [==============================] - ETA: 0s - loss: 0.3449 - n_outputs0_loss: 0.1870 - n_outputs1_loss: 0.1579\nEpoch 00004: val_loss improved from 0.34087 to 0.30588, saving model to ./models/msp-car-2.h5\n92/92 [==============================] - 11s 119ms/step - loss: 0.3449 - n_outputs0_loss: 0.1870 - n_outputs1_loss: 0.1579 - val_loss: 0.3059 - val_n_outputs0_loss: 0.1771 - val_n_outputs1_loss: 0.1288\nEpoch 5/100\n92/92 [==============================] - ETA: 0s - loss: 0.3161 - n_outputs0_loss: 0.1763 - n_outputs1_loss: 0.1397\nEpoch 00005: val_loss improved from 0.30588 to 0.28650, saving model to ./models/msp-car-2.h5\n92/92 [==============================] - 10s 113ms/step - loss: 0.3161 - n_outputs0_loss: 0.1763 - n_outputs1_loss: 0.1397 - val_loss: 0.2865 - val_n_outputs0_loss: 0.1722 - val_n_outputs1_loss: 0.1143\nEpoch 6/100\n92/92 [==============================] - ETA: 0s - loss: 0.2876 - n_outputs0_loss: 0.1633 - n_outputs1_loss: 0.1243\nEpoch 00006: val_loss improved from 0.28650 to 0.26754, saving model to ./models/msp-car-2.h5\n92/92 [==============================] - 10s 110ms/step - loss: 0.2876 - n_outputs0_loss: 0.1633 - n_outputs1_loss: 0.1243 - val_loss: 0.2675 - val_n_outputs0_loss: 0.1623 - val_n_outputs1_loss: 0.1053\nEpoch 7/100\n92/92 [==============================] - ETA: 0s - loss: 0.2612 - n_outputs0_loss: 0.1508 - n_outputs1_loss: 0.1103\nEpoch 00007: val_loss improved from 0.26754 to 0.25034, saving model to ./models/msp-car-2.h5\n92/92 [==============================] - 10s 111ms/step - loss: 0.2612 - n_outputs0_loss: 0.1508 - n_outputs1_loss: 0.1103 - val_loss: 0.2503 - val_n_outputs0_loss: 0.1551 - val_n_outputs1_loss: 0.0952\nEpoch 8/100\n92/92 [==============================] - ETA: 0s - loss: 0.2476 - n_outputs0_loss: 0.1435 - n_outputs1_loss: 0.1041\nEpoch 00008: val_loss improved from 0.25034 to 0.24291, saving model to ./models/msp-car-2.h5\n92/92 [==============================] - 10s 109ms/step - loss: 0.2476 - n_outputs0_loss: 0.1435 - n_outputs1_loss: 0.1041 - val_loss: 0.2429 - val_n_outputs0_loss: 0.1524 - val_n_outputs1_loss: 0.0905\nEpoch 9/100\n92/92 [==============================] - ETA: 0s - loss: 0.2283 - n_outputs0_loss: 0.1323 - n_outputs1_loss: 0.0960\nEpoch 00009: val_loss improved from 0.24291 to 0.22718, saving model to ./models/msp-car-2.h5\n92/92 [==============================] - 10s 110ms/step - loss: 0.2283 - n_outputs0_loss: 0.1323 - n_outputs1_loss: 0.0960 - val_loss: 0.2272 - val_n_outputs0_loss: 0.1450 - val_n_outputs1_loss: 0.0821\nEpoch 10/100\n92/92 [==============================] - ETA: 0s - loss: 0.2183 - n_outputs0_loss: 0.1267 - n_outputs1_loss: 0.0916\nEpoch 00010: val_loss did not improve from 0.22718\n92/92 [==============================] - 10s 109ms/step - loss: 0.2183 - n_outputs0_loss: 0.1267 - n_outputs1_loss: 0.0916 - val_loss: 0.2305 - val_n_outputs0_loss: 0.1471 - val_n_outputs1_loss: 0.0834\nEpoch 11/100\n92/92 [==============================] - ETA: 0s - loss: 0.2022 - n_outputs0_loss: 0.1187 - n_outputs1_loss: 0.0835\nEpoch 00011: val_loss improved from 0.22718 to 0.21581, saving model to ./models/msp-car-2.h5\n92/92 [==============================] - 10s 110ms/step - loss: 0.2022 - n_outputs0_loss: 0.1187 - n_outputs1_loss: 0.0835 - val_loss: 0.2158 - val_n_outputs0_loss: 0.1375 - val_n_outputs1_loss: 0.0783\nEpoch 12/100\n92/92 [==============================] - ETA: 0s - loss: 0.1921 - n_outputs0_loss: 0.1085 - n_outputs1_loss: 0.0836\nEpoch 00012: val_loss did not improve from 0.21581\n92/92 [==============================] - 10s 110ms/step - loss: 0.1921 - n_outputs0_loss: 0.1085 - n_outputs1_loss: 0.0836 - val_loss: 0.2185 - val_n_outputs0_loss: 0.1382 - val_n_outputs1_loss: 0.0802\nEpoch 13/100\n92/92 [==============================] - ETA: 0s - loss: 0.1826 - n_outputs0_loss: 0.1056 - n_outputs1_loss: 0.0770\nEpoch 00013: val_loss did not improve from 0.21581\n92/92 [==============================] - 10s 110ms/step - loss: 0.1826 - n_outputs0_loss: 0.1056 - n_outputs1_loss: 0.0770 - val_loss: 0.2198 - val_n_outputs0_loss: 0.1394 - val_n_outputs1_loss: 0.0804\nEpoch 14/100\n92/92 [==============================] - ETA: 0s - loss: 0.1771 - n_outputs0_loss: 0.1009 - n_outputs1_loss: 0.0762\nEpoch 00014: val_loss did not improve from 0.21581\n92/92 [==============================] - 10s 111ms/step - loss: 0.1771 - n_outputs0_loss: 0.1009 - n_outputs1_loss: 0.0762 - val_loss: 0.2167 - val_n_outputs0_loss: 0.1389 - val_n_outputs1_loss: 0.0778\nEpoch 15/100\n92/92 [==============================] - ETA: 0s - loss: 0.1676 - n_outputs0_loss: 0.0959 - n_outputs1_loss: 0.0718\nEpoch 00015: val_loss improved from 0.21581 to 0.20899, saving model to ./models/msp-car-2.h5\n92/92 [==============================] - 10s 112ms/step - loss: 0.1676 - n_outputs0_loss: 0.0959 - n_outputs1_loss: 0.0718 - val_loss: 0.2090 - val_n_outputs0_loss: 0.1345 - val_n_outputs1_loss: 0.0745\nEpoch 16/100\n92/92 [==============================] - ETA: 0s - loss: 0.1608 - n_outputs0_loss: 0.0910 - n_outputs1_loss: 0.0698\nEpoch 00016: val_loss did not improve from 0.20899\n92/92 [==============================] - 10s 110ms/step - loss: 0.1608 - n_outputs0_loss: 0.0910 - n_outputs1_loss: 0.0698 - val_loss: 0.2097 - val_n_outputs0_loss: 0.1348 - val_n_outputs1_loss: 0.0748\nEpoch 17/100\n92/92 [==============================] - ETA: 0s - loss: 0.1534 - n_outputs0_loss: 0.0870 - n_outputs1_loss: 0.0664\nEpoch 00017: val_loss improved from 0.20899 to 0.20324, saving model to ./models/msp-car-2.h5\n92/92 [==============================] - 10s 112ms/step - loss: 0.1534 - n_outputs0_loss: 0.0870 - n_outputs1_loss: 0.0664 - val_loss: 0.2032 - val_n_outputs0_loss: 0.1329 - val_n_outputs1_loss: 0.0703\nEpoch 18/100\n92/92 [==============================] - ETA: 0s - loss: 0.1490 - n_outputs0_loss: 0.0846 - n_outputs1_loss: 0.0644\nEpoch 00018: val_loss improved from 0.20324 to 0.19965, saving model to ./models/msp-car-2.h5\n92/92 [==============================] - 10s 111ms/step - loss: 0.1490 - n_outputs0_loss: 0.0846 - n_outputs1_loss: 0.0644 - val_loss: 0.1997 - val_n_outputs0_loss: 0.1309 - val_n_outputs1_loss: 0.0688\nEpoch 19/100\n92/92 [==============================] - ETA: 0s - loss: 0.1452 - n_outputs0_loss: 0.0828 - n_outputs1_loss: 0.0624\nEpoch 00019: val_loss improved from 0.19965 to 0.19877, saving model to ./models/msp-car-2.h5\n92/92 [==============================] - 10s 111ms/step - loss: 0.1452 - n_outputs0_loss: 0.0828 - n_outputs1_loss: 0.0624 - val_loss: 0.1988 - val_n_outputs0_loss: 0.1294 - val_n_outputs1_loss: 0.0694\nEpoch 20/100\n92/92 [==============================] - ETA: 0s - loss: 0.1353 - n_outputs0_loss: 0.0747 - n_outputs1_loss: 0.0606\nEpoch 00020: val_loss did not improve from 0.19877\n92/92 [==============================] - 10s 110ms/step - loss: 0.1353 - n_outputs0_loss: 0.0747 - n_outputs1_loss: 0.0606 - val_loss: 0.2004 - val_n_outputs0_loss: 0.1312 - val_n_outputs1_loss: 0.0692\nEpoch 21/100\n92/92 [==============================] - ETA: 0s - loss: 0.1319 - n_outputs0_loss: 0.0731 - n_outputs1_loss: 0.0588\nEpoch 00021: val_loss improved from 0.19877 to 0.19564, saving model to ./models/msp-car-2.h5\n92/92 [==============================] - 10s 111ms/step - loss: 0.1319 - n_outputs0_loss: 0.0731 - n_outputs1_loss: 0.0588 - val_loss: 0.1956 - val_n_outputs0_loss: 0.1252 - val_n_outputs1_loss: 0.0704\nEpoch 22/100\n92/92 [==============================] - ETA: 0s - loss: 0.1299 - n_outputs0_loss: 0.0713 - n_outputs1_loss: 0.0585\nEpoch 00022: val_loss improved from 0.19564 to 0.19422, saving model to ./models/msp-car-2.h5\n92/92 [==============================] - 10s 111ms/step - loss: 0.1299 - n_outputs0_loss: 0.0713 - n_outputs1_loss: 0.0585 - val_loss: 0.1942 - val_n_outputs0_loss: 0.1259 - val_n_outputs1_loss: 0.0683\nEpoch 23/100\n92/92 [==============================] - ETA: 0s - loss: 0.1231 - n_outputs0_loss: 0.0684 - n_outputs1_loss: 0.0548\nEpoch 00023: val_loss improved from 0.19422 to 0.19270, saving model to ./models/msp-car-2.h5\n92/92 [==============================] - 10s 111ms/step - loss: 0.1231 - n_outputs0_loss: 0.0684 - n_outputs1_loss: 0.0548 - val_loss: 0.1927 - val_n_outputs0_loss: 0.1245 - val_n_outputs1_loss: 0.0682\nEpoch 24/100\n92/92 [==============================] - ETA: 0s - loss: 0.1239 - n_outputs0_loss: 0.0673 - n_outputs1_loss: 0.0566\nEpoch 00024: val_loss did not improve from 0.19270\n92/92 [==============================] - 10s 110ms/step - loss: 0.1239 - n_outputs0_loss: 0.0673 - n_outputs1_loss: 0.0566 - val_loss: 0.1969 - val_n_outputs0_loss: 0.1283 - val_n_outputs1_loss: 0.0686\nEpoch 25/100\n92/92 [==============================] - ETA: 0s - loss: 0.1200 - n_outputs0_loss: 0.0650 - n_outputs1_loss: 0.0550\nEpoch 00025: val_loss did not improve from 0.19270\n92/92 [==============================] - 10s 111ms/step - loss: 0.1200 - n_outputs0_loss: 0.0650 - n_outputs1_loss: 0.0550 - val_loss: 0.1990 - val_n_outputs0_loss: 0.1284 - val_n_outputs1_loss: 0.0706\nEpoch 26/100\n92/92 [==============================] - ETA: 0s - loss: 0.1171 - n_outputs0_loss: 0.0636 - n_outputs1_loss: 0.0535\nEpoch 00026: val_loss did not improve from 0.19270\n92/92 [==============================] - 10s 111ms/step - loss: 0.1171 - n_outputs0_loss: 0.0636 - n_outputs1_loss: 0.0535 - val_loss: 0.1929 - val_n_outputs0_loss: 0.1250 - val_n_outputs1_loss: 0.0678\nEpoch 27/100\n92/92 [==============================] - ETA: 0s - loss: 0.1167 - n_outputs0_loss: 0.0638 - n_outputs1_loss: 0.0529\nEpoch 00027: val_loss did not improve from 0.19270\n92/92 [==============================] - 10s 112ms/step - loss: 0.1167 - n_outputs0_loss: 0.0638 - n_outputs1_loss: 0.0529 - val_loss: 0.1937 - val_n_outputs0_loss: 0.1269 - val_n_outputs1_loss: 0.0668\nEpoch 28/100\n92/92 [==============================] - ETA: 0s - loss: 0.1123 - n_outputs0_loss: 0.0610 - n_outputs1_loss: 0.0513\nEpoch 00028: val_loss improved from 0.19270 to 0.19161, saving model to ./models/msp-car-2.h5\n92/92 [==============================] - 10s 112ms/step - loss: 0.1123 - n_outputs0_loss: 0.0610 - n_outputs1_loss: 0.0513 - val_loss: 0.1916 - val_n_outputs0_loss: 0.1230 - val_n_outputs1_loss: 0.0686\nEpoch 29/100\n92/92 [==============================] - ETA: 0s - loss: 0.1086 - n_outputs0_loss: 0.0584 - n_outputs1_loss: 0.0501\nEpoch 00029: val_loss improved from 0.19161 to 0.18655, saving model to ./models/msp-car-2.h5\n92/92 [==============================] - 10s 110ms/step - loss: 0.1086 - n_outputs0_loss: 0.0584 - n_outputs1_loss: 0.0501 - val_loss: 0.1865 - val_n_outputs0_loss: 0.1216 - val_n_outputs1_loss: 0.0650\nEpoch 30/100\n92/92 [==============================] - ETA: 0s - loss: 0.1093 - n_outputs0_loss: 0.0593 - n_outputs1_loss: 0.0500\nEpoch 00030: val_loss did not improve from 0.18655\n92/92 [==============================] - 10s 109ms/step - loss: 0.1093 - n_outputs0_loss: 0.0593 - n_outputs1_loss: 0.0500 - val_loss: 0.1936 - val_n_outputs0_loss: 0.1240 - val_n_outputs1_loss: 0.0696\nEpoch 31/100\n92/92 [==============================] - ETA: 0s - loss: 0.1077 - n_outputs0_loss: 0.0578 - n_outputs1_loss: 0.0499\nEpoch 00031: val_loss did not improve from 0.18655\n92/92 [==============================] - 10s 110ms/step - loss: 0.1077 - n_outputs0_loss: 0.0578 - n_outputs1_loss: 0.0499 - val_loss: 0.1889 - val_n_outputs0_loss: 0.1222 - val_n_outputs1_loss: 0.0667\nEpoch 32/100\n92/92 [==============================] - ETA: 0s - loss: 0.1026 - n_outputs0_loss: 0.0551 - n_outputs1_loss: 0.0475\nEpoch 00032: val_loss improved from 0.18655 to 0.18343, saving model to ./models/msp-car-2.h5\n92/92 [==============================] - 10s 111ms/step - loss: 0.1026 - n_outputs0_loss: 0.0551 - n_outputs1_loss: 0.0475 - val_loss: 0.1834 - val_n_outputs0_loss: 0.1206 - val_n_outputs1_loss: 0.0629\nEpoch 33/100\n92/92 [==============================] - ETA: 0s - loss: 0.1022 - n_outputs0_loss: 0.0545 - n_outputs1_loss: 0.0477\nEpoch 00033: val_loss did not improve from 0.18343\n92/92 [==============================] - 10s 110ms/step - loss: 0.1022 - n_outputs0_loss: 0.0545 - n_outputs1_loss: 0.0477 - val_loss: 0.1843 - val_n_outputs0_loss: 0.1191 - val_n_outputs1_loss: 0.0652\nEpoch 34/100\n92/92 [==============================] - ETA: 0s - loss: 0.0995 - n_outputs0_loss: 0.0529 - n_outputs1_loss: 0.0466\nEpoch 00034: val_loss improved from 0.18343 to 0.18117, saving model to ./models/msp-car-2.h5\n92/92 [==============================] - 10s 110ms/step - loss: 0.0995 - n_outputs0_loss: 0.0529 - n_outputs1_loss: 0.0466 - val_loss: 0.1812 - val_n_outputs0_loss: 0.1166 - val_n_outputs1_loss: 0.0646\nEpoch 35/100\n92/92 [==============================] - ETA: 0s - loss: 0.0989 - n_outputs0_loss: 0.0526 - n_outputs1_loss: 0.0463\nEpoch 00035: val_loss did not improve from 0.18117\n92/92 [==============================] - 10s 110ms/step - loss: 0.0989 - n_outputs0_loss: 0.0526 - n_outputs1_loss: 0.0463 - val_loss: 0.1835 - val_n_outputs0_loss: 0.1177 - val_n_outputs1_loss: 0.0657\nEpoch 36/100\n92/92 [==============================] - ETA: 0s - loss: 0.0972 - n_outputs0_loss: 0.0514 - n_outputs1_loss: 0.0458\nEpoch 00036: val_loss did not improve from 0.18117\n92/92 [==============================] - 10s 111ms/step - loss: 0.0972 - n_outputs0_loss: 0.0514 - n_outputs1_loss: 0.0458 - val_loss: 0.1838 - val_n_outputs0_loss: 0.1198 - val_n_outputs1_loss: 0.0641\nEpoch 37/100\n92/92 [==============================] - ETA: 0s - loss: 0.0959 - n_outputs0_loss: 0.0509 - n_outputs1_loss: 0.0450\nEpoch 00037: val_loss did not improve from 0.18117\n92/92 [==============================] - 10s 109ms/step - loss: 0.0959 - n_outputs0_loss: 0.0509 - n_outputs1_loss: 0.0450 - val_loss: 0.1830 - val_n_outputs0_loss: 0.1191 - val_n_outputs1_loss: 0.0639\nEpoch 38/100\n92/92 [==============================] - ETA: 0s - loss: 0.0934 - n_outputs0_loss: 0.0496 - n_outputs1_loss: 0.0438\nEpoch 00038: val_loss did not improve from 0.18117\n92/92 [==============================] - 10s 110ms/step - loss: 0.0934 - n_outputs0_loss: 0.0496 - n_outputs1_loss: 0.0438 - val_loss: 0.1845 - val_n_outputs0_loss: 0.1185 - val_n_outputs1_loss: 0.0660\nEpoch 39/100\n92/92 [==============================] - ETA: 0s - loss: 0.0923 - n_outputs0_loss: 0.0477 - n_outputs1_loss: 0.0446\nEpoch 00039: val_loss did not improve from 0.18117\n92/92 [==============================] - 10s 110ms/step - loss: 0.0923 - n_outputs0_loss: 0.0477 - n_outputs1_loss: 0.0446 - val_loss: 0.1818 - val_n_outputs0_loss: 0.1186 - val_n_outputs1_loss: 0.0632\nWARNING: CPU random generator seem to be failing, disable hardware random number generation\nWARNING: RDRND generated: 0xffffffff 0xffffffff 0xffffffff 0xffffffff\n(donkey) arl@arl1:~/mycar$\n</code></pre>"},{"location":"training-logs/msp-car-2/#checking-the-models","title":"Checking the models","text":"<pre><code>ls -l models/*\n</code></pre> <p>returns</p> <pre><code>ls -l models/*\n-rw-r--r-- 1 arl arl    32317 Jul 26 20:30 models/database.json\n-rw-r--r-- 1 arl arl 35773936 Jul 26 20:17 models/msp-car-1-gpu.h5\n-rw-r--r-- 1 arl arl    27506 Jul 26 20:17 models/msp-car-1-gpu.png\n-rw-r--r-- 1 arl arl    23659 Jul 26 19:57 models/msp-car-1.png\n-rw-r--r-- 1 arl arl 35773936 Jul 26 20:29 models/msp-car-2.h5\n-rw-r--r-- 1 arl arl    25670 Jul 26 20:30 models/msp-car-2.png\n-rw-r--r-- 1 arl arl    22616 Feb  2  2020 models/mypilot.h5_loss_acc_0.040245.png\n-rw-r--r-- 1 arl arl    26687 Feb  2  2020 models/mypilot.h5_loss_acc_0.042222.png\n-rw-r--r-- 1 arl arl 11939744 Feb  2  2020 models/ref-model.h5\n</code></pre>"}]}